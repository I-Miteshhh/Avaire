# Week 8-9: Apache Flink â€” WHITEPAPERS Track

*"Reading the Flink, Dataflow Model, and Chandy-Lamport papers transforms you from a user of stream processing to an architect of distributed systems."* â€” Principal Engineer, LinkedIn

---

## ğŸ“œ Core Research Papers

This track covers the foundational research papers that define modern stream processing. Each paper is analyzed with context, key innovations, and practical implications.

---

## 1. Apache Flink: Stream and Batch Processing in a Single Engine (2015)

**Citation:**
Carbone, Paris, et al. "Apache Flink: Stream and batch processing in a single engine." Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 38.4 (2015).

**Link:** http://sites.computer.org/debull/A15dec/p28.pdf

---

### ğŸ¯ Context and Motivation

```
Problem (2015):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Lambda Architecture (Nathan Marz, 2011):               â”‚
â”‚                                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Batch Layer  â”‚ (Hadoop/Spark)  â”‚ Speed Layer  â”‚    â”‚
â”‚ â”‚ (hours)      â”‚                 â”‚ (Storm)      â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        â”‚                                 â”‚            â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                     â–¼                                 â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚             â”‚ Serving Layerâ”‚                          â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                        â”‚
â”‚ Issues:                                               â”‚
â”‚ â”œâ”€ Two separate codebases (MapReduce + Storm)        â”‚
â”‚ â”œâ”€ Data inconsistency between batch and stream       â”‚
â”‚ â”œâ”€ High maintenance overhead                         â”‚
â”‚ â””â”€ Eventual consistency only                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flink's Vision: Kappa Architecture (Jay Kreps, 2014)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚          â”‚  Unified Stream Processing  â”‚              â”‚
â”‚          â”‚  (Flink)                    â”‚              â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                     â”‚                                 â”‚
â”‚                     â–¼                                 â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚            â”‚ Serving Layerâ”‚                           â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                        â”‚
â”‚ Benefits:                                             â”‚
â”‚ â”œâ”€ Single codebase (batch = bounded stream)          â”‚
â”‚ â”œâ”€ Consistent semantics                              â”‚
â”‚ â”œâ”€ Lower operational complexity                      â”‚
â”‚ â””â”€ Exactly-once guarantees                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ”¬ Key Innovations

#### 1. Dataflow Programming Model

```
Flink Program = Directed Acyclic Graph (DAG)

User Code:
DataStream<Event> stream = env.addSource(kafkaSource);
stream
    .map(new ParseEvent())
    .keyBy(event -> event.getUserId())
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))
    .sum("amount")
    .addSink(kafkaSink);

Compiled to JobGraph:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Map    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KeyBy   â”‚ (Hash Partition)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Window  â”‚ (Stateful)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sum    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sink   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Execution Graph (with parallelism=4):
â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”
â”‚Src1â”‚ â”‚Src2â”‚ â”‚Src3â”‚ â”‚Src4â”‚
â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜
  â”‚      â”‚      â”‚      â”‚
  â–¼      â–¼      â–¼      â–¼
â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”
â”‚Map1â”‚ â”‚Map2â”‚ â”‚Map3â”‚ â”‚Map4â”‚
â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜
  â”‚      â”‚      â”‚      â”‚
  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (Shuffle)
  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
  â–¼      â–¼      â–¼      â–¼
â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”
â”‚Win1â”‚ â”‚Win2â”‚ â”‚Win3â”‚ â”‚Win4â”‚
â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜
  â”‚      â”‚      â”‚      â”‚
  â–¼      â–¼      â–¼      â–¼
â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”
â”‚Snk1â”‚ â”‚Snk2â”‚ â”‚Snk3â”‚ â”‚Snk4â”‚
â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜
```

---

#### 2. Asynchronous Barrier Snapshotting (ABS)

**Based on Chandy-Lamport algorithm (see next paper)**

```
Innovation: Non-blocking distributed snapshots

Traditional Approach (pause-the-world):
T=0:  Stop all processing
T=1:  Snapshot all state
T=2:  Resume processing
Cost: Downtime = 1-10 seconds

Flink ABS Approach:
T=0:  Inject checkpoint barriers into streams
T=0-5: Process data concurrently with checkpointing
T=5:  Checkpoint completes
Cost: No downtime, millisecond overhead

Algorithm:
1. JobManager triggers checkpoint N
2. Sources emit barrier(N) into data stream
3. Operators:
   - Receive barrier from all inputs (alignment)
   - Snapshot local state
   - Forward barrier to outputs
4. Sinks acknowledge checkpoint N
5. JobManager marks checkpoint N complete

State Snapshot Includes:
â”œâ”€ Operator state (keyed state, operator state)
â”œâ”€ In-flight data (for unaligned checkpoints)
â”œâ”€ Source positions (Kafka offsets, file pointers)
â””â”€ Metadata (checkpoint ID, timestamp)
```

---

#### 3. Pipelined Execution Engine

```
Traditional Batch (MapReduce):
Map Phase:   [====100%====] â†’ Write to disk
                                   â†“
Reduce Phase:                [====100%====] â†’ Write to disk

Latency: Minutes to hours

Flink Pipelined Execution:
Source â†’ Map â†’ KeyBy â†’ Window â†’ Sink
  â†“      â†“      â†“       â†“       â†“
 [====== continuous data flow ======]

Latency: Milliseconds

Key Difference:
â”œâ”€ No materialization between operators
â”œâ”€ Data flows through memory (network buffers)
â”œâ”€ Backpressure propagates upstream
â””â”€ Enables true streaming
```

---

#### 4. Memory Management

```
JVM Heap vs Flink Managed Memory:

Standard JVM Approach:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JVM Heap (all objects)               â”‚
â”‚ â”œâ”€ Application objects               â”‚
â”‚ â”œâ”€ Intermediate results              â”‚
â”‚ â””â”€ Cached data                       â”‚
â”‚                                      â”‚
â”‚ Problem:                             â”‚
â”‚ â”œâ”€ Full GC pauses (10+ seconds)     â”‚
â”‚ â””â”€ OutOfMemoryError unpredictable   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flink Managed Memory:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JVM Heap (small, controlled)         â”‚
â”‚ â”œâ”€ Application objects               â”‚
â”‚ â””â”€ Flink framework objects           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Off-Heap Managed Memory              â”‚
â”‚ â”œâ”€ Sorting/Hashing (batch)           â”‚
â”‚ â”œâ”€ RocksDB state (streaming)         â”‚
â”‚ â””â”€ Network buffers                   â”‚
â”‚                                      â”‚
â”‚ Benefits:                            â”‚
â”‚ â”œâ”€ Predictable memory usage          â”‚
â”‚ â”œâ”€ No GC overhead                    â”‚
â”‚ â””â”€ Efficient serialization           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Configuration:
taskmanager.memory.process.size: 4096m
â”œâ”€ Framework Heap: 128m (Flink internals)
â”œâ”€ Task Heap: 512m (user code)
â”œâ”€ Managed Memory: 1638m (40%, for RocksDB)
â”œâ”€ Network Memory: 409m (10%, for shuffle)
â””â”€ JVM Overhead: 409m (10%, for JVM)
```

---

### ğŸ’¡ Practical Implications

```
1. Batch as Bounded Stream:
   - Write once, run in batch or streaming mode
   - Same code for historical analysis and real-time

2. State as First-Class Citizen:
   - Persistent, fault-tolerant state
   - Enables complex stateful computations

3. Exactly-Once Guarantees:
   - ABS + transactional sinks
   - Production-ready consistency

4. High Throughput + Low Latency:
   - Pipelined execution: <10ms latency
   - Millions of events/second per core
```

---

## 2. Lightweight Asynchronous Snapshots for Distributed Dataflows (2015)

**Citation:**
Carbone, Paris, et al. "Lightweight asynchronous snapshots for distributed dataflows." arXiv preprint arXiv:1506.08603 (2015).

**Link:** https://arxiv.org/pdf/1506.08603.pdf

---

### ğŸ¯ The Distributed Snapshot Problem

```
Given:
- Distributed system with N processes
- Communication via message passing
- Continuous execution (no global clock)

Goal:
- Capture consistent global state
- Without stopping execution
- Such that: State can be used for recovery

Consistency Definition:
A snapshot is consistent if:
âˆ€ message m: 
  If receive(m) is in snapshot â†’ send(m) is in snapshot
  (No "orphan" messages)

Example of INCONSISTENT snapshot:
Process A: state = {sent: $100 to B}
Process B: state = {received: $0}
Problem: $100 lost!

Example of CONSISTENT snapshot:
Process A: state = {sent: $100 to B}
Process B: state = {received: $100}
OR:
Process A: state = {balance: $100}
Process B: state = {balance: $0}
In-flight: [$100 transfer]
```

---

### ğŸ”¬ Chandy-Lamport Algorithm (1985)

**Original Paper:** Chandy, K. M., and Leslie Lamport. "Distributed snapshots: Determining global states of distributed systems." ACM Transactions on Computer Systems (1985).

```
Algorithm:

Initiator Process Pi:
1. Record own state
2. Send MARKER on all outgoing channels
3. Start recording incoming messages

Non-Initiator Process Pj (receives MARKER from Pk):
IF first MARKER:
   1. Record own state
   2. Mark channel Pkâ†’Pj state as empty
   3. Send MARKER on all outgoing channels
   4. Start recording messages on OTHER channels
ELSE:
   1. Stop recording channel Pkâ†’Pj
   2. Channel state = recorded messages

Termination: All processes recorded + all channel states recorded

Example (3 processes):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T=0: P1 initiates                                      â”‚
â”‚ â”œâ”€ P1: state = {x=10}                                 â”‚
â”‚ â”œâ”€ P1 sends: MARKER â†’ P2, MARKER â†’ P3                 â”‚
â”‚ â””â”€ P1 records: channel P2â†’P1, channel P3â†’P1           â”‚
â”‚                                                        â”‚
â”‚ T=1: P2 receives MARKER from P1                       â”‚
â”‚ â”œâ”€ P2: state = {y=20}                                 â”‚
â”‚ â”œâ”€ P2: channel P1â†’P2 = {} (empty)                     â”‚
â”‚ â”œâ”€ P2 sends: MARKER â†’ P1, MARKER â†’ P3                 â”‚
â”‚ â””â”€ P2 records: channel P3â†’P2                          â”‚
â”‚                                                        â”‚
â”‚ T=2: P3 receives MARKER from P1                       â”‚
â”‚ â”œâ”€ P3: state = {z=30}                                 â”‚
â”‚ â”œâ”€ P3: channel P1â†’P3 = {}                             â”‚
â”‚ â”œâ”€ P3 sends: MARKER â†’ P1, MARKER â†’ P2                 â”‚
â”‚ â””â”€ P3 records: channel P2â†’P3                          â”‚
â”‚                                                        â”‚
â”‚ T=3: P2 receives MARKER from P3                       â”‚
â”‚ â”œâ”€ P2 stops recording channel P3â†’P2                   â”‚
â”‚ â””â”€ P2: channel P3â†’P2 = {msg1, msg2}                   â”‚
â”‚                                                        â”‚
â”‚ ... (all markers received) ...                        â”‚
â”‚                                                        â”‚
â”‚ Global Snapshot:                                       â”‚
â”‚ â”œâ”€ P1: {x=10}                                         â”‚
â”‚ â”œâ”€ P2: {y=20}                                         â”‚
â”‚ â”œâ”€ P3: {z=30}                                         â”‚
â”‚ â””â”€ Channels: P3â†’P2 = {msg1, msg2}, ...               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸš€ Flink's Adaptation: Barrier Snapshotting

```
Differences from Chandy-Lamport:

1. Markers â†’ Checkpoint Barriers
   - Injected at sources (not arbitrary process)
   - Flow with data stream

2. Channel State Recording â†’ Barrier Alignment
   - Buffer data between first and last barrier
   - Ensures exactly-once semantics

3. Asynchronous State Snapshot
   - Use copy-on-write (e.g., RocksDB snapshots)
   - No blocking of processing

Flink Algorithm:

Source Operators (on checkpoint trigger):
1. Snapshot source state (Kafka offsets, file position)
2. Emit barrier(checkpoint_id) into output stream
3. Acknowledge checkpoint

Stateful Operators (on barrier arrival):
1. IF multiple inputs:
      WAIT for barriers from ALL inputs (alignment)
      BUFFER late-arriving data
2. Snapshot operator state:
   - ValueState, ListState, MapState, etc.
   - Window contents
   - Timers
3. Emit barrier to downstream operators
4. Acknowledge checkpoint

Sink Operators (on barrier arrival):
1. Snapshot sink state (transactional state)
2. Pre-commit pending transactions (2PC phase 1)
3. Acknowledge checkpoint

JobManager (on all acknowledgments):
1. Mark checkpoint as COMPLETED
2. Notify sinks to COMMIT transactions (2PC phase 2)
3. Store checkpoint metadata
4. Trigger cleanup of old checkpoints
```

---

### ğŸ“Š Performance Analysis

```
Metrics from Paper:

Checkpoint Latency (time to complete):
â”œâ”€ Small state (<1 GB): 50-200ms
â”œâ”€ Medium state (1-10 GB): 200ms-2s
â””â”€ Large state (>10 GB): 2-10s (with incremental)

Overhead on Throughput:
â”œâ”€ Aligned checkpoints: 1-5% throughput decrease
â”œâ”€ Unaligned checkpoints: <1% throughput decrease
â””â”€ Network overhead: ~2% (barrier messages)

State Snapshot Methods:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method            â”‚ Time      â”‚ Blocking â”‚ Size    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Synchronous copy  â”‚ O(state)  â”‚ Yes      â”‚ Full    â”‚
â”‚ Copy-on-write     â”‚ O(1)      â”‚ No       â”‚ Full    â”‚
â”‚ Incremental       â”‚ O(delta)  â”‚ No       â”‚ Delta   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RocksDB Incremental Checkpointing:
- Uses SST file sharing
- Checkpoint time: O(modified data)
- 10x faster for large state (paper shows 100GB â†’ 1GB delta)
```

---

### ğŸ’¡ Practical Implications

```
1. Exactly-Once Guarantees:
   - Barrier alignment ensures no duplicates
   - Combined with transactional sinks (2PC)

2. Minimal Performance Impact:
   - Asynchronous snapshots (no pause-the-world)
   - Sub-second checkpoints for most workloads

3. Failure Recovery:
   - Restore state from last checkpoint
   - Replay source from checkpointed position
   - Mean time to recovery (MTTR): <1 minute

4. Tuning Trade-offs:
   Checkpoint Interval:
   â”œâ”€ Short (10s): Fast recovery, higher overhead
   â”œâ”€ Medium (1min): Balanced
   â””â”€ Long (10min): Low overhead, slower recovery

   Aligned vs Unaligned:
   â”œâ”€ Aligned: Smaller checkpoints, may cause backpressure
   â””â”€ Unaligned: Faster under backpressure, larger checkpoints
```

---

## 3. The Dataflow Model (2015)

**Citation:**
Akidau, Tyler, et al. "The dataflow model: a practical approach to balancing correctness, latency, and cost in massive-scale, unbounded, out-of-order data processing." Proceedings of the VLDB Endowment 8.12 (2015): 1792-1803.

**Link:** http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf

---

### ğŸ¯ The Four Questions

```
Framework for reasoning about stream processing:

1. WHAT results are calculated?
   â†’ Transformations, aggregations, windows

2. WHERE in event time are results calculated?
   â†’ Event time windows, processing time windows

3. WHEN in processing time are results materialized?
   â†’ Triggers (early, on-time, late)

4. HOW do refinements relate to each other?
   â†’ Accumulation modes (discarding, accumulating, retracting)
```

---

### ğŸ”¬ Windowing and Triggers

```
Example: 1-minute tumbling event time windows

Stream of events (event_time, processing_time):
â”œâ”€ e1: (10:00:05, 10:00:07)
â”œâ”€ e2: (10:00:30, 10:00:32)
â”œâ”€ e3: (10:00:45, 10:00:47)
â”œâ”€ e4: (10:01:10, 10:01:12)
â”œâ”€ e5: (10:00:50, 10:01:15)  â† LATE by 15 seconds!

Window: [10:00:00, 10:01:00)

Trigger Strategies:

1. Watermark Trigger (ON-TIME):
   Fires when: watermark passes window end
   â”œâ”€ Watermark = max_event_time - allowed_lateness
   â”œâ”€ If allowed_lateness = 5s:
   â”‚  â””â”€ Watermark(10:01:05) â†’ Trigger window [10:00, 10:01)
   â””â”€ Result: [e1, e2, e3] (e5 missed, it's late)

2. Early Trigger (SPECULATIVE):
   Fires when: Every N records or M seconds
   â”œâ”€ Every 2 events:
   â”‚  â”œâ”€ After e1, e2: Emit partial result
   â”‚  â””â”€ After e3: Emit updated result
   â””â”€ Allows early insights (trading accuracy)

3. Late Trigger (REFINEMENT):
   Fires when: Late data arrives after watermark
   â”œâ”€ e5 arrives (late)
   â”œâ”€ Re-trigger window [10:00, 10:01)
   â””â”€ Emit refined result: [e1, e2, e3, e5]

Combined Triggering:
stream
    .keyBy(...)
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))
    .trigger(
        EventTimeTrigger.create()
            .withEarlyFirings(ProcessingTimeTrigger.every(Time.seconds(10)))
            .withLateFirings(AfterWatermark.pastEndOfWindow())
    )
    .sum("amount");
```

---

### ğŸ“ Accumulation Modes

```
Question: How do we handle multiple outputs for same window?

Given events for window [10:00, 10:01):
â”œâ”€ Early trigger (10s): [e1, e2] â†’ sum=30
â”œâ”€ Early trigger (20s): [e1, e2, e3] â†’ sum=80
â”œâ”€ On-time trigger: [e1, e2, e3, e4] â†’ sum=120
â””â”€ Late trigger: [e1, e2, e3, e4, e5] â†’ sum=135

1. DISCARDING:
   - Emit only NEW data since last trigger
   - Output: 30, 50 (Î”), 40 (Î”), 15 (Î”)
   - Use case: Incremental updates (Kafka compaction)

2. ACCUMULATING:
   - Emit cumulative results
   - Output: 30, 80, 120, 135
   - Use case: Replacing previous result (overwrite table)

3. ACCUMULATING & RETRACTING:
   - Emit: (retract old, emit new)
   - Output: +30, (-30, +80), (-80, +120), (-120, +135)
   - Use case: Changelogs, correct downstream aggregations

Flink Implementation:
// Discarding
.window(...).evictor(CountEvictor.of(0))

// Accumulating (default)
.window(...).sum("amount")

// Retracting (for SQL)
tableEnv.executeSql("SELECT SUM(amount) FROM events GROUP BY TUMBLE(ts, INTERVAL '1' MINUTE)")
// Produces retraction stream
```

---

### ğŸ’¡ Allowed Lateness

```
Configuration:

stream
    .keyBy(...)
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))
    .allowedLateness(Time.minutes(5))  // Accept late data up to 5 min
    .sideOutputLateData(lateOutputTag)  // Capture very late data
    .sum("amount");

Behavior:
Window [10:00, 10:01):

T=10:01:05: Watermark passes 10:01:00
â”œâ”€ Trigger ON-TIME: [e1, e2, e3] â†’ sum=80
â”œâ”€ Keep window state (don't discard yet)

T=10:02:30: Late event e4 arrives (timestamp=10:00:50)
â”œâ”€ e4 is within allowed lateness (< 5 min late)
â”œâ”€ Add to window state
â”œâ”€ Trigger LATE: [e1, e2, e3, e4] â†’ sum=95

T=10:06:05: Watermark passes 10:06:00
â”œâ”€ Allowed lateness expires (10:01 + 5min)
â”œâ”€ Discard window state

T=10:07:00: Very late event e5 arrives (timestamp=10:00:55)
â”œâ”€ e5 is beyond allowed lateness
â”œâ”€ Emit to side output (lateOutputTag)
â””â”€ Do NOT update window

Side output handling:
DataStream<Event> veryLateEvents = result.getSideOutput(lateOutputTag);
veryLateEvents.addSink(new KafkaSink("late-events"));
```

---

### ğŸ“Š Performance Trade-offs

```
From Paper (Google Cloud Dataflow):

Latency vs Correctness:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Processing Time Windows:                           â”‚
â”‚ â”œâ”€ Latency: Low (~100ms)                          â”‚
â”‚ â”œâ”€ Correctness: Low (skewed by delays)            â”‚
â”‚ â””â”€ Cost: Low (no state for late data)             â”‚
â”‚                                                    â”‚
â”‚ Event Time Windows (no late data):                â”‚
â”‚ â”œâ”€ Latency: Medium (~1s, watermark delay)         â”‚
â”‚ â”œâ”€ Correctness: High (deterministic)              â”‚
â”‚ â””â”€ Cost: Medium (watermark computation)           â”‚
â”‚                                                    â”‚
â”‚ Event Time + Allowed Lateness:                    â”‚
â”‚ â”œâ”€ Latency: High (wait for late data)             â”‚
â”‚ â”œâ”€ Correctness: Very High (99.9% completeness)    â”‚
â”‚ â””â”€ Cost: High (retain state, recomputation)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Recommendation:
- Real-time dashboards: Early triggers, discarding mode
- Financial aggregations: Allowed lateness, accumulating mode
- ML training: Processing time (latency critical)
- Analytics: Event time + late firings (accuracy critical)
```

---

## ğŸ† Interview Discussion Topics

1. **Compare Chandy-Lamport with Flink's barrier snapshotting. What are the key differences?**

2. **Explain the "WHAT/WHERE/WHEN/HOW" questions from the Dataflow Model. Give examples.**

3. **How would you configure triggers for a real-time analytics dashboard that needs to show results every 10 seconds but still handle late data?**

4. **What are the trade-offs between aligned and unaligned checkpoints? When would you use each?**

5. **Describe RocksDB incremental checkpointing. Why is it faster for large state?**

6. **How does Flink achieve exactly-once semantics end-to-end? Explain the role of ABS and two-phase commit.**

---

## ğŸ“ Summary Checklist

- [ ] Understand Flink's dataflow programming model
- [ ] Explain asynchronous barrier snapshotting (ABS)
- [ ] Master Chandy-Lamport distributed snapshots
- [ ] Apply Dataflow Model's four questions
- [ ] Configure triggers and allowed lateness
- [ ] Understand accumulation modes (discarding, accumulating, retracting)
- [ ] Answer whitepaper-based interview questions

**Next:** [README.md â†’](README.md)
