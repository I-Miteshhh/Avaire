# Week 8-9: Apache Flink â€” BEGINNER Track

*"Flink is the most advanced stream processing engine. Understanding event time, watermarks, and state management is essential for building real-time data applications."* â€” Flink PMC Member

---

## ğŸ¯ Learning Outcomes

By the end of this track, you will:
- Understand stream processing fundamentals (bounded vs unbounded data)
- Master event time vs processing time concepts
- Learn watermark mechanics for handling out-of-order data
- Implement windowed computations (tumbling, sliding, session windows)
- Use Flink DataStream API for basic transformations
- Deploy simple Flink applications on local cluster

---

## ğŸ“š Prerequisites

- âœ… Java/Scala basics
- âœ… Distributed systems fundamentals
- âœ… Kafka basics (producers, consumers, topics)

---

## ğŸŒŠ 1. Stream Processing Fundamentals

### Bounded vs Unbounded Data

```
Bounded Data (Batch Processing):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset: Fixed size, known end         â”‚
â”‚ Example: Historical sales (Jan-Dec)    â”‚
â”‚ Processing: MapReduce, Spark Batch     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Unbounded Data (Stream Processing):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset: Infinite, continuous          â”‚
â”‚ Example: Real-time clicks, IoT sensors â”‚
â”‚ Processing: Flink, Kafka Streams        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Difference:
- Batch: "Process all data, then produce result"
- Stream: "Process data incrementally as it arrives"
```

### Why Flink?

```
Comparison with other systems:

Spark Streaming (Micro-Batching):
â”œâ”€ Processes data in small batches (500ms-2s)
â”œâ”€ Higher latency (seconds)
â””â”€ Simpler model, but not true streaming

Kafka Streams (Library):
â”œâ”€ True streaming, low latency (<100ms)
â”œâ”€ Embedded in application (no separate cluster)
â””â”€ Limited to Kafka sources/sinks

Flink (True Streaming):
â”œâ”€ True streaming, ultra-low latency (<10ms)
â”œâ”€ Rich state management
â”œâ”€ Event time processing, watermarks
â”œâ”€ Exactly-once guarantees
â””â”€ Batch + streaming in one engine
```

---

## â° 2. Event Time vs Processing Time

### The Problem: Out-of-Order Events

```
Real-World Scenario: Mobile app events

User clicks "Buy" at 10:00:00 (event time)
â”œâ”€ Network delay: 5 seconds
â””â”€ Arrives at server: 10:00:05 (processing time)

User clicks "View" at 10:00:03 (event time)
â”œâ”€ Fast network
â””â”€ Arrives at server: 10:00:04 (processing time)

Order of arrival: "View" (10:00:04), "Buy" (10:00:05)
Order of occurrence: "Buy" (10:00:00), "View" (10:00:03)

Question: How to count events in 1-minute windows correctly?
```

### Event Time vs Processing Time

```
Processing Time:
- Time when event is processed by the system
- Simple, no need to extract timestamps
- Problem: Non-deterministic (network delays)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Window: 10:00 - 10:01                â”‚
â”‚ Events: All events that arrived      â”‚
â”‚         between 10:00 and 10:01      â”‚
â”‚ Issue: Events from 9:59 might arrive â”‚
â”‚        at 10:00:05 (wrong window!)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Event Time:
- Time when event actually occurred
- Deterministic, reproducible results
- Requires: Extract timestamp from event data

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Window: 10:00 - 10:01                â”‚
â”‚ Events: All events with timestamp    â”‚
â”‚         between 10:00 and 10:01      â”‚
â”‚ Benefit: Correct semantics even with â”‚
â”‚          delays!                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’§ 3. Watermarks: Handling Late Data

### What Are Watermarks?

```
Watermark = "All events with timestamp < T have arrived"

Example:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stream of events:                                        â”‚
â”‚                                                          â”‚
â”‚ Event A: timestamp=10:00:00, arrives at 10:00:02       â”‚
â”‚ Event B: timestamp=10:00:05, arrives at 10:00:06       â”‚
â”‚ Event C: timestamp=10:00:03, arrives at 10:00:07 (late!)â”‚
â”‚ Event D: timestamp=10:00:10, arrives at 10:00:11       â”‚
â”‚                                                          â”‚
â”‚ Watermarks:                                              â”‚
â”‚ â”œâ”€ After Event B: Watermark(10:00:05)                  â”‚
â”‚ â”‚  â†’ "Events up to 10:00:05 have arrived"             â”‚
â”‚ â”œâ”€ Event C arrives (late, but within watermark)        â”‚
â”‚ â””â”€ After Event D: Watermark(10:00:10)                  â”‚
â”‚    â†’ "Events up to 10:00:10 have arrived"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When watermark(T) passes:
- Trigger window computation for windows ending at T
- Assume no more events with timestamp < T will arrive
```

### Watermark Strategies

```java
// 1. Bounded Out-of-Orderness (most common)
WatermarkStrategy<Event> strategy = WatermarkStrategy
    .<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))
    .withTimestampAssigner((event, timestamp) -> event.getTimestamp());

// Meaning: "Events can arrive up to 5 seconds late"
// Watermark = max_event_timestamp - 5 seconds

// 2. Monotonous Timestamps (no late data)
WatermarkStrategy<Event> strategy = WatermarkStrategy
    .<Event>forMonotonousTimestamps()
    .withTimestampAssigner((event, timestamp) -> event.getTimestamp());

// Meaning: "Events arrive in order"
// Watermark = max_event_timestamp
```

**Example:**

```
Events:
â”œâ”€ Event A: timestamp=10:00:00, arrives
â”‚  Watermark = 10:00:00 - 5s = 09:59:55
â”œâ”€ Event B: timestamp=10:00:10, arrives
â”‚  Watermark = 10:00:10 - 5s = 10:00:05
â”œâ”€ Event C: timestamp=10:00:03, arrives (late, but within 5s)
â”‚  Accepted! (10:00:03 > 10:00:05 - 5s)
â””â”€ Event D: timestamp=09:59:50, arrives (too late!)
   Dropped! (09:59:50 < 10:00:05 - 5s)
```

---

## ğŸªŸ 4. Windowing: Grouping Events

### Tumbling Windows (Non-Overlapping)

```
Window size: 1 minute

10:00:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10:01:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10:02:00
â”‚        Window 1               â”‚        Window 2               â”‚
â”‚ Events: [A, B, C]             â”‚ Events: [D, E, F]             â”‚
â”‚ Result: COUNT=3               â”‚ Result: COUNT=3               â”‚

Use case: "Count clicks per minute"
```

```java
DataStream<Event> stream = ...;

stream
    .keyBy(event -> event.getUserId())
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))
    .sum("amount");
```

### Sliding Windows (Overlapping)

```
Window size: 1 minute, slide: 30 seconds

10:00:00 â”€â”€â”€â”€â”€â”€â”€ 10:00:30 â”€â”€â”€â”€â”€â”€â”€ 10:01:00 â”€â”€â”€â”€â”€â”€â”€ 10:01:30
â”‚        Window 1        â”‚
                â”‚        Window 2        â”‚
                                â”‚        Window 3        â”‚

Window 1: [10:00:00, 10:01:00) â†’ Events: [A, B, C]
Window 2: [10:00:30, 10:01:30) â†’ Events: [B, C, D, E]
Window 3: [10:01:00, 10:02:00) â†’ Events: [D, E, F, G]

Use case: "Moving average of last 1 minute, updated every 30 seconds"
```

```java
stream
    .keyBy(event -> event.getUserId())
    .window(SlidingEventTimeWindows.of(Time.minutes(1), Time.seconds(30)))
    .sum("amount");
```

### Session Windows (Gap-Based)

```
Session gap: 10 minutes (inactivity timeout)

Events:
â”œâ”€ 10:00:00 (Event A)
â”œâ”€ 10:05:00 (Event B) â† Within 10 min
â”œâ”€ 10:08:00 (Event C) â† Within 10 min
â”œâ”€ 10:20:00 (Event D) â† Gap > 10 min, NEW SESSION!

Session 1: [10:00:00, 10:18:00) â†’ Events: [A, B, C]
Session 2: [10:20:00, ...) â†’ Events: [D, ...]

Use case: "User session analytics (activity bursts)"
```

```java
stream
    .keyBy(event -> event.getUserId())
    .window(EventTimeSessionWindows.withGap(Time.minutes(10)))
    .sum("amount");
```

---

## ğŸ› ï¸ 5. Flink DataStream API Basics

### Simple Word Count (Stream Version)

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.util.Collector;

public class StreamingWordCount {
    public static void main(String[] args) throws Exception {
        // 1. Create execution environment
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 2. Read from socket (for testing)
        DataStream<String> text = env.socketTextStream("localhost", 9999);
        
        // 3. Transform: Split into words
        DataStream<Tuple2<String, Integer>> counts = text
            .flatMap(new Tokenizer())
            .keyBy(value -> value.f0)
            .sum(1);
        
        // 4. Print to console
        counts.print();
        
        // 5. Execute
        env.execute("Streaming WordCount");
    }
    
    public static class Tokenizer 
        implements FlatMapFunction<String, Tuple2<String, Integer>> {
        
        @Override
        public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
            for (String word : value.split("\\s")) {
                out.collect(new Tuple2<>(word, 1));
            }
        }
    }
}
```

**Test:**

```bash
# Terminal 1: Start netcat
nc -lk 9999

# Terminal 2: Run Flink application
./bin/flink run WordCount.jar

# Terminal 1: Type words
hello world
hello flink

# Terminal 2: See output
(hello, 1)
(world, 1)
(hello, 2)
(flink, 1)
```

---

### Reading from Kafka

```java
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.api.common.serialization.SimpleStringSchema;

Properties props = new Properties();
props.setProperty("bootstrap.servers", "localhost:9092");
props.setProperty("group.id", "flink-consumer");

FlinkKafkaConsumer<String> consumer = new FlinkKafkaConsumer<>(
    "events",                  // Topic
    new SimpleStringSchema(),  // Deserialization schema
    props                      // Kafka properties
);

DataStream<String> stream = env.addSource(consumer);
```

---

### Windowed Aggregation

```java
public class SlidingWindowExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // Enable event time
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
        
        DataStream<Event> events = env
            .addSource(new KafkaSource())
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                    .withTimestampAssigner((event, ts) -> event.getTimestamp())
            );
        
        // Sliding window: 1 minute, slide 10 seconds
        DataStream<Tuple2<String, Double>> avgPerUser = events
            .keyBy(event -> event.getUserId())
            .window(SlidingEventTimeWindows.of(
                Time.minutes(1),  // Window size
                Time.seconds(10)  // Slide interval
            ))
            .aggregate(new AverageAggregate());
        
        avgPerUser.print();
        
        env.execute("Sliding Window Average");
    }
}
```

---

## ğŸ“Š 6. State Management Basics

### Keyed State (Per-Key Storage)

```java
public class StatefulCounter extends RichFlatMapFunction<Event, Tuple2<String, Long>> {
    
    // State: Store count per key
    private transient ValueState<Long> countState;
    
    @Override
    public void open(Configuration config) {
        ValueStateDescriptor<Long> descriptor = 
            new ValueStateDescriptor<>("count", Long.class, 0L);
        countState = getRuntimeContext().getState(descriptor);
    }
    
    @Override
    public void flatMap(Event event, Collector<Tuple2<String, Long>> out) throws Exception {
        // Get current count
        Long currentCount = countState.value();
        
        // Increment
        currentCount++;
        
        // Update state
        countState.update(currentCount);
        
        // Emit result
        out.collect(new Tuple2<>(event.getUserId(), currentCount));
    }
}

// Usage:
stream
    .keyBy(event -> event.getUserId())
    .flatMap(new StatefulCounter());
```

**State Lifecycle:**

```
State per key:
â”œâ”€ user_id=123 â†’ count=5
â”œâ”€ user_id=456 â†’ count=12
â””â”€ user_id=789 â†’ count=3

When event for user_id=123 arrives:
1. Retrieve state: count=5
2. Process: count++
3. Update state: count=6
4. Emit: (123, 6)
```

---

## ğŸš€ 7. Deploying Flink Applications

### Local Cluster (Development)

```bash
# Download Flink
wget https://archive.apache.org/dist/flink/flink-1.18.0/flink-1.18.0-bin-scala_2.12.tgz
tar -xzf flink-1.18.0-bin-scala_2.12.tgz
cd flink-1.18.0

# Start local cluster
./bin/start-cluster.sh

# Open Web UI
http://localhost:8081

# Submit job
./bin/flink run examples/streaming/WordCount.jar

# Stop cluster
./bin/stop-cluster.sh
```

### Docker Compose Setup

```yaml
version: '3.8'
services:
  jobmanager:
    image: flink:1.18.0
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager

  taskmanager:
    image: flink:1.18.0
    depends_on:
      - jobmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
```

```bash
docker-compose up -d
```

---

## ğŸ† Interview Questions (Beginner Level)

1. **What is the difference between event time and processing time?**
2. **What are watermarks and why are they needed?**
3. **Explain tumbling vs sliding vs session windows.**
4. **How does Flink handle late-arriving events?**
5. **What is keyed state in Flink?**

---

## ğŸ“ Summary Checklist

- [ ] Understand bounded vs unbounded data
- [ ] Explain event time vs processing time
- [ ] Implement watermark strategies
- [ ] Use tumbling, sliding, and session windows
- [ ] Write basic Flink DataStream programs
- [ ] Deploy Flink application on local cluster
- [ ] Answer beginner interview questions

**Next:** [INTERMEDIATE.md â†’](INTERMEDIATE.md)
