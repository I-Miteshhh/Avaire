# Week 12-13: Lakehouse Architecture â€” EXPERT Track

*"Apache Hudi's merge-on-read with incremental processing changed the game for real-time analytics at Uber. Understanding COW vs MOR, compaction strategies, and indexing is critical for L7+ engineers."* â€” Principal Engineer, Uber

---

## ğŸ¯ Learning Outcomes

- Master Apache Hudi copy-on-write (COW) vs merge-on-read (MOR) architectures
- Implement incremental processing with Hudi timeline server
- Design compaction strategies for MOR tables
- Optimize query performance with indexing (Bloom filters, HBase, record-level)
- Implement CDC pipelines with Hudi DeltaStreamer
- Handle small file problems with clustering
- Design multi-table transactions with Iceberg
- Tune lakehouse performance for 10+ PB scale (Netflix, Uber case studies)

---

## ğŸ”· 1. Apache Hudi Deep Dive

### Copy-on-Write (COW) vs Merge-on-Read (MOR)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Copy-on-Write (COW)                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Write Path:                                                    â”‚
â”‚ 1. Read existing Parquet file (if update)                     â”‚
â”‚ 2. Merge with new records                                     â”‚
â”‚ 3. Write new Parquet file (full rewrite)                      â”‚
â”‚ 4. Delete old file                                            â”‚
â”‚                                                                â”‚
â”‚ Storage:                                                       â”‚
â”‚ â”œâ”€ Only Parquet files (.parquet)                              â”‚
â”‚ â””â”€ No delta logs                                              â”‚
â”‚                                                                â”‚
â”‚ Read Path:                                                     â”‚
â”‚ â”œâ”€ Read Parquet files directly                                â”‚
â”‚ â””â”€ No merge required âœ… FAST                                  â”‚
â”‚                                                                â”‚
â”‚ Write Performance:                                             â”‚
â”‚ â”œâ”€ Slow (full file rewrite)                                   â”‚
â”‚ â””â”€ High I/O cost âŒ                                           â”‚
â”‚                                                                â”‚
â”‚ Use Case:                                                      â”‚
â”‚ â”œâ”€ Read-heavy workloads                                       â”‚
â”‚ â”œâ”€ Batch processing (hourly/daily)                            â”‚
â”‚ â””â”€ Append-mostly data                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Merge-on-Read (MOR)                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Write Path:                                                    â”‚
â”‚ 1. Write updates to delta log (.log file)                     â”‚
â”‚ 2. No file rewrite (append-only)                              â”‚
â”‚ 3. Periodic compaction merges deltas â†’ Parquet                â”‚
â”‚                                                                â”‚
â”‚ Storage:                                                       â”‚
â”‚ â”œâ”€ Base files: Parquet (.parquet)                             â”‚
â”‚ â””â”€ Delta files: Avro logs (.log)                              â”‚
â”‚                                                                â”‚
â”‚ Read Path (Two Views):                                        â”‚
â”‚ 1. Read Optimized (RO):                                       â”‚
â”‚    â”œâ”€ Read only Parquet files                                 â”‚
â”‚    â”œâ”€ Skip delta logs                                         â”‚
â”‚    â””â”€ Fast but stale âš ï¸                                       â”‚
â”‚                                                                â”‚
â”‚ 2. Real-Time (RT):                                            â”‚
â”‚    â”œâ”€ Read Parquet + delta logs                               â”‚
â”‚    â”œâ”€ Merge on-the-fly                                        â”‚
â”‚    â””â”€ Slow but fresh âœ…                                       â”‚
â”‚                                                                â”‚
â”‚ Write Performance:                                             â”‚
â”‚ â”œâ”€ Fast (append-only writes)                                  â”‚
â”‚ â””â”€ Low I/O cost âœ…                                            â”‚
â”‚                                                                â”‚
â”‚ Use Case:                                                      â”‚
â”‚ â”œâ”€ Write-heavy workloads                                      â”‚
â”‚ â”œâ”€ Real-time upserts (CDC)                                    â”‚
â”‚ â””â”€ Low-latency writes (<1 min)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### File Layout Example

```
Copy-on-Write (COW):
s3://bucket/hudi-table/
â”œâ”€ .hoodie/
â”‚  â”œâ”€ 20231015100000.commit
â”‚  â””â”€ 20231015110000.commit
â””â”€ partition_path=2023-10-15/
   â”œâ”€ abc123_0_20231015100000.parquet (Version 1)
   â””â”€ abc123_0_20231015110000.parquet (Version 2, replaced Version 1)

Merge-on-Read (MOR):
s3://bucket/hudi-table/
â”œâ”€ .hoodie/
â”‚  â”œâ”€ 20231015100000.deltacommit
â”‚  â”œâ”€ 20231015101000.deltacommit
â”‚  â””â”€ 20231015110000.commit (compaction)
â””â”€ partition_path=2023-10-15/
   â”œâ”€ abc123_0_20231015100000.parquet (Base file)
   â”œâ”€ .abc123_0_20231015100000.log.1 (Delta 1)
   â”œâ”€ .abc123_0_20231015100000.log.2 (Delta 2)
   â””â”€ abc123_0_20231015110000.parquet (Compacted base file)
```

---

### Creating COW vs MOR Tables

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog") \
    .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \
    .getOrCreate()

# Copy-on-Write Table
spark.sql("""
    CREATE TABLE events_cow (
        event_id BIGINT,
        event_time TIMESTAMP,
        user_id BIGINT,
        data STRING
    )
    USING hudi
    TBLPROPERTIES (
        type = 'cow',
        primaryKey = 'event_id',
        preCombineField = 'event_time'
    )
    PARTITIONED BY (event_date DATE)
    LOCATION 's3://bucket/events_cow'
""")

# Merge-on-Read Table
spark.sql("""
    CREATE TABLE events_mor (
        event_id BIGINT,
        event_time TIMESTAMP,
        user_id BIGINT,
        data STRING
    )
    USING hudi
    TBLPROPERTIES (
        type = 'mor',
        primaryKey = 'event_id',
        preCombineField = 'event_time'
    )
    PARTITIONED BY (event_date DATE)
    LOCATION 's3://bucket/events_mor'
""")
```

---

## âš™ï¸ 2. Compaction Strategies

### Inline vs Async Compaction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inline Compaction                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Behavior:                                                  â”‚
â”‚ â”œâ”€ Compaction runs DURING write                           â”‚
â”‚ â”œâ”€ Blocks write until compaction completes                â”‚
â”‚ â””â”€ Configurable interval (every N commits)                â”‚
â”‚                                                            â”‚
â”‚ Pros:                                                      â”‚
â”‚ â”œâ”€ Simple (no separate job)                               â”‚
â”‚ â””â”€ Automatic compaction                                   â”‚
â”‚                                                            â”‚
â”‚ Cons:                                                      â”‚
â”‚ â”œâ”€ Slow writes (compaction overhead)                      â”‚
â”‚ â””â”€ Resource contention (write + compact)                  â”‚
â”‚                                                            â”‚
â”‚ Use Case:                                                  â”‚
â”‚ â””â”€ Low write volume (<10 GB/hour)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Async Compaction                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Behavior:                                                  â”‚
â”‚ â”œâ”€ Compaction scheduled by writer                         â”‚
â”‚ â”œâ”€ Executed by separate compaction job                    â”‚
â”‚ â””â”€ Does NOT block writes                                  â”‚
â”‚                                                            â”‚
â”‚ Pros:                                                      â”‚
â”‚ â”œâ”€ Fast writes (no compaction overhead)                   â”‚
â”‚ â”œâ”€ Dedicated resources for compaction                     â”‚
â”‚ â””â”€ Scalable (parallel compaction)                         â”‚
â”‚                                                            â”‚
â”‚ Cons:                                                      â”‚
â”‚ â”œâ”€ Operational complexity (separate job)                  â”‚
â”‚ â””â”€ Potential lag (if compactor slow)                      â”‚
â”‚                                                            â”‚
â”‚ Use Case:                                                  â”‚
â”‚ â””â”€ High write volume (>100 GB/hour)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Compaction Configuration

```python
# Inline compaction (simple)
hudi_options = {
    'hoodie.table.name': 'events_mor',
    'hoodie.datasource.write.table.type': 'MERGE_ON_READ',
    'hoodie.datasource.write.operation': 'upsert',
    'hoodie.datasource.write.recordkey.field': 'event_id',
    'hoodie.datasource.write.precombine.field': 'event_time',
    'hoodie.datasource.write.partitionpath.field': 'event_date',
    
    # Inline compaction config
    'hoodie.compact.inline': 'true',
    'hoodie.compact.inline.max.delta.commits': '5',  # Compact every 5 commits
    'hoodie.compaction.strategy': 'org.apache.hudi.table.action.compact.strategy.LogFileSizeBasedCompactionStrategy',
    'hoodie.compaction.logfile.size.threshold': '1073741824',  # 1 GB
}

df.write.format('hudi').options(**hudi_options).mode('append').save('s3://bucket/events_mor')

# Async compaction (production)
hudi_options_async = {
    'hoodie.table.name': 'events_mor',
    'hoodie.datasource.write.table.type': 'MERGE_ON_READ',
    'hoodie.datasource.write.operation': 'upsert',
    'hoodie.datasource.write.recordkey.field': 'event_id',
    'hoodie.datasource.write.precombine.field': 'event_time',
    'hoodie.datasource.write.partitionpath.field': 'event_date',
    
    # Async compaction config
    'hoodie.compact.inline': 'false',  # Disable inline
    'hoodie.compact.schedule.inline': 'true',  # Schedule only
    'hoodie.compact.inline.max.delta.commits': '10',
}

# Writer: Schedule compaction
df.write.format('hudi').options(**hudi_options_async).mode('append').save('s3://bucket/events_mor')

# Separate Compactor Job:
from pyspark.sql import SparkSession

compactor = SparkSession.builder \
    .appName("HudiCompactor") \
    .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \
    .getOrCreate()

compactor.sql("""
    CALL run_compaction(
        table => 'events_mor',
        path => 's3://bucket/events_mor'
    )
""")
```

---

### Compaction Strategies

```
1. LogFileSizeBasedCompactionStrategy:
   â”œâ”€ Trigger: Total log file size > threshold
   â”œâ”€ Config: hoodie.compaction.logfile.size.threshold = 1 GB
   â””â”€ Use case: Balanced (default)

2. UnboundedCompactionStrategy:
   â”œâ”€ Trigger: Every scheduled commit
   â”œâ”€ Compacts ALL file groups
   â””â”€ Use case: Aggressive compaction (small tables)

3. BoundedPartitionAwareCompactionStrategy:
   â”œâ”€ Trigger: Based on partition age
   â”œâ”€ Config: hoodie.compaction.strategy.daybased.lookback.partitions = 3
   â”œâ”€ Compacts only recent partitions (last 3 days)
   â””â”€ Use case: Large tables (>10 TB)

Example:
hudi_options = {
    'hoodie.compaction.strategy': 'org.apache.hudi.table.action.compact.strategy.BoundedPartitionAwareCompactionStrategy',
    'hoodie.compaction.strategy.daybased.lookback.partitions': '3',
    'hoodie.compaction.daybased.target.io': '5368709120',  # 5 GB target I/O
}
```

---

## ğŸ“‡ 3. Indexing for Fast Upserts

### Index Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Index Type      â”‚ Lookup Time  â”‚ Storage     â”‚ Use Case       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BLOOM           â”‚ O(files)     â”‚ Metadata    â”‚ Small tables   â”‚
â”‚                 â”‚ ~100ms       â”‚ (~1% size)  â”‚ (<100 GB)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SIMPLE (Global) â”‚ O(1)         â”‚ Spark state â”‚ Medium tables  â”‚
â”‚                 â”‚ ~10ms        â”‚ (in-memory) â”‚ (100 GB-1 TB)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HBASE           â”‚ O(1)         â”‚ HBase       â”‚ Large tables   â”‚
â”‚                 â”‚ ~5ms         â”‚ (external)  â”‚ (>1 TB)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BUCKET          â”‚ O(1)         â”‚ Metadata    â”‚ Known dist.    â”‚
â”‚                 â”‚ ~1ms         â”‚ (minimal)   â”‚ (hash buckets) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Bloom Filter Index

```python
# Bloom filter index (default for COW/MOR)
hudi_options = {
    'hoodie.index.type': 'BLOOM',
    'hoodie.bloom.index.parallelism': '100',
    'hoodie.bloom.index.prune.by.ranges': 'true',  # Prune by column ranges
    'hoodie.bloom.index.use.metadata': 'true',  # Use metadata table
    
    # Bloom filter tuning
    'hoodie.index.bloom.num_entries': '60000',  # Expected entries per file
    'hoodie.index.bloom.fpp': '0.000000001',  # False positive probability (1 in 1B)
}

# How it works:
# 1. Upsert 1000 records with keys [k1, k2, ..., k1000]
# 2. For each file:
#    â”œâ”€ Check bloom filter: "Does file contain k1?"
#    â”œâ”€ If NO â†’ Skip file âœ…
#    â””â”€ If MAYBE â†’ Read file, check actual keys
# 3. Only read files with matching keys
# 4. Merge updates, write new files
```

---

### HBase Index

```python
# HBase index for large-scale upserts
hudi_options = {
    'hoodie.index.type': 'HBASE',
    'hoodie.index.hbase.zkquorum': 'zk1:2181,zk2:2181,zk3:2181',
    'hoodie.index.hbase.zkport': '2181',
    'hoodie.index.hbase.table': 'hudi_index_events',
    'hoodie.index.hbase.zknode.path': '/hbase',
    
    # HBase tuning
    'hoodie.index.hbase.put.batch.size': '100',
    'hoodie.index.hbase.get.batch.size': '100',
}

# How it works:
# 1. Upsert 1M records with keys [k1, k2, ..., k1000000]
# 2. Batch lookup in HBase (100 keys at a time):
#    HBase: k1 â†’ partition=2023-10-15, file=abc123.parquet
# 3. Group updates by file
# 4. Read only affected files (~100 files instead of 10,000)
# 5. Merge and write

# Throughput:
# â”œâ”€ Bloom: 10K upserts/sec
# â””â”€ HBase: 100K upserts/sec âœ…
```

---

### Bucket Index (Hash-Based)

```python
# Bucket index for uniform distribution
hudi_options = {
    'hoodie.index.type': 'BUCKET',
    'hoodie.index.bucket.engine': 'CONSISTENT_HASHING',
    'hoodie.bucket.index.num.buckets': '256',  # Fixed bucket count
    'hoodie.bucket.index.hash.field': 'user_id',  # Hash on user_id
}

# How it works:
# 1. Hash user_id to bucket: bucket = hash(user_id) % 256
# 2. Each bucket = 1 file group
# 3. Upsert record with user_id=12345:
#    â”œâ”€ bucket = hash(12345) % 256 = 45
#    â””â”€ Write to bucket 45 file
# 4. No index lookup needed! O(1) âœ…

# Requirements:
# â”œâ”€ Uniform key distribution (hash works well)
# â”œâ”€ Fixed bucket count (no auto-scaling)
# â””â”€ Single hash key (no composite keys)
```

---

## ğŸš° 4. Incremental Processing

### Hudi Timeline and Incremental Queries

```
Hudi Timeline = Ordered sequence of commits

Timeline:
â”œâ”€ 20231015100000.commit (1000 records inserted)
â”œâ”€ 20231015110000.commit (500 records updated)
â”œâ”€ 20231015120000.commit (200 records deleted)
â””â”€ 20231015130000.commit (1500 records inserted)

Incremental Query:
"Give me all changes since commit 20231015100000"

Result:
â”œâ”€ 500 updated records (from commit 110000)
â”œâ”€ 200 deleted records (from commit 120000)
â””â”€ 1500 inserted records (from commit 130000)
Total: 2200 records (instead of scanning full table)
```

**Implementation:**

```python
# Initial read (full table scan)
df = spark.read.format('hudi').load('s3://bucket/events')

# Get latest commit
latest_commit = spark.read.format('hudi') \
    .option('hoodie.datasource.query.type', 'snapshot') \
    .load('s3://bucket/events') \
    .selectExpr('max(_hoodie_commit_time) as max_commit') \
    .collect()[0]['max_commit']

print(f"Latest commit: {latest_commit}")

# Incremental query (only changes since last commit)
incremental_df = spark.read.format('hudi') \
    .option('hoodie.datasource.query.type', 'incremental') \
    .option('hoodie.datasource.read.begin.instanttime', '20231015100000') \
    .option('hoodie.datasource.read.end.instanttime', latest_commit) \
    .load('s3://bucket/events')

print(f"Incremental rows: {incremental_df.count()}")

# Process only changed data
incremental_df.write.format('delta').mode('append').save('s3://bucket/processed')
```

---

### DeltaStreamer for CDC

```python
# Hudi DeltaStreamer: Incremental ingestion from Kafka

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("HudiDeltaStreamer") \
    .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \
    .getOrCreate()

# DeltaStreamer config
deltastreamer_config = {
    'hoodie.datasource.write.recordkey.field': 'id',
    'hoodie.datasource.write.partitionpath.field': 'date',
    'hoodie.datasource.write.precombine.field': 'updated_at',
    'hoodie.datasource.write.table.name': 'users',
    'hoodie.datasource.write.operation': 'upsert',
    'hoodie.datasource.write.table.type': 'MERGE_ON_READ',
    
    # Kafka source
    'hoodie.deltastreamer.source.kafka.topic': 'users_cdc',
    'hoodie.deltastreamer.source.kafka.bootstrap.servers': 'kafka:9092',
    'hoodie.deltastreamer.source.kafka.value.deserializer.class': 'io.confluent.kafka.serializers.KafkaAvroDeserializer',
    'hoodie.deltastreamer.schema.registry.url': 'http://schema-registry:8081',
    
    # Checkpoint
    'hoodie.deltastreamer.checkpoint.provider.path': 's3://bucket/checkpoints/users',
    
    # Compaction
    'hoodie.compact.inline': 'false',
    'hoodie.compact.schedule.inline': 'true',
    'hoodie.compact.inline.max.delta.commits': '5',
}

# Run DeltaStreamer
spark.sparkContext.setSystemProperty('hoodie.deltastreamer.config.file', 'deltastreamer.properties')

spark.sql("""
    CALL run_deltastreamer(
        table_name => 'users',
        table_type => 'MERGE_ON_READ',
        base_path => 's3://bucket/users',
        source_class => 'org.apache.hudi.utilities.sources.JsonKafkaSource',
        source_ordering_field => 'updated_at',
        target_table => 'users',
        target_base_path => 's3://bucket/users',
        props => 'deltastreamer.properties'
    )
""")
```

**Pipeline:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MySQL CDC   â”‚ â”€â”€â”€>  â”‚ Kafka (Debezium)â”‚ â”€â”€â”€>  â”‚ Hudi Table  â”‚
â”‚ (binlog)    â”‚       â”‚ (JSON/Avro)     â”‚       â”‚ (MOR)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                      â”‚                         â”‚
      â–¼                      â–¼                         â–¼
   INSERT                 Produce                  Upsert
   UPDATE                 {                        (merge)
   DELETE                   "op": "u",
                           "before": {...},
                           "after": {...}
                         }

Latency: <1 minute (Kafka â†’ Hudi)
Throughput: 100K events/sec
```

---

## ğŸ† Production Case Studies

### Uber: Trip Data Processing

```
Requirements:
â”œâ”€ 100M trips/day
â”œâ”€ Real-time updates (ETA, fare changes)
â”œâ”€ Historical analysis (3 years retention)
â””â”€ Sub-minute freshness

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source: Kafka (trip events)                              â”‚
â”‚ â”œâ”€ trip_started, trip_updated, trip_completed            â”‚
â”‚ â””â”€ 1M events/hour                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hudi DeltaStreamer (MOR table)                           â”‚
â”‚ â”œâ”€ Upsert by trip_id (primary key)                      â”‚
â”‚ â”œâ”€ Partitioned by date                                  â”‚
â”‚ â”œâ”€ HBase index (fast lookups)                           â”‚
â”‚ â””â”€ Async compaction (every hour)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hudi Table: trips (10 PB)                                â”‚
â”‚ â”œâ”€ Real-time view: Fresh data (<1 min lag)              â”‚
â”‚ â”œâ”€ Read-optimized view: Compacted data (1 hour lag)     â”‚
â”‚ â””â”€ Incremental queries: Only changed trips               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Consumers:                                               â”‚
â”‚ â”œâ”€ Presto (real-time analytics)                         â”‚
â”‚ â”œâ”€ Spark (ML training on read-optimized)                â”‚
â”‚ â””â”€ Hive (batch ETL on incremental)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Configuration:
- Table type: MERGE_ON_READ
- Index: HBASE (HBase cluster with 50 nodes)
- Compaction: Async (dedicated Spark cluster)
- Partitions: Daily (365 partitions/year)
- File size target: 512 MB
- Retention: 3 years (automated cleanup)
```

---

### Netflix: Content Metadata (Iceberg)

```
Requirements:
â”œâ”€ 10M content items (movies, series, episodes)
â”œâ”€ 1000 updates/sec (metadata changes)
â”œâ”€ Query across 100+ tables (JOIN optimization)
â””â”€ Multi-region replication

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Iceberg Tables (100+ tables)                      â”‚
â”‚ â”œâ”€ content_metadata (10M rows)                          â”‚
â”‚ â”œâ”€ viewing_history (1B rows)                            â”‚
â”‚ â”œâ”€ recommendations (500M rows)                           â”‚
â”‚ â””â”€ user_profiles (200M rows)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Features Used:                                           â”‚
â”‚ 1. Hidden Partitioning:                                 â”‚
â”‚    PARTITIONED BY (days(updated_at))                    â”‚
â”‚    â†’ Users query: WHERE updated_at = '2023-10-15'       â”‚
â”‚    â†’ Iceberg auto-prunes partitions                     â”‚
â”‚                                                          â”‚
â”‚ 2. Snapshot Isolation:                                  â”‚
â”‚    â”œâ”€ Read-after-write consistency                      â”‚
â”‚    â”œâ”€ Time travel for auditing                          â”‚
â”‚    â””â”€ Rollback on bad writes                            â”‚
â”‚                                                          â”‚
â”‚ 3. Schema Evolution:                                    â”‚
â”‚    â”œâ”€ Add columns without downtime                      â”‚
â”‚    â”œâ”€ Rename columns (column ID mapping)                â”‚
â”‚    â””â”€ Backward compatible reads                         â”‚
â”‚                                                          â”‚
â”‚ 4. Multi-Engine Support:                                â”‚
â”‚    â”œâ”€ Spark (ETL)                                       â”‚
â”‚    â”œâ”€ Presto (ad-hoc queries)                           â”‚
â”‚    â”œâ”€ Flink (real-time updates)                         â”‚
â”‚    â””â”€ Trino (federated queries)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Storage:
â”œâ”€ S3 (primary: 10 PB)
â”œâ”€ Multi-region replication (us-east-1, eu-west-1)
â””â”€ Intelligent tiering (Glacier for old data)

Performance:
â”œâ”€ Query latency: <1 second (P50), <5 seconds (P99)
â”œâ”€ Write throughput: 1M rows/minute
â””â”€ Concurrent readers: 1000+ Spark jobs
```

---

## ğŸ† Interview Questions (Expert Level)

1. **Compare Hudi COW vs MOR. When would you choose each for a real-time analytics platform?**
2. **Explain async compaction in Hudi. How would you tune it for a 10 TB table with 1M upserts/hour?**
3. **What is the HBase index? How does it achieve O(1) lookups for upserts?**
4. **Design a CDC pipeline from PostgreSQL to Hudi with sub-minute latency. What are the bottlenecks?**
5. **Explain Iceberg's three-level metadata. Why is it more scalable than Delta Lake's JSON log?**
6. **How would you handle small file problems in a lakehouse with 100K partitions?**
7. **Compare bloom filter vs HBase vs bucket indexing. Which would you use for 1B row table?**
8. **Design a multi-table transaction in Iceberg. How does it ensure atomicity?**
9. **Explain partition evolution in Iceberg. How does it work without rewriting data?**
10. **You have a Hudi MOR table with 1000 log files per partition. Read queries are slow. How do you fix it?**

---

## ğŸ“ Summary Checklist

- [ ] Understand Hudi COW vs MOR architectures
- [ ] Implement compaction strategies (inline vs async)
- [ ] Configure indexing (Bloom, HBase, Bucket)
- [ ] Design incremental processing pipelines
- [ ] Implement CDC with Hudi DeltaStreamer
- [ ] Optimize for 10+ PB scale (Uber, Netflix patterns)
- [ ] Handle schema evolution and partition evolution
- [ ] Answer expert-level interview questions

**Next:** [WHITEPAPERS.md â†’](WHITEPAPERS.md)
