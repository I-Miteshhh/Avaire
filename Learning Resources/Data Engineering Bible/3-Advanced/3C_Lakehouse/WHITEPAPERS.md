# Week 12-13: Lakehouse Architecture â€” WHITEPAPERS

*"Reading the Delta Lake protocol, Iceberg spec, and Hudi design papers is essential for understanding the trade-offs. These aren't just formatsâ€”they're distributed systems solving different problems."* â€” Distinguished Engineer, Databricks

---

## ğŸ“š Paper 1: Delta Lake Protocol (Databricks, 2019)

**Link:** https://github.com/delta-io/delta/blob/master/PROTOCOL.md

### Abstract

Delta Lake provides ACID transactions on top of object stores (S3, ADLS, GCS) by maintaining a transaction log as a sequence of JSON files. The protocol defines:
1. How to read/write transaction logs atomically
2. Optimistic concurrency control for conflicting writes
3. Checkpoint mechanism for fast metadata access

---

### Core Concepts

#### 1. Transaction Log Structure

```
_delta_log/
â”œâ”€ 00000000000000000000.json  (Version 0: CREATE TABLE)
â”œâ”€ 00000000000000000001.json  (Version 1: INSERT)
â”œâ”€ 00000000000000000002.json  (Version 2: UPDATE)
â”œâ”€ 00000000000000000003.json  (Version 3: DELETE)
â”œâ”€ ...
â”œâ”€ 00000000000000000010.checkpoint.parquet  (Checkpoint at v10)
â””â”€ 00000000000000000020.json  (Version 20)

Each JSON file contains actions:
{
  "add": {
    "path": "part-00000.parquet",
    "size": 1024,
    "partitionValues": {"date": "2023-10-15"},
    "stats": "{\"numRecords\": 1000, \"minValues\": {...}, \"maxValues\": {...}}"
  }
}

{
  "remove": {
    "path": "part-00001.parquet",
    "deletionTimestamp": 1697385600000
  }
}
```

**Insight:** By listing files in sorted order (`00000.json`, `00001.json`, ...), Delta can reconstruct table state at any version by replaying actions.

---

#### 2. Optimistic Concurrency Control (OCC)

```
Writer 1:                          Writer 2:
â”œâ”€ Read latest version: v10       â”œâ”€ Read latest version: v10
â”œâ”€ Compute changes                â”œâ”€ Compute changes
â”œâ”€ Write v11.json âœ…              â”œâ”€ Write v11.json âŒ CONFLICT!
â””â”€ Success                         â””â”€ Retry as v12

Conflict Detection:
1. Writer reads current version (v10)
2. Writer computes changes locally
3. Writer attempts to write v11.json
4. S3 PUT with precondition:
   "if-none-match: v11.json does not exist"
5. If another writer created v11.json first:
   â”œâ”€ PUT fails (409 Conflict)
   â”œâ”€ Writer re-reads table (now v11)
   â”œâ”€ Re-computes changes
   â””â”€ Retries as v12

Conflict Resolution Rules:
â”œâ”€ APPEND + APPEND = No conflict (both succeed)
â”œâ”€ UPDATE + UPDATE (disjoint files) = No conflict
â”œâ”€ UPDATE + UPDATE (same files) = Conflict (retry)
â”œâ”€ DELETE + UPDATE (same files) = Conflict (retry)
â””â”€ Schema change + Data change = Conflict (retry)
```

**Insight:** Delta trades off availability (retry on conflict) for consistency (no lost updates).

---

#### 3. Checkpointing

```
Problem: Replaying 1M JSON files is slow

Solution: Checkpoint every 10 commits

_delta_log/
â”œâ”€ 00000000000000000000.json
â”œâ”€ ...
â”œâ”€ 00000000000000000010.checkpoint.parquet  â† Snapshot at v10
â”‚    (Contains aggregated state of all actions 0-10)
â”œâ”€ 00000000000000000011.json
â”œâ”€ ...
â”œâ”€ 00000000000000000020.checkpoint.parquet  â† Snapshot at v20
â””â”€ 00000000000000000025.json

Reading at v25:
1. Find latest checkpoint â‰¤ 25 â†’ v20.checkpoint.parquet
2. Load checkpoint (Parquet, fast)
3. Replay JSON files 21-25
4. Result: Table state at v25

Without checkpoint: Replay 0-25 (26 files)
With checkpoint: Replay 20-25 (6 files) âœ…
```

**Performance:**
- Checkpoint read: 100 ms
- JSON replay (10 files): 50 ms
- **Total: 150 ms** (vs 2.6 seconds without checkpoint)

---

#### 4. Stats Pruning

```json
{
  "add": {
    "path": "date=2023-10-15/part-00000.parquet",
    "size": 1048576,
    "partitionValues": {"date": "2023-10-15"},
    "stats": {
      "numRecords": 1000,
      "minValues": {"user_id": 1, "timestamp": "2023-10-15T00:00:00Z"},
      "maxValues": {"user_id": 5000, "timestamp": "2023-10-15T23:59:59Z"}
    }
  }
}
```

**Query Pruning:**

```sql
SELECT * FROM events WHERE user_id = 12345 AND date = '2023-10-15'
```

Delta optimizer:
1. Partition pruning: `date = '2023-10-15'` â†’ Only read partition `date=2023-10-15/`
2. Stats pruning: `user_id = 12345` â†’ Check min/max:
   - File 1: min=1, max=5000 â†’ âœ… Read (12345 in range)
   - File 2: min=6000, max=10000 â†’ âŒ Skip (12345 not in range)

**Result:** Read 1 file instead of 10 files âœ…

---

### Critical Insights

1. **S3 Consistency Challenges:**
   - Old S3 (pre-2020): Eventually consistent LIST
   - Delta protocol assumes atomic PUT (exists since S3 launch)
   - Solved with S3 Strong Consistency (Dec 2020)

2. **Why JSON for Transaction Log?**
   - Human-readable (debug-friendly)
   - Schema evolution support (add new fields)
   - Small size (<1 KB/commit)
   - Trade-off: Slower than binary (Parquet), but fast enough with checkpoints

3. **Comparison to Iceberg:**
   - Delta: Single JSON file per commit (simple, but scales to ~1M commits)
   - Iceberg: 3-level metadata (complex, but scales to ~100M snapshots)

---

## ğŸ“„ Paper 2: Apache Iceberg Table Format Specification (Netflix, 2018)

**Link:** https://iceberg.apache.org/spec/

### Abstract

Iceberg is a table format designed for **multi-petabyte** analytic datasets with:
1. Three-level metadata hierarchy (metadata.json â†’ manifest list â†’ manifest files)
2. Hidden partitioning with automatic partition pruning
3. Time travel via immutable snapshots
4. Schema evolution with backward compatibility

---

### Core Architecture

#### 1. Three-Level Metadata

```
s3://bucket/my_table/
â”œâ”€ metadata/
â”‚  â”œâ”€ v1.metadata.json             â† Version 1
â”‚  â”œâ”€ v2.metadata.json             â† Version 2
â”‚  â”œâ”€ snap-12345.avro              â† Snapshot 12345 manifest list
â”‚  â”œâ”€ snap-12346.avro              â† Snapshot 12346 manifest list
â”‚  â”œâ”€ manifest-abc123.avro         â† Manifest file (file-level metadata)
â”‚  â””â”€ manifest-def456.avro
â””â”€ data/
   â”œâ”€ part-00000.parquet
   â””â”€ part-00001.parquet

Level 1: metadata.json (Pointer to current snapshot)
{
  "format-version": 2,
  "table-uuid": "abc-123",
  "current-snapshot-id": 12346,
  "snapshots": [
    {"snapshot-id": 12345, "manifest-list": "snap-12345.avro"},
    {"snapshot-id": 12346, "manifest-list": "snap-12346.avro"}
  ]
}

Level 2: snap-12346.avro (Manifest list)
[
  {"manifest_path": "manifest-abc123.avro", "added_files": 10},
  {"manifest_path": "manifest-def456.avro", "added_files": 5}
]

Level 3: manifest-abc123.avro (Manifest file)
[
  {
    "file_path": "data/part-00000.parquet",
    "file_size_in_bytes": 1048576,
    "partition": {"date": 18915},  # days since epoch
    "record_count": 1000,
    "column_sizes": {...},
    "value_counts": {...},
    "null_value_counts": {...},
    "lower_bounds": {"user_id": 1},
    "upper_bounds": {"user_id": 5000}
  }
]
```

**Why 3 Levels?**

- **Level 1 (metadata.json):** Small (<10 KB), atomic updates via S3 PUT
- **Level 2 (manifest list):** Reusable across snapshots (unchanged manifests reused)
- **Level 3 (manifest files):** File-level stats for pruning

**Scalability:**
- Delta Lake: 1M commits â†’ 1M JSON files (~1 GB metadata)
- Iceberg: 100M snapshots â†’ 1 metadata.json + 100M manifest entries (~10 GB metadata) âœ…

---

#### 2. Hidden Partitioning

```sql
-- Traditional partitioning (Hive-style)
CREATE TABLE events (
    event_id BIGINT,
    event_time TIMESTAMP,
    user_id BIGINT
)
PARTITIONED BY (event_date DATE);  -- User must provide partition value

INSERT INTO events PARTITION(event_date = '2023-10-15')
VALUES (1, '2023-10-15 10:00:00', 100);

-- Iceberg hidden partitioning
CREATE TABLE events (
    event_id BIGINT,
    event_time TIMESTAMP,
    user_id BIGINT
)
PARTITIONED BY (days(event_time));  -- Transform function

INSERT INTO events VALUES (1, '2023-10-15 10:00:00', 100);
-- Iceberg automatically computes: days('2023-10-15 10:00:00') = 18915
```

**Transform Functions:**

```
1. days(timestamp_col) â†’ Integer (days since 1970-01-01)
   '2023-10-15 10:30:00' â†’ 18915

2. months(timestamp_col) â†’ Integer (months since 1970-01)
   '2023-10-15' â†’ 645

3. years(timestamp_col) â†’ Integer (years since 1970)
   '2023-10-15' â†’ 53

4. hours(timestamp_col) â†’ Integer (hours since 1970-01-01 00:00:00)
   '2023-10-15 10:30:00' â†’ 454584

5. bucket(N, col) â†’ Integer (hash(col) % N)
   bucket(10, user_id) â†’ hash(12345) % 10 = 5

6. truncate(width, col) â†’ Same type (round down)
   truncate(10, user_id) â†’ user_id = 12345 â†’ 12340
```

**Automatic Partition Pruning:**

```sql
SELECT * FROM events WHERE event_time BETWEEN '2023-10-15' AND '2023-10-16'

Iceberg optimizer:
1. Compute partition range:
   days('2023-10-15') = 18915
   days('2023-10-16') = 18916
2. Scan manifests for partitions [18915, 18916]
3. Skip all other partitions âœ…

Result: Read 2 days instead of 365 days
```

---

#### 3. Partition Evolution

```sql
-- Initial: Partition by day
CREATE TABLE events PARTITIONED BY (days(event_time));

-- Insert data for 2023
INSERT INTO events VALUES (...);  -- Partitioned by day

-- After 1 year: Too many partitions (365 partitions/year)
-- Change to monthly partitioning
ALTER TABLE events SET PARTITION SPEC (months(event_time));

-- Insert data for 2024
INSERT INTO events VALUES (...);  -- Partitioned by month

-- Query works across both partition schemes!
SELECT * FROM events WHERE event_time >= '2023-01-01'
-- Iceberg reads:
-- â”œâ”€ 2023 data: daily partitions (365 partitions)
-- â””â”€ 2024 data: monthly partitions (12 partitions)
```

**How it works:**

```
metadata.json:
{
  "partition-specs": [
    {"spec-id": 0, "fields": [{"name": "event_day", "transform": "days", "source-id": 2}]},
    {"spec-id": 1, "fields": [{"name": "event_month", "transform": "months", "source-id": 2}]}
  ],
  "default-spec-id": 1
}

Manifest files:
manifest-2023.avro:
  file: data/2023-10-15/part-00000.parquet, spec-id: 0, partition: {event_day: 18915}

manifest-2024.avro:
  file: data/2024-10/part-00000.parquet, spec-id: 1, partition: {event_month: 660}

Query planner:
1. For spec-id=0 files: Use days(event_time) for pruning
2. For spec-id=1 files: Use months(event_time) for pruning
3. Combine results
```

**Delta/Hudi limitation:** Changing partitioning requires full table rewrite âŒ

---

#### 4. Schema Evolution with Column IDs

```
Problem: Rename column without breaking old files

Hive/Delta approach:
â”œâ”€ Column identified by NAME
â”œâ”€ Rename "user_id" â†’ "customer_id"
â””â”€ Old files still have "user_id" â†’ Need rewrite âŒ

Iceberg approach:
â”œâ”€ Column identified by ID (immutable)
â”œâ”€ Schema mapping:
â”‚  v1: {1: "user_id", 2: "event_time"}
â”‚  v2: {1: "customer_id", 2: "event_time"}  â† Rename
â””â”€ Old files: Read field ID=1 as "customer_id" âœ…
```

**Implementation:**

```json
// Schema v1
{
  "schema-id": 0,
  "fields": [
    {"id": 1, "name": "user_id", "type": "long"},
    {"id": 2, "name": "event_time", "type": "timestamp"}
  ]
}

// Schema v2 (after rename)
{
  "schema-id": 1,
  "fields": [
    {"id": 1, "name": "customer_id", "type": "long"},  // Same ID!
    {"id": 2, "name": "event_time", "type": "timestamp"}
  ]
}

// Old Parquet file (written with schema v1):
// Column 0: user_id (maps to ID=1)
// Column 1: event_time (maps to ID=2)

// Reading with schema v2:
// Query: SELECT customer_id FROM events
// Iceberg: Read field ID=1 from Parquet (column 0) âœ…
```

---

### Critical Insights

1. **Manifest Reuse:**
   - When writing new data, Iceberg reuses unchanged manifest files
   - Example: Insert to partition `2023-10-15`
     - Manifest for partition `2023-10-14` unchanged â†’ Reuse
     - Only create new manifest for `2023-10-15`
   - Result: Faster commits (less metadata I/O)

2. **Optimistic Concurrency:**
   - Similar to Delta: Atomic S3 PUT on `metadata.json`
   - Conflict detection: Check snapshot ID
   - Retry on conflict

3. **Comparison to Delta:**
   - Iceberg: Better scalability (100M+ snapshots)
   - Delta: Simpler (single JSON log)
   - Trade-off: Complexity vs simplicity

---

## ğŸ“˜ Paper 3: Apache Hudi Design Paper (Uber, 2016)

**Link:** https://www.uber.com/blog/hoodie/

### Abstract

Hudi (Hadoop Upserts Deletes Incrementals) is designed for:
1. **Low-latency upserts** on data lakes
2. **Incremental processing** (consume only changes)
3. **Minute-level freshness** (real-time analytics)

---

### Core Architecture

#### 1. Timeline-Based Design

```
Hudi Timeline = Ordered sequence of instants

Instant types:
â”œâ”€ REQUESTED: Action requested (not started)
â”œâ”€ INFLIGHT: Action in progress
â””â”€ COMPLETED: Action finished

Example timeline:
â”œâ”€ 20231015100000.commit.requested
â”œâ”€ 20231015100000.commit.inflight
â”œâ”€ 20231015100000.commit (completed)
â”œâ”€ 20231015110000.deltacommit.inflight
â”œâ”€ 20231015110000.deltacommit (completed)
â”œâ”€ 20231015120000.compaction.requested
â””â”€ 20231015120000.compaction.inflight

Timeline enables:
1. Incremental queries: "Give me commits 100000 to 110000"
2. Rollback: Delete inflight commits
3. Crash recovery: Resume inflight commits
```

---

#### 2. Copy-on-Write vs Merge-on-Read (Revisited)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Copy-on-Write (COW)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ File Layout:                                                 â”‚
â”‚ partition/                                                   â”‚
â”‚ â”œâ”€ abc123_0_20231015100000.parquet (base file v1)           â”‚
â”‚ â””â”€ abc123_0_20231015110000.parquet (base file v2, replaces v1)â”‚
â”‚                                                              â”‚
â”‚ Write:                                                       â”‚
â”‚ 1. Read existing file                                       â”‚
â”‚ 2. Merge with updates                                       â”‚
â”‚ 3. Write new file                                           â”‚
â”‚ 4. Delete old file                                          â”‚
â”‚                                                              â”‚
â”‚ Read:                                                        â”‚
â”‚ â””â”€ Read latest base files directly                          â”‚
â”‚                                                              â”‚
â”‚ Latency:                                                     â”‚
â”‚ â”œâ”€ Write: High (full rewrite)                               â”‚
â”‚ â””â”€ Read: Low (no merge needed)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Merge-on-Read (MOR)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ File Layout:                                                 â”‚
â”‚ partition/                                                   â”‚
â”‚ â”œâ”€ abc123_0_20231015100000.parquet (base file)              â”‚
â”‚ â”œâ”€ .abc123_0_20231015100000.log.1_0-1-0 (delta 1)           â”‚
â”‚ â”œâ”€ .abc123_0_20231015100000.log.2_0-1-0 (delta 2)           â”‚
â”‚ â””â”€ .abc123_0_20231015100000.log.3_0-1-0 (delta 3)           â”‚
â”‚                                                              â”‚
â”‚ Write:                                                       â”‚
â”‚ 1. Append updates to log file (Avro)                        â”‚
â”‚ 2. No base file rewrite                                     â”‚
â”‚ 3. Periodic compaction: Merge logs â†’ new base file          â”‚
â”‚                                                              â”‚
â”‚ Read (Two Views):                                           â”‚
â”‚ 1. Read Optimized (RO):                                     â”‚
â”‚    â””â”€ Read only base files (skip logs)                      â”‚
â”‚    â†’ Fast, but stale                                        â”‚
â”‚                                                              â”‚
â”‚ 2. Real-Time (RT):                                          â”‚
â”‚    â”œâ”€ Read base file                                        â”‚
â”‚    â”œâ”€ Read log files                                        â”‚
â”‚    â””â”€ Merge on-the-fly                                      â”‚
â”‚    â†’ Slow, but fresh                                        â”‚
â”‚                                                              â”‚
â”‚ Latency:                                                     â”‚
â”‚ â”œâ”€ Write: Low (append-only)                                 â”‚
â”‚ â”œâ”€ Read (RO): Low (skip logs)                               â”‚
â”‚ â””â”€ Read (RT): High (merge logs)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight:** MOR trades read latency (merge cost) for write latency (append-only).

---

#### 3. Indexing for Upserts

```
Problem: Upsert 1M records with keys [k1, k2, ..., k1000000]
         Table has 10,000 files
         How to find which files contain these keys?

Naive approach: Scan all 10,000 files âŒ
Hudi approach: Use index âœ…

Index Types:

1. Bloom Filter Index:
   â”œâ”€ Store bloom filter for each file (in metadata)
   â”œâ”€ Check: "Does file contain key k1?"
   â”œâ”€ False positives: Possible (1% FPP)
   â”œâ”€ False negatives: Never
   â””â”€ Lookup: O(files) = O(10,000)

2. HBase Index:
   â”œâ”€ External HBase table: key â†’ (partition, file_id)
   â”œâ”€ Lookup: O(1) via HBase Get
   â””â”€ Overhead: Maintain HBase cluster

3. Simple Index (In-Memory):
   â”œâ”€ Load all keys into Spark executor memory
   â”œâ”€ HashMap: key â†’ (partition, file_id)
   â””â”€ Limitation: Memory intensive (10 GB+ for 1B keys)
```

**Performance Comparison:**

```
Table: 1B rows, 10K files, 1M upserts

Bloom Filter Index:
â”œâ”€ Lookup: 10K bloom filter checks = 100 ms
â”œâ”€ False positives: 10K * 0.01 = 100 files to scan
â””â”€ Total: 100 ms + (100 files * 10 ms) = 1.1 seconds

HBase Index:
â”œâ”€ Batch lookup: 1M keys in batches of 1000
â”œâ”€ HBase Get latency: 5 ms/batch
â””â”€ Total: (1M / 1000) * 5 ms = 5 seconds

Simple Index:
â”œâ”€ Load keys into memory: 10 GB
â”œâ”€ HashMap lookup: O(1) = 0.1 ms
â””â”€ Total: 0.1 ms âœ… (but 10 GB memory)
```

---

#### 4. Incremental Processing

```
Use Case: Process only changed records

Traditional approach:
1. Scan full table (1B rows)
2. Filter by updated_at > last_checkpoint
3. Process filtered rows

Hudi approach:
1. Query incremental view:
   BEGIN_INSTANT = 20231015100000
   END_INSTANT = 20231015110000
2. Hudi returns only commits in range
3. Process only changed rows (10K rows instead of 1B)

Code:
df = spark.read.format('hudi') \
    .option('hoodie.datasource.query.type', 'incremental') \
    .option('hoodie.datasource.read.begin.instanttime', '20231015100000') \
    .option('hoodie.datasource.read.end.instanttime', '20231015110000') \
    .load('s3://bucket/hudi_table')

print(f"Incremental rows: {df.count()}")  # 10,000 rows
```

**Latency:**
- Full scan: 10 minutes (1B rows)
- Incremental: 5 seconds (10K rows) âœ…

---

### Critical Insights

1. **Timeline as First-Class Citizen:**
   - Hudi's timeline enables incremental processing natively
   - Delta/Iceberg: Possible via time travel, but not primary design goal
   - Hudi: Built for streaming use cases (Kafka â†’ Hudi â†’ Incremental consumers)

2. **DeltaStreamer for CDC:**
   - Hudi DeltaStreamer: Kafka â†’ Hudi in <1 minute
   - Handles schema evolution, deduplication, compaction
   - Production-ready (Uber runs 100+ DeltaStreamer jobs)

3. **Comparison to Delta/Iceberg:**
   - Hudi: Best for upsert-heavy + incremental processing
   - Delta: Best for Databricks ecosystem + batch processing
   - Iceberg: Best for multi-engine + partition evolution

---

## ğŸ”¬ Comparison Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature        â”‚ Delta Lake  â”‚ Iceberg     â”‚ Hudi        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ACID           â”‚ OCC (JSON)  â”‚ OCC (JSON)  â”‚ OCC (JSON)  â”‚
â”‚ Metadata       â”‚ 1-level     â”‚ 3-level     â”‚ Timeline    â”‚
â”‚ Partitioning   â”‚ Static      â”‚ Hidden/Evol â”‚ Static      â”‚
â”‚ Schema Evol    â”‚ Name-based  â”‚ ID-based    â”‚ Name-based  â”‚
â”‚ Indexing       â”‚ Stats only  â”‚ Stats only  â”‚ Bloom/HBase â”‚
â”‚ Incremental    â”‚ Time travel â”‚ Snapshots   â”‚ Native      â”‚
â”‚ Upsert         â”‚ MERGE       â”‚ MERGE       â”‚ Native      â”‚
â”‚ Compaction     â”‚ N/A (COW)   â”‚ N/A (COW)   â”‚ Async (MOR) â”‚
â”‚ Multi-Engine   â”‚ Limited     â”‚ Excellent   â”‚ Moderate    â”‚
â”‚ Maturity       â”‚ High        â”‚ High        â”‚ High        â”‚
â”‚ Scalability    â”‚ 1M commits  â”‚ 100M snaps  â”‚ 10M commits â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Key Takeaways

1. **Delta Lake:**
   - Simple, production-ready
   - Best for Databricks/Spark ecosystem
   - Limitation: Scalability (~1M commits)

2. **Iceberg:**
   - Highly scalable (100M+ snapshots)
   - Multi-engine support (Spark, Flink, Presto, Trino)
   - Hidden partitioning + partition evolution
   - Complexity: 3-level metadata

3. **Hudi:**
   - Built for upserts + incremental processing
   - MOR table type for low-latency writes
   - Timeline-based design
   - Indexing (Bloom, HBase) for fast upserts

4. **When to Choose:**
   - **Delta:** Databricks platform, batch-heavy, simple operations
   - **Iceberg:** Multi-engine, massive scale, partition evolution
   - **Hudi:** Real-time upserts, CDC, incremental consumers

**Next:** [README.md â†’](README.md)
