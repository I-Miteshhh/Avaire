# Week 12-13: Lakehouse Architecture â€” INTERMEDIATE Track

*"Iceberg's hidden partitioning and time travel implementation is brilliantâ€”partition pruning without exposing partition columns to users. This is the evolution beyond Hive."* â€” Staff Engineer, Netflix

---

## ğŸ¯ Learning Outcomes

By the end of this track, you will:
- Master Apache Iceberg table format and metadata architecture
- Implement hidden partitioning for automatic partition pruning
- Use Iceberg's snapshot isolation for time travel
- Compare Delta Lake, Iceberg, and Hudi architectures
- Optimize partition layouts for query performance
- Handle schema evolution with column mapping
- Implement partition evolution (change partitioning without rewriting data)
- Configure catalog integrations (Hive, Glue, Nessie)

---

## ğŸ§Š 1. Apache Iceberg Deep Dive

### Iceberg Metadata Architecture

```
Iceberg Table Structure:

s3://my-bucket/iceberg-table/
â”œâ”€ metadata/
â”‚  â”œâ”€ v1.metadata.json  â† Version 1 metadata
â”‚  â”œâ”€ v2.metadata.json  â† Version 2 metadata (current)
â”‚  â”œâ”€ snap-123.avro     â† Snapshot 123 manifest list
â”‚  â”œâ”€ snap-124.avro     â† Snapshot 124 manifest list
â”‚  â””â”€ manifest-*.avro   â† Manifest files (list of data files)
â”‚
â””â”€ data/
   â”œâ”€ date=2023-10-15/
   â”‚  â”œâ”€ part-00000.parquet
   â”‚  â””â”€ part-00001.parquet
   â””â”€ date=2023-10-16/
      â””â”€ part-00000.parquet

Three-Level Metadata Hierarchy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Metadata File (v2.metadata.json)                     â”‚
â”‚    â”œâ”€ Current snapshot ID: 124                          â”‚
â”‚    â”œâ”€ Schema (columns, types)                           â”‚
â”‚    â”œâ”€ Partition spec                                    â”‚
â”‚    â””â”€ Snapshot list: [123, 124]                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Manifest List (snap-124.avro)                        â”‚
â”‚    â”œâ”€ manifest-abc123.avro (partition: date=2023-10-15) â”‚
â”‚    â”œâ”€ manifest-def456.avro (partition: date=2023-10-16) â”‚
â”‚    â””â”€ Added files: 2, Deleted files: 0                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Manifest File (manifest-abc123.avro)                 â”‚
â”‚    â”œâ”€ part-00000.parquet (status: ADDED)                â”‚
â”‚    â”‚  â”œâ”€ size: 1 GB                                     â”‚
â”‚    â”‚  â”œâ”€ record_count: 1,000,000                        â”‚
â”‚    â”‚  â”œâ”€ partition: {date: 2023-10-15}                  â”‚
â”‚    â”‚  â””â”€ lower_bounds/upper_bounds (min/max values)     â”‚
â”‚    â””â”€ part-00001.parquet (status: ADDED)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Three Levels?**

```
Benefits:
1. Fast snapshot listing (metadata file only)
2. Partition pruning (manifest list level)
3. File-level stats (manifest file level)
4. Efficient time travel (swap metadata pointer)

Example Query:
SELECT * FROM events WHERE date = '2023-10-15' AND user_id = 123

Step 1: Read metadata file (v2.metadata.json)
â”œâ”€ Current snapshot: 124
â””â”€ Read manifest list: snap-124.avro

Step 2: Partition pruning (manifest list)
â”œâ”€ Filter manifests by partition: date=2023-10-15
â””â”€ Read only manifest-abc123.avro (skip manifest-def456.avro)

Step 3: File pruning (manifest file)
â”œâ”€ Check lower_bounds/upper_bounds for user_id
â”œâ”€ part-00000.parquet: user_id in [1, 500] â†’ Read
â””â”€ part-00001.parquet: user_id in [501, 1000] â†’ Skip

Result: Read 1 of 3 data files (66% reduction)
```

---

### Hidden Partitioning

```
Problem with Hive-style partitioning:

-- User must know partition column
SELECT * FROM events
WHERE year = 2023 AND month = 10 AND day = 15;  âŒ Tedious!

-- Without partition columns â†’ Full scan
SELECT * FROM events
WHERE event_date = '2023-10-15';  âŒ Slow!

Iceberg Solution: Hidden Partitioning

CREATE TABLE events (
  event_id BIGINT,
  event_date DATE,
  user_id BIGINT,
  data STRING
)
USING iceberg
PARTITIONED BY (days(event_date));  â† Partition function!

-- Query with natural column (no partition exposure)
SELECT * FROM events
WHERE event_date = '2023-10-15';  âœ… Automatic partition pruning!

-- Iceberg translates:
-- event_date = '2023-10-15' â†’ days(event_date) = 19650
-- â†’ Read only partition 19650
```

**Partition Transform Functions:**

```sql
-- Temporal transforms
PARTITIONED BY (years(timestamp_col))   -- Year partition
PARTITIONED BY (months(timestamp_col))  -- Month partition
PARTITIONED BY (days(date_col))         -- Day partition
PARTITIONED BY (hours(timestamp_col))   -- Hour partition

-- Identity transform
PARTITIONED BY (identity(region))       -- Exact value

-- Hash transform (for uniform distribution)
PARTITIONED BY (bucket(10, user_id))    -- 10 buckets by user_id

-- Truncate transform
PARTITIONED BY (truncate(5, price))     -- Truncate to nearest 5
```

**Example:**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "hadoop") \
    .config("spark.sql.catalog.spark_catalog.warehouse", "s3://my-bucket/warehouse") \
    .getOrCreate()

# Create table with hidden partitioning
spark.sql("""
    CREATE TABLE events (
        event_id BIGINT,
        event_time TIMESTAMP,
        user_id BIGINT,
        data STRING
    )
    USING iceberg
    PARTITIONED BY (days(event_time), bucket(100, user_id))
""")

# Insert data
spark.sql("""
    INSERT INTO events VALUES
    (1, TIMESTAMP '2023-10-15 10:00:00', 12345, 'login'),
    (2, TIMESTAMP '2023-10-15 11:00:00', 67890, 'purchase'),
    (3, TIMESTAMP '2023-10-16 09:00:00', 12345, 'logout')
""")

# Query with automatic partition pruning
result = spark.sql("""
    SELECT * FROM events
    WHERE event_time >= TIMESTAMP '2023-10-15 00:00:00'
      AND event_time < TIMESTAMP '2023-10-16 00:00:00'
      AND user_id = 12345
""")

# Iceberg automatically prunes:
# â”œâ”€ Partition: days(event_time) = 19650 (Oct 15)
# â””â”€ Partition: bucket(100, 12345) = 45
# Result: Reads only 1 of 100+ partitions
```

---

## ğŸ•°ï¸ 2. Snapshot Isolation and Time Travel

### Iceberg Snapshot Model

```
Snapshots = Immutable point-in-time table states

Timeline:
T1: Snapshot 100 (1000 files)
â”œâ”€ metadata: v10.metadata.json â†’ snap-100.avro
â”œâ”€ Added: 1000 files
â””â”€ Schema: v1

T2: INSERT (100 new files)
â”œâ”€ Snapshot 101 created
â”œâ”€ metadata: v11.metadata.json â†’ snap-101.avro
â”œâ”€ Added: 100 files (incremental)
â””â”€ Total files: 1100

T3: DELETE (remove 50 files)
â”œâ”€ Snapshot 102 created
â”œâ”€ metadata: v12.metadata.json â†’ snap-102.avro
â”œâ”€ Deleted: 50 files (marked as deleted)
â””â”€ Live files: 1050

Snapshot Lineage:
snap-100 â†’ snap-101 â†’ snap-102 (current)
   â”‚          â”‚           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   All snapshots retained (until expiration)
```

**Time Travel Query:**

```sql
-- Current state (snapshot 102)
SELECT COUNT(*) FROM events;
-- Result: 1,050,000 rows

-- Time travel to snapshot 100
SELECT COUNT(*) FROM events VERSION AS OF 100;
-- Result: 1,000,000 rows

-- Time travel by timestamp
SELECT COUNT(*) FROM events TIMESTAMP AS OF '2023-10-15T10:00:00Z';
-- Result: 1,050,000 rows (snapshot at that time)
```

**PySpark Time Travel:**

```python
# Read current snapshot
df_current = spark.read.format("iceberg").load("events")
print(f"Current: {df_current.count()}")

# Read specific snapshot
df_snapshot = spark.read.format("iceberg") \
    .option("snapshot-id", "100") \
    .load("events")
print(f"Snapshot 100: {df_snapshot.count()}")

# Read as of timestamp
df_timestamp = spark.read.format("iceberg") \
    .option("as-of-timestamp", "1697385600000") \
    .load("events")
print(f"As of timestamp: {df_timestamp.count()}")
```

---

## ğŸ”„ 3. Comparing Lakehouse Formats

### Delta Lake vs Iceberg vs Hudi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature          â”‚ Delta Lake     â”‚ Iceberg        â”‚ Hudi           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metadata         â”‚ JSON log       â”‚ Avro metadata  â”‚ Avro timeline  â”‚
â”‚                  â”‚ (_delta_log/)  â”‚ (3-level)      â”‚ (.hoodie/)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ACID             â”‚ Optimistic     â”‚ Optimistic     â”‚ MVCC           â”‚
â”‚                  â”‚ concurrency    â”‚ concurrency    â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Partitioning     â”‚ Hive-style     â”‚ Hidden         â”‚ Hive-style     â”‚
â”‚                  â”‚ (exposed)      â”‚ (automatic)    â”‚ (exposed)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Time Travel      â”‚ âœ… Version/TS  â”‚ âœ… Snapshot/TS â”‚ âœ… Instant/TS  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Schema Evolution â”‚ âœ… Add columns â”‚ âœ… Add/rename/ â”‚ âœ… Add columns â”‚
â”‚                  â”‚                â”‚ reorder        â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Partition Evol.  â”‚ âŒ No          â”‚ âœ… Yes         â”‚ âŒ No          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Engine Support   â”‚ Spark, Presto, â”‚ Spark, Flink,  â”‚ Spark, Flink,  â”‚
â”‚                  â”‚ Flink, Trino   â”‚ Presto, Trino, â”‚ Presto, Hive   â”‚
â”‚                  â”‚                â”‚ Dremio         â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Streaming        â”‚ âœ… Spark       â”‚ âœ… Flink, Sparkâ”‚ âœ… Spark, Flinkâ”‚
â”‚                  â”‚ Structured     â”‚                â”‚ DeltaStreamer  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cloud Native     â”‚ âœ… Excellent   â”‚ âœ… Excellent   â”‚ âš ï¸ Good        â”‚
â”‚                  â”‚ (S3, ADLS, GCS)â”‚ (S3, ADLS, GCS)â”‚ (S3, HDFS)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Maturity         â”‚ Production     â”‚ Production     â”‚ Production     â”‚
â”‚                  â”‚ (Databricks)   â”‚ (Netflix, Appleâ”‚ (Uber, Amazon) â”‚
â”‚                  â”‚                â”‚ LinkedIn)      â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### When to Choose Each

```
Choose Delta Lake:
â”œâ”€ Databricks ecosystem (tight integration)
â”œâ”€ Strong Spark focus
â”œâ”€ Need MERGE INTO with advanced conditions
â”œâ”€ Real-time streaming with Delta Live Tables
â””â”€ Simplicity (fewer moving parts)

Choose Apache Iceberg:
â”œâ”€ Multi-engine support (Spark, Flink, Trino, Presto)
â”œâ”€ Need hidden partitioning (user-friendly queries)
â”œâ”€ Partition evolution required
â”œâ”€ Large-scale analytics (Netflix: 10+ PB)
â”œâ”€ Open governance (Apache Foundation)
â””â”€ Advanced schema evolution (rename columns)

Choose Apache Hudi:
â”œâ”€ Upsert-heavy workloads (CDC, near-real-time)
â”œâ”€ Need incremental processing (MOR tables)
â”œâ”€ Record-level indexing (Bloom filters, HBase index)
â”œâ”€ AWS ecosystem (EMR, Glue)
â””â”€ Low-latency reads with MOR (merge-on-read)
```

---

## ğŸ“Š 4. Partition Evolution (Iceberg Only)

### The Problem with Static Partitioning

```
Initial design (2020):
PARTITIONED BY (days(event_time))
â”œâ”€ 365 partitions/year
â”œâ”€ Works well for 1M events/day

2023 (10M events/day):
â”œâ”€ 365 partitions/year
â”œâ”€ Each partition: 10M events
â”œâ”€ Queries slow (large partition scans)
â””â”€ Need finer granularity!

Traditional solution (Hive, Delta):
1. CREATE new table with hours(event_time)
2. Rewrite ALL historical data (expensive!)
3. Swap table names
Cost: $10,000+ for 10 PB rewrite âŒ
```

---

### Iceberg Partition Evolution

```sql
-- Initial partition spec (2020)
CREATE TABLE events (
  event_id BIGINT,
  event_time TIMESTAMP,
  data STRING
)
USING iceberg
PARTITIONED BY (days(event_time));

-- 2023: Evolve to hourly partitioning (NO rewrite!)
ALTER TABLE events
SET PARTITION SPEC (hours(event_time));

-- Iceberg behavior:
-- Old data: Partitioned by days (unchanged)
-- New data: Partitioned by hours (from now on)

-- Query optimization:
SELECT * FROM events
WHERE event_time >= TIMESTAMP '2023-01-01 00:00:00';

-- Iceberg uses:
-- â”œâ”€ 2020-2022 data: 1095 day partitions
-- â””â”€ 2023 data: 8760 hour partitions
-- Mixed granularity, optimal for each period!
```

**Partition Evolution Metadata:**

```
metadata/v15.metadata.json:
{
  "partition-specs": [
    {
      "spec-id": 0,
      "fields": [
        {"name": "event_time_day", "transform": "day", "source-id": 2}
      ]
    },
    {
      "spec-id": 1,  â† New spec
      "fields": [
        {"name": "event_time_hour", "transform": "hour", "source-id": 2}
      ]
    }
  ],
  "default-spec-id": 1  â† Use spec-id 1 for new writes
}

Data files:
â”œâ”€ spec-id=0 (old files, day partitions)
â”‚  â”œâ”€ event_time_day=19000/part-00000.parquet
â”‚  â””â”€ event_time_day=19001/part-00001.parquet
â””â”€ spec-id=1 (new files, hour partitions)
   â”œâ”€ event_time_hour=456000/part-00002.parquet
   â””â”€ event_time_hour=456001/part-00003.parquet
```

---

## ğŸ—‚ï¸ 5. Schema Evolution with Column Mapping

### Column ID Mapping (Iceberg)

```
Problem (Parquet):
Parquet stores columns by NAME or POSITION
â”œâ”€ Rename column â†’ breaks existing files
â””â”€ Reorder columns â†’ breaks position-based readers

Iceberg Solution: Column ID Mapping

Schema v1:
{
  "fields": [
    {"id": 1, "name": "event_id", "type": "long"},
    {"id": 2, "name": "event_time", "type": "timestamp"},
    {"id": 3, "name": "user_id", "type": "long"}
  ]
}

Schema v2 (rename "event_id" â†’ "id"):
{
  "fields": [
    {"id": 1, "name": "id", "type": "long"},  â† Same ID!
    {"id": 2, "name": "event_time", "type": "timestamp"},
    {"id": 3, "name": "user_id", "type": "long"}
  ]
}

Reading old files with new schema:
â”œâ”€ Map column ID 1 (old: "event_id") â†’ (new: "id")
â”œâ”€ Read column by ID, not name
â””â”€ Works seamlessly! âœ…
```

**Advanced Schema Evolution:**

```sql
-- Add column
ALTER TABLE events ADD COLUMN country STRING;

-- Rename column (Iceberg only)
ALTER TABLE events RENAME COLUMN user_id TO customer_id;

-- Reorder columns (Iceberg only)
ALTER TABLE events ALTER COLUMN country AFTER event_time;

-- Change column type (widen only)
ALTER TABLE events ALTER COLUMN user_id TYPE BIGINT;  -- int â†’ long

-- Drop column (Iceberg only)
ALTER TABLE events DROP COLUMN country;
```

---

## ğŸ† Interview Questions (Intermediate Level)

1. **Explain Iceberg's three-level metadata hierarchy. Why is it more efficient than Delta's JSON log?**
2. **What is hidden partitioning? How does it improve query usability?**
3. **Compare Delta Lake, Iceberg, and Hudi. When would you choose each?**
4. **How does Iceberg achieve partition evolution without rewriting data?**
5. **Explain column ID mapping. Why is it important for schema evolution?**
6. **How does snapshot isolation enable time travel in Iceberg?**

---

## ğŸ“ Summary Checklist

- [ ] Understand Iceberg metadata architecture
- [ ] Implement hidden partitioning with transform functions
- [ ] Use snapshot isolation for time travel queries
- [ ] Compare Delta Lake, Iceberg, and Hudi
- [ ] Evolve partition specs without data rewrite
- [ ] Implement schema evolution with column mapping
- [ ] Answer intermediate interview questions

**Next:** [EXPERT.md â†’](EXPERT.md)
