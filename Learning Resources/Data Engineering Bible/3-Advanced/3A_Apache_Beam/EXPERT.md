# Week 10-11: Apache Beam â€” EXPERT Track

*"Beam's portability framework is engineering excellenceâ€”write once in Java, run on Flink with RocksDB state, or Dataflow with autoscaling. Understanding runner internals separates architects from users."* â€” Distinguished Engineer, Google

---

## ğŸ¯ Learning Outcomes

- Master Beam portability framework architecture
- Understand runner-specific optimizations (Dataflow, Flink, Spark)
- Implement custom IO connectors with Splittable DoFn
- Design exactly-once semantics with Beam
- Optimize large-scale pipelines (>10TB/day)
- Handle schema evolution and data lineage
- Troubleshoot production pipeline failures

---

## ğŸ—ï¸ 1. Portability Framework Architecture

### The Portability Layer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Beam SDK (Java/Python/Go)              â”‚
â”‚  User Code: Pipeline construction, DoFns, CombineFns     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Beam Model Pipeline Proto
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Beam Portability Framework                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Runner API: Language-agnostic pipeline proto      â”‚  â”‚
â”‚  â”‚ Fn API: Cross-language SDK harness                â”‚  â”‚
â”‚  â”‚ Job API: Job management, metrics, logs            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Flink Runner  â”‚ â”‚Spark Runner  â”‚ â”‚Dataflow      â”‚ â”‚Custom Runner â”‚
â”‚              â”‚ â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
â”‚ Translates   â”‚ â”‚ Translates   â”‚ â”‚ Translates   â”‚ â”‚ Translates   â”‚
â”‚ to Flink     â”‚ â”‚ to Spark     â”‚ â”‚ to Dataflow  â”‚ â”‚ to ...       â”‚
â”‚ DAG          â”‚ â”‚ RDD/Dataset  â”‚ â”‚ execution    â”‚ â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight:** Pipeline proto is runner-agnostic. Runners translate to native execution.

---

### Runner Translation Example

```java
// Beam Pipeline:
PCollection<KV<String, Integer>> wordCounts = lines
    .apply(ParDo.of(new ExtractWordsFn()))  // ParDo
    .apply(Count.perElement());              // GroupByKey + Combine

// Flink Runner Translation:
DataStream<String> lines = ...;
DataStream<Tuple2<String, Long>> counts = lines
    .flatMap(new ExtractWordsFn())          // FlatMapFunction
    .keyBy(word -> word)                    // keyBy (hash partition)
    .window(GlobalWindows.create())         // Global window
    .reduce((a, b) -> a + b);               // ReduceFunction

// Spark Runner Translation:
JavaRDD<String> lines = ...;
JavaPairRDD<String, Long> counts = lines
    .flatMap(new ExtractWordsFn())          // flatMap
    .mapToPair(word -> new Tuple2<>(word, 1L))
    .reduceByKey((a, b) -> a + b);          // reduceByKey
```

---

## ğŸ¯ 2. Exactly-Once Semantics in Beam

### Source Checkpointing

```java
// Kafka source with checkpointed offsets
PCollection<KV<String, String>> events = pipeline.apply(
    KafkaIO.<String, String>read()
        .withBootstrapServers("localhost:9092")
        .withTopic("events")
        .withKeyDeserializer(StringDeserializer.class)
        .withValueDeserializer(StringDeserializer.class)
        // Enable checkpointing (runner-dependent)
        .commitOffsetsInFinalize()
);

// Flink Runner: Offsets checkpointed with Flink snapshots
// Dataflow Runner: Offsets stored in Dataflow state
```

**Checkpoint Recovery:**

```
Job fails at T=100:
â”œâ”€ Last successful checkpoint: T=80
â”œâ”€ Kafka offsets at T=80: partition-0: 10000, partition-1: 50000
â”œâ”€ Recovery: Seek to offset 10000, 50000
â””â”€ Replay events from T=80 to T=100 (exactly-once)
```

---

### Transactional Sinks

```java
// Kafka sink with exactly-once (2PC)
PCollection<KV<String, String>> results = ...;

results.apply(
    KafkaIO.<String, String>write()
        .withBootstrapServers("localhost:9092")
        .withTopic("output")
        .withKeySerializer(StringSerializer.class)
        .withValueSerializer(StringSerializer.class)
        // Exactly-once requires transactions
        .withEOS(20, "beam-txn")  // Transactional writes
);

// Under the hood (Flink Runner):
// 1. Pre-commit during checkpoint barrier
// 2. Commit after checkpoint completes
// 3. Abort if checkpoint fails
```

---

## ğŸ”Œ 3. Custom IO Connectors

### Splittable DoFn (SDF) for Bounded Sources

```java
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.splittabledofn.*;

// Custom file source with parallel reading
@BoundedPerElement
public class ReadFileFn extends DoFn<String, String> {
    
    // Restriction: byte range to read
    @GetInitialRestriction
    public OffsetRange getInitialRestriction(@Element String filename) {
        long fileSize = getFileSize(filename);
        return new OffsetRange(0, fileSize);
    }
    
    // Split restriction for parallel processing
    @SplitRestriction
    public void splitRestriction(
        @Restriction OffsetRange range,
        OutputReceiver<OffsetRange> receiver
    ) {
        long chunkSize = 64 * 1024 * 1024;  // 64 MB chunks
        for (long offset = range.getFrom(); offset < range.getTo(); offset += chunkSize) {
            receiver.output(new OffsetRange(
                offset,
                Math.min(offset + chunkSize, range.getTo())
            ));
        }
    }
    
    // Process restriction
    @ProcessElement
    public void processElement(
        ProcessContext c,
        RestrictionTracker<OffsetRange, Long> tracker
    ) {
        String filename = c.element();
        OffsetRange range = tracker.currentRestriction();
        
        try (RandomAccessFile file = new RandomAccessFile(filename, "r")) {
            file.seek(range.getFrom());
            
            long position = range.getFrom();
            while (tracker.tryClaim(position)) {
                String line = file.readLine();
                if (line == null || position >= range.getTo()) {
                    break;
                }
                c.output(line);
                position = file.getFilePointer();
            }
        }
    }
}

// Usage:
PCollection<String> files = pipeline.apply(Create.of("file1.txt", "file2.txt"));
PCollection<String> lines = files.apply(ParDo.of(new ReadFileFn()));
```

**Execution:**

```
Input: file1.txt (200 MB)

Step 1: getInitialRestriction
â”œâ”€ file1.txt â†’ OffsetRange(0, 200MB)

Step 2: splitRestriction (64 MB chunks)
â”œâ”€ OffsetRange(0, 64MB)
â”œâ”€ OffsetRange(64MB, 128MB)
â”œâ”€ OffsetRange(128MB, 192MB)
â””â”€ OffsetRange(192MB, 200MB)

Step 3: processElement (parallel on 4 workers)
â”œâ”€ Worker 1: Read 0-64MB
â”œâ”€ Worker 2: Read 64-128MB
â”œâ”€ Worker 3: Read 128-192MB
â””â”€ Worker 4: Read 192-200MB

Result: 4x parallelism, linear scalability
```

---

### Unbounded Source with SDF

```java
// Custom Kafka-like source
@UnboundedPerElement
public class ReadStreamFn extends DoFn<String, String> {
    
    @GetInitialRestriction
    public OffsetRange getInitialRestriction(@Element String topic) {
        return new OffsetRange(0, Long.MAX_VALUE);  // Unbounded
    }
    
    @GetInitialWatermarkEstimatorState
    public Instant getInitialWatermarkState(@Element String topic) {
        return Instant.EPOCH;
    }
    
    @NewWatermarkEstimator
    public WatermarkEstimator<Instant> newWatermarkEstimator(
        @WatermarkEstimatorState Instant state
    ) {
        return new Manual(state);
    }
    
    @ProcessElement
    public ProcessContinuation processElement(
        ProcessContext c,
        RestrictionTracker<OffsetRange, Long> tracker,
        WatermarkEstimator<Instant> watermarkEstimator
    ) {
        String topic = c.element();
        
        // Poll messages
        List<Message> messages = pollMessages(topic, 100);  // Max 100
        
        for (Message msg : messages) {
            if (!tracker.tryClaim(msg.getOffset())) {
                return ProcessContinuation.stop();
            }
            
            c.outputWithTimestamp(msg.getData(), msg.getTimestamp());
            watermarkEstimator.setWatermark(msg.getTimestamp());
        }
        
        // Continue processing (unbounded)
        return ProcessContinuation.resume().withResumeDelay(Duration.standardSeconds(1));
    }
}
```

---

## ğŸš€ 4. Runner-Specific Optimizations

### Dataflow Runner

```java
// Auto-scaling configuration
DataflowPipelineOptions options = 
    PipelineOptionsFactory.as(DataflowPipelineOptions.class);

options.setRunner(DataflowRunner.class);
options.setProject("my-project");
options.setRegion("us-central1");
options.setTempLocation("gs://my-bucket/temp");

// Auto-scaling
options.setAutoscalingAlgorithm(AutoscalingAlgorithmType.THROUGHPUT_BASED);
options.setMaxNumWorkers(100);  // Scale to 100 workers
options.setWorkerMachineType("n1-standard-4");

// Dataflow-specific optimizations
options.setEnableStreamingEngine(true);  // Offload shuffle to service
options.setStreaming(true);

Pipeline pipeline = Pipeline.create(options);
```

**Streaming Engine:**

```
Traditional:
â”œâ”€ Shuffle on worker VMs (memory + disk)
â”œâ”€ State on worker persistent disks
â””â”€ Scaling requires state migration

Streaming Engine:
â”œâ”€ Shuffle offloaded to Google service
â”œâ”€ State in disaggregated storage
â”œâ”€ Scaling: Attach new workers instantly
â””â”€ No state migration overhead
```

---

### Flink Runner

```java
// Flink-specific pipeline options
FlinkPipelineOptions options = 
    PipelineOptionsFactory.as(FlinkPipelineOptions.class);

options.setRunner(FlinkRunner.class);
options.setFlinkMaster("localhost:8081");
options.setParallelism(64);

// Flink state backend (use RocksDB for large state)
options.setStateBackend("rocksdb");
options.setCheckpointingInterval(60000L);  // 1 minute checkpoints

// Flink-specific optimizations
options.setObjectReuse(true);  // Reuse objects (avoid serialization)
options.setAutoWatermarkInterval(200L);  // Watermark interval

Pipeline pipeline = Pipeline.create(options);
```

**State Backend Mapping:**

```
Beam State â†’ Flink RocksDB State:
â”œâ”€ ValueState â†’ ValueState
â”œâ”€ BagState â†’ ListState
â”œâ”€ MapState â†’ MapState
â””â”€ SetState â†’ MapState (value=null)

Checkpointing:
â”œâ”€ Beam checkpoints trigger Flink snapshots
â”œâ”€ State persisted to S3/HDFS
â””â”€ Recovery: Restore from last Flink checkpoint
```

---

### Spark Runner

```java
// Spark-specific options
SparkPipelineOptions options = 
    PipelineOptionsFactory.as(SparkPipelineOptions.class);

options.setRunner(SparkRunner.class);
options.setSparkMaster("spark://master:7077");

// Spark configurations
Map<String, String> sparkConf = new HashMap<>();
sparkConf.put("spark.executor.memory", "8g");
sparkConf.put("spark.executor.cores", "4");
sparkConf.put("spark.dynamicAllocation.enabled", "true");

options.setSparkMasterUrl("spark://master:7077");
options.setFilesToStage(Collections.singletonList("pipeline.jar"));

Pipeline pipeline = Pipeline.create(options);
```

---

## ğŸ“Š 5. Production Case Studies

### Netflix: Content Recommendation Pipeline

```
Requirements:
â”œâ”€ Process 500M viewing events/day
â”œâ”€ Join with content catalog (1M items)
â”œâ”€ Generate recommendations in <1 hour
â””â”€ Run on Google Cloud Dataflow

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka (viewing events)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Beam Pipeline (Dataflow Runner)                        â”‚
â”‚                                                        â”‚
â”‚ 1. Read: KafkaIO                                       â”‚
â”‚    â”œâ”€ 500M events/day                                 â”‚
â”‚    â””â”€ Parallelism: 200 (match Kafka partitions)       â”‚
â”‚                                                        â”‚
â”‚ 2. Enrich: Side input (content catalog)               â”‚
â”‚    â”œâ”€ AsMap view (1M items)                           â”‚
â”‚    â””â”€ Broadcast to all workers                        â”‚
â”‚                                                        â”‚
â”‚ 3. Window: Fixed windows (1 hour)                     â”‚
â”‚    â””â”€ Group viewing sessions                          â”‚
â”‚                                                        â”‚
â”‚ 4. Aggregate: CombineFn (user preferences)            â”‚
â”‚    â”œâ”€ Genres watched, total time, recency             â”‚
â”‚    â””â”€ Custom CombineFn (efficient)                    â”‚
â”‚                                                        â”‚
â”‚ 5. Compute: ML inference (TensorFlow)                 â”‚
â”‚    â”œâ”€ ParDo with TensorFlow Serving                   â”‚
â”‚    â””â”€ Batch predictions (100 users/request)           â”‚
â”‚                                                        â”‚
â”‚ 6. Write: BigQuery (analytics), Cassandra (serving)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Configuration:
- Dataflow Streaming Engine: Enabled
- Max workers: 500 (auto-scaling)
- Machine type: n1-highmem-8 (52 GB RAM)
- Checkpoint interval: 5 minutes
- State: Managed by Dataflow service
```

---

### Spotify: Real-Time Music Analytics

```
Requirements:
â”œâ”€ Process 2B play events/day
â”œâ”€ Calculate artist/playlist statistics
â”œâ”€ Update dashboards every 1 minute
â””â”€ Run on Apache Flink

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka (play events)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Beam Pipeline (Flink Runner)                           â”‚
â”‚                                                        â”‚
â”‚ 1. Read: KafkaIO with Flink offset management         â”‚
â”‚    â”œâ”€ 2B events/day (~23k/sec sustained)             â”‚
â”‚    â””â”€ Parallelism: 128                                â”‚
â”‚                                                        â”‚
â”‚ 2. Window: Sliding (5 min size, 1 min slide)          â”‚
â”‚    â””â”€ For moving averages                             â”‚
â”‚                                                        â”‚
â”‚ 3. Aggregate: Combine.perKey                          â”‚
â”‚    â”œâ”€ Play count per artist                           â”‚
â”‚    â”œâ”€ Unique listeners (HyperLogLog)                  â”‚
â”‚    â””â”€ Avg play duration                               â”‚
â”‚                                                        â”‚
â”‚ 4. Triggers: Early + On-time + Late                   â”‚
â”‚    â”œâ”€ Early: Every 10 seconds (dashboard updates)     â”‚
â”‚    â”œâ”€ On-time: Watermark (accurate count)             â”‚
â”‚    â””â”€ Late: Accept 5 min late data                    â”‚
â”‚                                                        â”‚
â”‚ 5. Write: Kafka (downstream), Redis (caching)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Configuration:
- Flink TaskManagers: 32 (8 slots each)
- RocksDB state backend (incremental checkpoints)
- Checkpoint interval: 1 minute
- Unaligned checkpoints: Enabled (high throughput)
- State size: ~200 GB (5-minute windows, millions of keys)
```

---

## ğŸ† Interview Questions (Expert Level)

1. **Explain Beam's portability framework. How does it achieve write-once-run-anywhere?**
2. **How would you implement exactly-once semantics in a Beam pipeline with Kafka source and sink?**
3. **What is Splittable DoFn? When would you use it instead of a standard source?**
4. **Compare Dataflow Streaming Engine vs traditional Flink execution. What are the trade-offs?**
5. **Design a real-time recommendation pipeline processing 1B events/day. What are the bottlenecks?**
6. **How would you handle schema evolution in a long-running Beam pipeline?**
7. **Explain runner translation: How does a Beam GroupByKey map to Flink's keyBy + window?**
8. **What optimizations does Beam perform (fusion, combiner lifting)? How do they affect performance?**
9. **How would you debug a Beam pipeline with high checkpoint latency on Flink runner?**
10. **Design a multi-tenant Beam platform. How would you isolate tenants and allocate resources?**

---

## ğŸ“ Summary Checklist

- [ ] Understand Beam portability framework architecture
- [ ] Implement exactly-once semantics with transactional sinks
- [ ] Build custom IO connectors with Splittable DoFn
- [ ] Configure runner-specific optimizations (Dataflow, Flink, Spark)
- [ ] Design large-scale production pipelines (>10TB/day)
- [ ] Handle schema evolution and late data
- [ ] Troubleshoot performance issues (checkpoints, backpressure)
- [ ] Answer expert-level interview questions

**Next:** [WHITEPAPERS.md â†’](WHITEPAPERS.md)
