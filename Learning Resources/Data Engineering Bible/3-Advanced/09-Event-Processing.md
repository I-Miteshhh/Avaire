# Event Processing & Event-Driven Architecture

**Target Role:** Principal Data Engineer / Staff Engineer (40+ LPA)  
**Difficulty:** Advanced  
**Learning Time:** 2-3 weeks  
**Interview Frequency:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

## üìö Table of Contents

1. [Event-Driven Architecture Overview](#1-event-driven-architecture-overview)
2. [Complex Event Processing (CEP)](#2-complex-event-processing-cep)
3. [Event Sourcing](#3-event-sourcing)
4. [CQRS (Command Query Responsibility Segregation)](#4-cqrs-command-query-responsibility-segregation)
5. [Kafka Streams](#5-kafka-streams)
6. [Apache Flink CEP](#6-apache-flink-cep)
7. [Production Patterns](#7-production-patterns)
8. [Real-World Case Studies](#8-real-world-case-studies)

---

## 1. Event-Driven Architecture Overview

### What is Event-Driven Architecture (EDA)?

**Definition:** Architecture pattern where system components communicate through events.

```
Traditional Request-Response:
Client ‚Üí [Request] ‚Üí Service ‚Üí [Response] ‚Üí Client
        (Synchronous, Blocking)

Event-Driven:
Producer ‚Üí [Event] ‚Üí Event Bus ‚Üí Consumers (1..N)
        (Asynchronous, Non-blocking)
```

**Key Characteristics:**
- **Loose Coupling:** Services don't know about each other
- **Asynchronous:** No blocking waits
- **Scalable:** Add consumers without changing producers
- **Resilient:** Failures don't cascade

**Real-World Examples:**
- **Uber:** Ride request ‚Üí event ‚Üí matching service, notification service, analytics
- **Netflix:** Video play ‚Üí event ‚Üí recommendation engine, billing, analytics
- **Amazon:** Order placed ‚Üí event ‚Üí inventory, shipping, billing, notifications

---

### Event vs Message vs Command

| Type | Direction | Semantics | Example |
|------|-----------|-----------|---------|
| **Event** | Broadcast | "Something happened" | `OrderPlaced`, `UserRegistered` |
| **Message** | Point-to-point | "Do this task" | "Process payment for order 123" |
| **Command** | Directed | "Please do X" | `PlaceOrder`, `UpdateProfile` |

**Event Naming Convention:**
- Past tense (something that happened)
- Domain-specific
- Immutable once published

```
Good: OrderPlaced, PaymentProcessed, UserRegistered
Bad: ProcessOrder, UpdateUser, SendEmail
```

---

### Event Structure

```json
{
  "event_id": "uuid-12345",
  "event_type": "OrderPlaced",
  "event_version": "1.0",
  "timestamp": "2024-01-15T10:30:00Z",
  "aggregate_id": "order-789",
  "aggregate_type": "Order",
  "data": {
    "order_id": "order-789",
    "user_id": "user-456",
    "items": [
      {"product_id": "prod-123", "quantity": 2, "price": 29.99}
    ],
    "total_amount": 59.98,
    "payment_method": "credit_card"
  },
  "metadata": {
    "correlation_id": "req-abc",
    "causation_id": "cmd-def",
    "user_agent": "Mozilla/5.0...",
    "ip_address": "192.168.1.1"
  }
}
```

**Key Fields:**
- `event_id`: Unique identifier (for idempotency)
- `event_type`: Type of event (for routing)
- `timestamp`: When it happened (for ordering)
- `aggregate_id`: Entity this event is about
- `data`: Event payload
- `metadata`: Context (correlation, causation)

---

## 2. Complex Event Processing (CEP)

### What is CEP?

**Definition:** Detect patterns across multiple events in real-time.

**Use Cases:**
- **Fraud Detection:** Suspicious pattern of transactions
- **Monitoring:** Server down ‚Üí no recovery ‚Üí alert
- **Trading:** Price pattern ‚Üí buy/sell signal
- **IoT:** Sensor readings ‚Üí anomaly detection

**Example: Credit Card Fraud**

```
Pattern: Multiple transactions in different countries within 1 hour

Events:
10:00 AM - Purchase in USA ($50)
10:15 AM - Purchase in UK ($200)
10:30 AM - Purchase in Japan ($500)

CEP Rule: IF (3 transactions in 1 hour) AND (3 different countries)
          THEN Alert("Possible fraud")
```

---

### CEP with Apache Flink

```java
// Flink CEP fraud detection example
package com.dateng.cep;

import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.windowing.time.Time;

import java.util.List;
import java.util.Map;

/**
 * Fraud detection using Complex Event Processing
 * 
 * Detects:
 * 1. Multiple high-value transactions in short time
 * 2. Transactions from different countries
 * 3. Rapid succession of failed attempts
 */
public class FraudDetectionCEP {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // Stream of transactions
        DataStream<Transaction> transactions = env
            .addSource(new KafkaSource<>("transactions"))
            .keyBy(Transaction::getUserId);
        
        // Pattern 1: High-value transactions
        Pattern<Transaction, ?> highValuePattern = Pattern
            .<Transaction>begin("first")
            .where(new SimpleCondition<Transaction>() {
                @Override
                public boolean filter(Transaction txn) {
                    return txn.getAmount() > 1000;
                }
            })
            .next("second")
            .where(new SimpleCondition<Transaction>() {
                @Override
                public boolean filter(Transaction txn) {
                    return txn.getAmount() > 1000;
                }
            })
            .next("third")
            .where(new SimpleCondition<Transaction>() {
                @Override
                public boolean filter(Transaction txn) {
                    return txn.getAmount() > 1000;
                }
            })
            .within(Time.minutes(10));
        
        // Apply pattern
        PatternStream<Transaction> patternStream = CEP.pattern(
            transactions,
            highValuePattern
        );
        
        // Select matching events
        DataStream<FraudAlert> alerts = patternStream.select(
            new PatternSelectFunction<Transaction, FraudAlert>() {
                @Override
                public FraudAlert select(Map<String, List<Transaction>> pattern) {
                    List<Transaction> txns = pattern.get("first");
                    txns.addAll(pattern.get("second"));
                    txns.addAll(pattern.get("third"));
                    
                    return new FraudAlert(
                        txns.get(0).getUserId(),
                        "Multiple high-value transactions",
                        txns
                    );
                }
            }
        );
        
        // Send alerts
        alerts.addSink(new AlertSink());
        
        env.execute("Fraud Detection CEP");
    }
}
```

---

### CEP Patterns

**1. Sequence Pattern**
```
A ‚Üí B ‚Üí C (events must occur in order)

Example: Login ‚Üí Add to cart ‚Üí Purchase
```

**2. Non-Sequence Pattern**
```
A AND B AND C (events can occur in any order)

Example: 3 failed login attempts (order doesn't matter)
```

**3. Within Time Window**
```
A ‚Üí B ‚Üí C within 5 minutes

Example: 3 purchases within 5 min ‚Üí possible fraud
```

**4. Negation Pattern**
```
A NOT followed by B within 10 minutes

Example: Server down NOT followed by recovery ‚Üí alert
```

**5. Iteration Pattern**
```
A{3,5} (event A occurs 3 to 5 times)

Example: Failed login {3,} ‚Üí block account
```

---

### Production CEP Implementation

```java
package com.dateng.cep;

import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.IterativeCondition;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.windowing.time.Time;

/**
 * Production-grade CEP for e-commerce platform
 * 
 * Scenarios:
 * 1. Cart abandonment (add to cart ‚Üí no purchase in 30 min)
 * 2. Fraud detection (suspicious transaction patterns)
 * 3. Service degradation (error rate spike)
 */
public class ProductionCEP {
    
    /**
     * Cart Abandonment Detection
     * 
     * Pattern: AddToCart NOT followed by Purchase within 30 minutes
     */
    public static DataStream<CartAbandonmentEvent> detectCartAbandonment(
        DataStream<UserEvent> events
    ) {
        Pattern<UserEvent, ?> pattern = Pattern
            .<UserEvent>begin("addToCart")
            .where(new IterativeCondition<UserEvent>() {
                @Override
                public boolean filter(UserEvent event, Context<UserEvent> ctx) {
                    return "ADD_TO_CART".equals(event.getEventType());
                }
            })
            .notFollowedBy("purchase")
            .where(new IterativeCondition<UserEvent>() {
                @Override
                public boolean filter(UserEvent event, Context<UserEvent> ctx) {
                    return "PURCHASE".equals(event.getEventType());
                }
            })
            .within(Time.minutes(30));
        
        PatternStream<UserEvent> patternStream = CEP.pattern(
            events.keyBy(UserEvent::getUserId),
            pattern
        );
        
        return patternStream.select((Map<String, List<UserEvent>> match) -> {
            UserEvent addToCart = match.get("addToCart").get(0);
            return new CartAbandonmentEvent(
                addToCart.getUserId(),
                addToCart.getCartItems(),
                addToCart.getTimestamp()
            );
        });
    }
    
    /**
     * Rapid Failure Detection
     * 
     * Pattern: 5 failed API calls within 1 minute ‚Üí circuit breaker
     */
    public static DataStream<ServiceAlert> detectRapidFailures(
        DataStream<APICall> apiCalls
    ) {
        Pattern<APICall, ?> pattern = Pattern
            .<APICall>begin("failures")
            .where(new IterativeCondition<APICall>() {
                @Override
                public boolean filter(APICall call, Context<APICall> ctx) {
                    return call.getStatusCode() >= 500;
                }
            })
            .times(5)  // 5 occurrences
            .within(Time.minutes(1));
        
        PatternStream<APICall> patternStream = CEP.pattern(
            apiCalls.keyBy(APICall::getServiceName),
            pattern
        );
        
        return patternStream.select((Map<String, List<APICall>> match) -> {
            List<APICall> failures = match.get("failures");
            return new ServiceAlert(
                failures.get(0).getServiceName(),
                "High error rate detected",
                failures.size(),
                System.currentTimeMillis()
            );
        });
    }
    
    /**
     * Price Surge Detection
     * 
     * Pattern: 3 consecutive price increases totaling >20%
     */
    public static DataStream<PriceSurgeAlert> detectPriceSurge(
        DataStream<PriceUpdate> priceUpdates
    ) {
        Pattern<PriceUpdate, ?> pattern = Pattern
            .<PriceUpdate>begin("first")
            .where(new IterativeCondition<PriceUpdate>() {
                @Override
                public boolean filter(PriceUpdate update, Context<PriceUpdate> ctx) {
                    return update.getPriceChange() > 0;
                }
            })
            .followedBy("second")
            .where(new IterativeCondition<PriceUpdate>() {
                @Override
                public boolean filter(PriceUpdate update, Context<PriceUpdate> ctx) {
                    PriceUpdate first = ctx.getEventsForPattern("first").iterator().next();
                    return update.getPrice() > first.getPrice();
                }
            })
            .followedBy("third")
            .where(new IterativeCondition<PriceUpdate>() {
                @Override
                public boolean filter(PriceUpdate update, Context<PriceUpdate> ctx) {
                    PriceUpdate first = ctx.getEventsForPattern("first").iterator().next();
                    double totalIncrease = (update.getPrice() - first.getPrice()) / first.getPrice();
                    return totalIncrease > 0.20;  // 20% increase
                }
            })
            .within(Time.hours(1));
        
        PatternStream<PriceUpdate> patternStream = CEP.pattern(
            priceUpdates.keyBy(PriceUpdate::getProductId),
            pattern
        );
        
        return patternStream.select((Map<String, List<PriceUpdate>> match) -> {
            PriceUpdate first = match.get("first").get(0);
            PriceUpdate third = match.get("third").get(0);
            double percentageIncrease = (third.getPrice() - first.getPrice()) / first.getPrice() * 100;
            
            return new PriceSurgeAlert(
                first.getProductId(),
                first.getPrice(),
                third.getPrice(),
                percentageIncrease
            );
        });
    }
}
```

---

### CEP Performance Considerations

**State Size:**
```
Pattern: A ‚Üí B ‚Üí C within 1 hour

Worst case state per key:
- Store all A events for 1 hour
- For 1M active users, ~1KB per user
- Total: 1 GB state

Mitigation:
- Use shorter time windows when possible
- Implement state pruning
- Use RocksDB state backend
```

**Latency:**
```
Pattern matching latency:
- Simple patterns: < 1ms
- Complex patterns with iterations: < 10ms
- With many keys in state: < 100ms

Optimization:
- Use KeyedProcessFunction for simple patterns
- Limit number of pattern states
- Increase parallelism
```

---

## 3. Event Sourcing

### What is Event Sourcing?

**Definition:** Store all changes to application state as a sequence of events.

**Traditional Approach:**
```
Database stores current state:

Users table:
id | name  | email | balance
1  | Alice | a@... | 1000

Problem: Lost history of how balance became 1000
```

**Event Sourcing:**
```
Event Store:
1. AccountCreated(userId=1, initialBalance=0)
2. MoneyDeposited(userId=1, amount=500)
3. MoneyDeposited(userId=1, amount=700)
4. MoneyWithdrawn(userId=1, amount=200)

Current balance = sum of all events = 1000
```

**Benefits:**
- **Complete History:** Know exactly what happened and when
- **Audit Trail:** Compliance, debugging
- **Time Travel:** Recreate state at any point in time
- **Event Replay:** Fix bugs by replaying with new logic

**Drawbacks:**
- **Complexity:** More code than traditional CRUD
- **Query Performance:** Need projections for queries
- **Schema Evolution:** Must handle old event versions

---

### Event Sourcing Implementation

```java
// Event base class
package com.dateng.eventsourcing;

import java.time.Instant;
import java.util.UUID;

public abstract class DomainEvent {
    private final String eventId;
    private final String aggregateId;
    private final Instant timestamp;
    private final long version;
    
    protected DomainEvent(String aggregateId, long version) {
        this.eventId = UUID.randomUUID().toString();
        this.aggregateId = aggregateId;
        this.timestamp = Instant.now();
        this.version = version;
    }
    
    // Getters
    public String getEventId() { return eventId; }
    public String getAggregateId() { return aggregateId; }
    public Instant getTimestamp() { return timestamp; }
    public long getVersion() { return version; }
}
```

```java
// Bank account events
package com.dateng.eventsourcing;

public class AccountCreated extends DomainEvent {
    private final String userId;
    private final double initialBalance;
    
    public AccountCreated(String accountId, String userId, double initialBalance, long version) {
        super(accountId, version);
        this.userId = userId;
        this.initialBalance = initialBalance;
    }
    
    public String getUserId() { return userId; }
    public double getInitialBalance() { return initialBalance; }
}

public class MoneyDeposited extends DomainEvent {
    private final double amount;
    private final String source;
    
    public MoneyDeposited(String accountId, double amount, String source, long version) {
        super(accountId, version);
        this.amount = amount;
        this.source = source;
    }
    
    public double getAmount() { return amount; }
    public String getSource() { return source; }
}

public class MoneyWithdrawn extends DomainEvent {
    private final double amount;
    private final String destination;
    
    public MoneyWithdrawn(String accountId, double amount, String destination, long version) {
        super(accountId, version);
        this.amount = amount;
        this.destination = destination;
    }
    
    public double getAmount() { return amount; }
    public String getDestination() { return destination; }
}
```

```java
// Aggregate (Bank Account)
package com.dateng.eventsourcing;

import java.util.ArrayList;
import java.util.List;

public class BankAccount {
    private String accountId;
    private String userId;
    private double balance;
    private long version;
    
    // Uncommitted events (to be saved)
    private List<DomainEvent> uncommittedEvents = new ArrayList<>();
    
    /**
     * Create new account
     */
    public static BankAccount create(String userId, double initialBalance) {
        BankAccount account = new BankAccount();
        account.apply(new AccountCreated(
            UUID.randomUUID().toString(),
            userId,
            initialBalance,
            0
        ));
        return account;
    }
    
    /**
     * Deposit money
     */
    public void deposit(double amount, String source) {
        if (amount <= 0) {
            throw new IllegalArgumentException("Amount must be positive");
        }
        
        apply(new MoneyDeposited(accountId, amount, source, version + 1));
    }
    
    /**
     * Withdraw money
     */
    public void withdraw(double amount, String destination) {
        if (amount <= 0) {
            throw new IllegalArgumentException("Amount must be positive");
        }
        if (amount > balance) {
            throw new IllegalStateException("Insufficient funds");
        }
        
        apply(new MoneyWithdrawn(accountId, amount, destination, version + 1));
    }
    
    /**
     * Apply event to aggregate
     */
    private void apply(DomainEvent event) {
        // Update state based on event
        if (event instanceof AccountCreated) {
            AccountCreated e = (AccountCreated) event;
            this.accountId = e.getAggregateId();
            this.userId = e.getUserId();
            this.balance = e.getInitialBalance();
            this.version = e.getVersion();
        } else if (event instanceof MoneyDeposited) {
            MoneyDeposited e = (MoneyDeposited) event;
            this.balance += e.getAmount();
            this.version = e.getVersion();
        } else if (event instanceof MoneyWithdrawn) {
            MoneyWithdrawn e = (MoneyWithdrawn) event;
            this.balance -= e.getAmount();
            this.version = e.getVersion();
        }
        
        // Track uncommitted event
        uncommittedEvents.add(event);
    }
    
    /**
     * Rebuild aggregate from event history
     */
    public static BankAccount fromEvents(List<DomainEvent> events) {
        BankAccount account = new BankAccount();
        
        for (DomainEvent event : events) {
            account.apply(event);
        }
        
        account.uncommittedEvents.clear();  // Already persisted
        return account;
    }
    
    /**
     * Get uncommitted events (to be saved)
     */
    public List<DomainEvent> getUncommittedEvents() {
        return new ArrayList<>(uncommittedEvents);
    }
    
    /**
     * Mark events as committed
     */
    public void markEventsAsCommitted() {
        uncommittedEvents.clear();
    }
    
    // Getters
    public String getAccountId() { return accountId; }
    public String getUserId() { return userId; }
    public double getBalance() { return balance; }
    public long getVersion() { return version; }
}
```

---

### Event Store

```java
package com.dateng.eventsourcing;

import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.stream.Collectors;

/**
 * In-memory event store (use PostgreSQL/EventStoreDB in production)
 */
public class InMemoryEventStore {
    
    // Store: aggregateId ‚Üí list of events
    private final Map<String, List<DomainEvent>> events = new ConcurrentHashMap<>();
    
    /**
     * Save events for an aggregate
     * 
     * @param aggregateId Aggregate ID
     * @param expectedVersion Expected current version (for optimistic locking)
     * @param newEvents Events to append
     */
    public void saveEvents(String aggregateId, long expectedVersion, List<DomainEvent> newEvents) {
        synchronized (events) {
            List<DomainEvent> existing = events.getOrDefault(aggregateId, new ArrayList<>());
            
            // Optimistic concurrency check
            long currentVersion = existing.isEmpty() ? 0 : existing.get(existing.size() - 1).getVersion();
            
            if (currentVersion != expectedVersion) {
                throw new ConcurrencyException(
                    String.format("Expected version %d but found %d", expectedVersion, currentVersion)
                );
            }
            
            // Append new events
            existing.addAll(newEvents);
            events.put(aggregateId, existing);
        }
    }
    
    /**
     * Load all events for an aggregate
     */
    public List<DomainEvent> getEvents(String aggregateId) {
        return new ArrayList<>(events.getOrDefault(aggregateId, Collections.emptyList()));
    }
    
    /**
     * Load events after a certain version
     */
    public List<DomainEvent> getEventsAfterVersion(String aggregateId, long afterVersion) {
        return events.getOrDefault(aggregateId, Collections.emptyList())
            .stream()
            .filter(e -> e.getVersion() > afterVersion)
            .collect(Collectors.toList());
    }
    
    /**
     * Get all events (for projection building)
     */
    public List<DomainEvent> getAllEvents() {
        return events.values().stream()
            .flatMap(List::stream)
            .sorted(Comparator.comparing(DomainEvent::getTimestamp))
            .collect(Collectors.toList());
    }
    
    public static class ConcurrencyException extends RuntimeException {
        public ConcurrencyException(String message) {
            super(message);
        }
    }
}
```

---

### Repository Pattern

```java
package com.dateng.eventsourcing;

/**
 * Repository for bank accounts using event sourcing
 */
public class BankAccountRepository {
    
    private final InMemoryEventStore eventStore;
    
    public BankAccountRepository(InMemoryEventStore eventStore) {
        this.eventStore = eventStore;
    }
    
    /**
     * Load aggregate by ID
     */
    public BankAccount findById(String accountId) {
        List<DomainEvent> events = eventStore.getEvents(accountId);
        
        if (events.isEmpty()) {
            return null;
        }
        
        return BankAccount.fromEvents(events);
    }
    
    /**
     * Save aggregate
     */
    public void save(BankAccount account) {
        List<DomainEvent> uncommittedEvents = account.getUncommittedEvents();
        
        if (uncommittedEvents.isEmpty()) {
            return;  // Nothing to save
        }
        
        // Save events
        eventStore.saveEvents(
            account.getAccountId(),
            account.getVersion() - uncommittedEvents.size(),
            uncommittedEvents
        );
        
        // Mark as committed
        account.markEventsAsCommitted();
    }
}
```

---

### Usage Example

```java
public class EventSourcingDemo {
    
    public static void main(String[] args) {
        InMemoryEventStore eventStore = new InMemoryEventStore();
        BankAccountRepository repository = new BankAccountRepository(eventStore);
        
        // Create account
        BankAccount account = BankAccount.create("user-123", 100.0);
        repository.save(account);
        
        System.out.printf("Created account %s with balance %.2f%n",
                         account.getAccountId(), account.getBalance());
        
        // Deposit money
        String accountId = account.getAccountId();
        account = repository.findById(accountId);
        account.deposit(50.0, "Paycheck");
        repository.save(account);
        
        System.out.printf("After deposit: %.2f%n", account.getBalance());
        
        // Withdraw money
        account = repository.findById(accountId);
        account.withdraw(30.0, "ATM");
        repository.save(account);
        
        System.out.printf("After withdrawal: %.2f%n", account.getBalance());
        
        // Show event history
        System.out.println("\nEvent History:");
        List<DomainEvent> events = eventStore.getEvents(accountId);
        for (DomainEvent event : events) {
            System.out.printf("%s: %s (version %d)%n",
                             event.getTimestamp(),
                             event.getClass().getSimpleName(),
                             event.getVersion());
        }
        
        // Time travel: Rebuild account at version 1
        account = BankAccount.fromEvents(events.subList(0, 2));
        System.out.printf("\nBalance at version 1: %.2f%n", account.getBalance());
    }
}
```

**Output:**
```
Created account acc-123 with balance 100.00
After deposit: 150.00
After withdrawal: 120.00

Event History:
2024-01-15T10:00:00Z: AccountCreated (version 0)
2024-01-15T10:05:00Z: MoneyDeposited (version 1)
2024-01-15T10:10:00Z: MoneyWithdrawn (version 2)

Balance at version 1: 150.00
```

---

## 4. CQRS (Command Query Responsibility Segregation)

### What is CQRS?

**Definition:** Separate read and write models of your data.

```
Traditional Architecture:
Client ‚Üí API ‚Üí Single Database (reads + writes)

CQRS:
Client ‚Üí Commands ‚Üí Write Model ‚Üí Event Store
                                      ‚Üì
                             Projection Builder
                                      ‚Üì
Client ‚Üê Queries ‚Üê Read Model (denormalized views)
```

**Benefits:**
- **Independent Scaling:** Scale reads and writes separately
- **Optimized Models:** Write model for consistency, read model for performance
- **Multiple Views:** Different projections for different use cases
- **Event Sourcing Synergy:** Natural fit with event sourcing

**When to Use:**
- Complex domain logic
- High read/write ratio mismatch
- Multiple read patterns (dashboard, API, reports)
- Event sourcing already in use

**When NOT to Use:**
- Simple CRUD applications
- Tight coupling between reads and writes needed
- Team inexperienced with CQRS

---

### CQRS Implementation

```java
// Commands (write side)
package com.dateng.cqrs.commands;

public interface Command {
    String getAggregateId();
}

public class CreateAccountCommand implements Command {
    private final String accountId;
    private final String userId;
    private final double initialBalance;
    
    public CreateAccountCommand(String accountId, String userId, double initialBalance) {
        this.accountId = accountId;
        this.userId = userId;
        this.initialBalance = initialBalance;
    }
    
    @Override
    public String getAggregateId() { return accountId; }
    public String getUserId() { return userId; }
    public double getInitialBalance() { return initialBalance; }
}

public class DepositMoneyCommand implements Command {
    private final String accountId;
    private final double amount;
    private final String source;
    
    public DepositMoneyCommand(String accountId, double amount, String source) {
        this.accountId = accountId;
        this.amount = amount;
        this.source = source;
    }
    
    @Override
    public String getAggregateId() { return accountId; }
    public double getAmount() { return amount; }
    public String getSource() { return source; }
}
```

```java
// Command Handler (write side)
package com.dateng.cqrs.commands;

import com.dateng.eventsourcing.BankAccount;
import com.dateng.eventsourcing.BankAccountRepository;

/**
 * Handles commands and produces events
 */
public class BankAccountCommandHandler {
    
    private final BankAccountRepository repository;
    
    public BankAccountCommandHandler(BankAccountRepository repository) {
        this.repository = repository;
    }
    
    /**
     * Handle CreateAccount command
     */
    public void handle(CreateAccountCommand command) {
        // Validate command
        if (command.getInitialBalance() < 0) {
            throw new IllegalArgumentException("Initial balance cannot be negative");
        }
        
        // Create aggregate
        BankAccount account = BankAccount.create(
            command.getUserId(),
            command.getInitialBalance()
        );
        
        // Save (produces events)
        repository.save(account);
    }
    
    /**
     * Handle DepositMoney command
     */
    public void handle(DepositMoneyCommand command) {
        // Load aggregate
        BankAccount account = repository.findById(command.getAggregateId());
        if (account == null) {
            throw new IllegalArgumentException("Account not found");
        }
        
        // Execute business logic
        account.deposit(command.getAmount(), command.getSource());
        
        // Save (produces events)
        repository.save(account);
    }
}
```

```java
// Query Model (read side)
package com.dateng.cqrs.queries;

/**
 * Denormalized view for queries
 */
public class AccountSummary {
    private String accountId;
    private String userId;
    private double balance;
    private int transactionCount;
    private double totalDeposits;
    private double totalWithdrawals;
    private Instant lastTransactionTime;
    
    // Getters and setters
    public String getAccountId() { return accountId; }
    public void setAccountId(String accountId) { this.accountId = accountId; }
    
    public String getUserId() { return userId; }
    public void setUserId(String userId) { this.userId = userId; }
    
    public double getBalance() { return balance; }
    public void setBalance(double balance) { this.balance = balance; }
    
    public int getTransactionCount() { return transactionCount; }
    public void setTransactionCount(int count) { this.transactionCount = count; }
    
    public double getTotalDeposits() { return totalDeposits; }
    public void setTotalDeposits(double total) { this.totalDeposits = total; }
    
    public double getTotalWithdrawals() { return totalWithdrawals; }
    public void setTotalWithdrawals(double total) { this.totalWithdrawals = total; }
    
    public Instant getLastTransactionTime() { return lastTransactionTime; }
    public void setLastTransactionTime(Instant time) { this.lastTransactionTime = time; }
}
```

```java
// Projection Builder (read side)
package com.dateng.cqrs.projections;

import com.dateng.eventsourcing.*;
import com.dateng.cqrs.queries.AccountSummary;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Builds read model from events
 */
public class AccountSummaryProjection {
    
    private final Map<String, AccountSummary> summaries = new ConcurrentHashMap<>();
    
    /**
     * Handle AccountCreated event
     */
    public void on(AccountCreated event) {
        AccountSummary summary = new AccountSummary();
        summary.setAccountId(event.getAggregateId());
        summary.setUserId(event.getUserId());
        summary.setBalance(event.getInitialBalance());
        summary.setTransactionCount(0);
        summary.setTotalDeposits(event.getInitialBalance());
        summary.setTotalWithdrawals(0);
        summary.setLastTransactionTime(event.getTimestamp());
        
        summaries.put(event.getAggregateId(), summary);
    }
    
    /**
     * Handle MoneyDeposited event
     */
    public void on(MoneyDeposited event) {
        AccountSummary summary = summaries.get(event.getAggregateId());
        if (summary != null) {
            summary.setBalance(summary.getBalance() + event.getAmount());
            summary.setTransactionCount(summary.getTransactionCount() + 1);
            summary.setTotalDeposits(summary.getTotalDeposits() + event.getAmount());
            summary.setLastTransactionTime(event.getTimestamp());
        }
    }
    
    /**
     * Handle MoneyWithdrawn event
     */
    public void on(MoneyWithdrawn event) {
        AccountSummary summary = summaries.get(event.getAggregateId());
        if (summary != null) {
            summary.setBalance(summary.getBalance() - event.getAmount());
            summary.setTransactionCount(summary.getTransactionCount() + 1);
            summary.setTotalWithdrawals(summary.getTotalWithdrawals() + event.getAmount());
            summary.setLastTransactionTime(event.getTimestamp());
        }
    }
    
    /**
     * Query: Get account summary
     */
    public AccountSummary getAccountSummary(String accountId) {
        return summaries.get(accountId);
    }
    
    /**
     * Query: Get all accounts for a user
     */
    public List<AccountSummary> getAccountsForUser(String userId) {
        return summaries.values().stream()
            .filter(s -> s.getUserId().equals(userId))
            .collect(Collectors.toList());
    }
    
    /**
     * Rebuild projection from all events
     */
    public void rebuildFrom(InMemoryEventStore eventStore) {
        summaries.clear();
        
        List<DomainEvent> events = eventStore.getAllEvents();
        for (DomainEvent event : events) {
            if (event instanceof AccountCreated) {
                on((AccountCreated) event);
            } else if (event instanceof MoneyDeposited) {
                on((MoneyDeposited) event);
            } else if (event instanceof MoneyWithdrawn) {
                on((MoneyWithdrawn) event);
            }
        }
    }
}
```

---

### Complete CQRS Example

```java
package com.dateng.cqrs;

/**
 * Complete CQRS demonstration
 */
public class CQRSDemo {
    
    public static void main(String[] args) {
        // Setup
        InMemoryEventStore eventStore = new InMemoryEventStore();
        BankAccountRepository repository = new BankAccountRepository(eventStore);
        BankAccountCommandHandler commandHandler = new BankAccountCommandHandler(repository);
        AccountSummaryProjection projection = new AccountSummaryProjection();
        
        // Subscribe projection to events
        eventStore.subscribe(event -> {
            if (event instanceof AccountCreated) {
                projection.on((AccountCreated) event);
            } else if (event instanceof MoneyDeposited) {
                projection.on((MoneyDeposited) event);
            } else if (event instanceof MoneyWithdrawn) {
                projection.on((MoneyWithdrawn) event);
            }
        });
        
        // Execute commands (WRITE side)
        System.out.println("=== Write Side (Commands) ===");
        
        CreateAccountCommand createCmd = new CreateAccountCommand(
            "acc-123",
            "user-456",
            1000.0
        );
        commandHandler.handle(createCmd);
        System.out.println("‚úì Account created");
        
        DepositMoneyCommand depositCmd = new DepositMoneyCommand(
            "acc-123",
            500.0,
            "Paycheck"
        );
        commandHandler.handle(depositCmd);
        System.out.println("‚úì Money deposited");
        
        WithdrawMoneyCommand withdrawCmd = new WithdrawMoneyCommand(
            "acc-123",
            200.0,
            "ATM"
        );
        commandHandler.handle(withdrawCmd);
        System.out.println("‚úì Money withdrawn");
        
        // Execute queries (READ side)
        System.out.println("\n=== Read Side (Queries) ===");
        
        AccountSummary summary = projection.getAccountSummary("acc-123");
        System.out.printf("Account ID: %s%n", summary.getAccountId());
        System.out.printf("User ID: %s%n", summary.getUserId());
        System.out.printf("Balance: $%.2f%n", summary.getBalance());
        System.out.printf("Transactions: %d%n", summary.getTransactionCount());
        System.out.printf("Total deposits: $%.2f%n", summary.getTotalDeposits());
        System.out.printf("Total withdrawals: $%.2f%n", summary.getTotalWithdrawals());
        
        // Rebuild projection (demonstrate event replay)
        System.out.println("\n=== Rebuilding Projection ===");
        projection.rebuildFrom(eventStore);
        summary = projection.getAccountSummary("acc-123");
        System.out.printf("Rebuilt balance: $%.2f%n", summary.getBalance());
    }
}
```

**Output:**
```
=== Write Side (Commands) ===
‚úì Account created
‚úì Money deposited
‚úì Money withdrawn

=== Read Side (Queries) ===
Account ID: acc-123
User ID: user-456
Balance: $1300.00
Transactions: 2
Total deposits: $1500.00
Total withdrawals: $200.00

=== Rebuilding Projection ===
Rebuilt balance: $1300.00
```

---

## 5. Kafka Streams

### What is Kafka Streams?

**Definition:** Java library for building stream processing applications on top of Kafka.

**Key Features:**
- **Stateful Processing:** Joins, aggregations, windowing
- **Exactly-Once Semantics:** No duplicates or data loss
- **Scalable:** Automatically distributes load
- **Fault-Tolerant:** Handles failures gracefully

**Kafka Streams vs Flink:**

| Feature | Kafka Streams | Apache Flink |
|---------|---------------|--------------|
| Deployment | Library (part of app) | Separate cluster |
| Language | Java/Scala | Java/Scala/Python |
| State Backend | RocksDB (embedded) | RocksDB/Heap |
| SQL Support | ksqlDB | Flink SQL |
| Learning Curve | Lower | Higher |
| Use When | Kafka-centric | Complex CEP |

---

### Kafka Streams Topology

```java
package com.dateng.kafkastreams;

import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.*;

import java.time.Duration;
import java.util.Properties;

/**
 * Real-time clickstream analytics with Kafka Streams
 */
public class ClickstreamAnalytics {
    
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "clickstream-analytics");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        
        StreamsBuilder builder = new StreamsBuilder();
        
        // Input stream: clickstream events
        KStream<String, String> events = builder.stream("clickstream_events");
        
        // Parse JSON
        KStream<String, ClickEvent> clickEvents = events
            .mapValues(json -> ClickEvent.fromJson(json));
        
        // Filter page views
        KStream<String, ClickEvent> pageViews = clickEvents
            .filter((key, event) -> "page_view".equals(event.getEventType()));
        
        // Count page views per URL (1-minute tumbling window)
        KTable<Windowed<String>, Long> pageViewCounts = pageViews
            .groupBy((key, event) -> event.getPageUrl())
            .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
            .count();
        
        // Convert to stream and print
        pageViewCounts
            .toStream()
            .map((windowedKey, count) -> KeyValue.pair(
                windowedKey.key(),
                String.format("URL: %s, Views: %d, Window: %s",
                             windowedKey.key(), count, windowedKey.window())
            ))
            .to("page_view_counts");
        
        // Start streaming
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
        
        // Graceful shutdown
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

---

### Stateful Operations

```java
package com.dateng.kafkastreams;

/**
 * Advanced Kafka Streams patterns
 */
public class StatefulOperations {
    
    /**
     * Sessionization with session windows
     */
    public static void sessionization(StreamsBuilder builder) {
        KStream<String, ClickEvent> events = builder.stream("clickstream_events");
        
        // Session window: 30-minute inactivity gap
        KTable<Windowed<String>, SessionData> sessions = events
            .groupByKey()
            .windowedBy(SessionWindows.with(Duration.ofMinutes(30)))
            .aggregate(
                SessionData::new,
                (key, event, session) -> {
                    session.addEvent(event);
                    return session;
                },
                (key, session1, session2) -> {
                    session1.merge(session2);
                    return session1;
                },
                Materialized.with(Serdes.String(), new SessionDataSerde())
            );
        
        // Output sessions
        sessions.toStream().to("user_sessions");
    }
    
    /**
     * Join streams (user events + user profile)
     */
    public static void enrichEvents(StreamsBuilder builder) {
        KStream<String, ClickEvent> events = builder.stream("clickstream_events");
        KTable<String, UserProfile> profiles = builder.table("user_profiles");
        
        // Join event with user profile
        KStream<String, EnrichedEvent> enriched = events
            .join(
                profiles,
                (event, profile) -> new EnrichedEvent(event, profile),
                Joined.with(Serdes.String(), new ClickEventSerde(), new UserProfileSerde())
            );
        
        enriched.to("enriched_events");
    }
    
    /**
     * Aggregation with tumbling windows
     */
    public static void realtimeMetrics(StreamsBuilder builder) {
        KStream<String, ClickEvent> events = builder.stream("clickstream_events");
        
        // 1-minute tumbling window aggregations
        KTable<Windowed<String>, Metrics> metrics = events
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
            .aggregate(
                Metrics::new,
                (key, event, metrics) -> {
                    metrics.incrementEventCount();
                    if ("purchase".equals(event.getEventType())) {
                        metrics.addRevenue(event.getAmount());
                    }
                    return metrics;
                },
                Materialized.with(Serdes.String(), new MetricsSerde())
            );
        
        metrics.toStream().to("realtime_metrics");
    }
    
    /**
     * Deduplication using state store
     */
    public static void deduplication(StreamsBuilder builder) {
        KStream<String, ClickEvent> events = builder.stream("clickstream_events");
        
        // Deduplicate based on event_id
        KStream<String, ClickEvent> deduplicated = events
            .transform(() -> new DeduplicationTransformer<>(
                Duration.ofHours(1),
                (event) -> event.getEventId()
            ));
        
        deduplicated.to("deduplicated_events");
    }
}
```

---

### Deduplication Transformer

```java
package com.dateng.kafkastreams;

import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.kstream.Transformer;
import org.apache.kafka.streams.processor.ProcessorContext;
import org.apache.kafka.streams.state.WindowStore;
import org.apache.kafka.streams.state.WindowStoreIterator;

import java.time.Duration;

/**
 * Transformer for event deduplication
 */
public class DeduplicationTransformer<K, V> implements Transformer<K, V, KeyValue<K, V>> {
    
    private final Duration maintainDuration;
    private final KeyExtractor<V> keyExtractor;
    private ProcessorContext context;
    private WindowStore<String, Boolean> store;
    
    public DeduplicationTransformer(Duration maintainDuration, KeyExtractor<V> keyExtractor) {
        this.maintainDuration = maintainDuration;
        this.keyExtractor = keyExtractor;
    }
    
    @Override
    public void init(ProcessorContext context) {
        this.context = context;
        this.store = (WindowStore<String, Boolean>) context.getStateStore("dedup-store");
    }
    
    @Override
    public KeyValue<K, V> transform(K key, V value) {
        String deduplicationKey = keyExtractor.extract(value);
        
        // Check if we've seen this event before
        long eventTime = context.timestamp();
        long fromTime = eventTime - maintainDuration.toMillis();
        
        WindowStoreIterator<Boolean> iterator = store.fetch(
            deduplicationKey,
            fromTime,
            eventTime
        );
        
        boolean isDuplicate = iterator.hasNext();
        iterator.close();
        
        if (isDuplicate) {
            // Drop duplicate
            return null;
        } else {
            // Store and forward
            store.put(deduplicationKey, true, eventTime);
            return KeyValue.pair(key, value);
        }
    }
    
    @Override
    public void close() {
        // Cleanup
    }
    
    public interface KeyExtractor<V> {
        String extract(V value);
    }
}
```

---

## 6. Apache Flink CEP

### Advanced CEP Patterns

```java
package com.dateng.flink.cep;

import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.IterativeCondition;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.windowing.time.Time;

/**
 * Advanced CEP patterns for production use
 */
public class AdvancedCEPPatterns {
    
    /**
     * Pattern: Detect slowdown in API response times
     * 
     * 3 consecutive slow responses (>1s) within 5 minutes
     */
    public static DataStream<PerformanceAlert> detectSlowAPI(
        DataStream<APICall> apiCalls
    ) {
        Pattern<APICall, ?> pattern = Pattern
            .<APICall>begin("slow1")
            .where(new IterativeCondition<APICall>() {
                @Override
                public boolean filter(APICall call, Context<APICall> ctx) {
                    return call.getResponseTime() > 1000;  // >1 second
                }
            })
            .next("slow2")
            .where(new IterativeCondition<APICall>() {
                @Override
                public boolean filter(APICall call, Context<APICall> ctx) {
                    return call.getResponseTime() > 1000;
                }
            })
            .next("slow3")
            .where(new IterativeCondition<APICall>() {
                @Override
                public boolean filter(APICall call, Context<APICall> ctx) {
                    return call.getResponseTime() > 1000;
                }
            })
            .within(Time.minutes(5));
        
        PatternStream<APICall> patternStream = CEP.pattern(
            apiCalls.keyBy(APICall::getEndpoint),
            pattern
        );
        
        return patternStream.select((match) -> {
            List<APICall> slowCalls = new ArrayList<>();
            slowCalls.addAll(match.get("slow1"));
            slowCalls.addAll(match.get("slow2"));
            slowCalls.addAll(match.get("slow3"));
            
            double avgResponseTime = slowCalls.stream()
                .mapToLong(APICall::getResponseTime)
                .average()
                .orElse(0);
            
            return new PerformanceAlert(
                slowCalls.get(0).getEndpoint(),
                slowCalls.size(),
                avgResponseTime,
                System.currentTimeMillis()
            );
        });
    }
    
    /**
     * Pattern: Login anomaly detection
     * 
     * Failed login followed by successful login from different IP
     */
    public static DataStream<SecurityAlert> detectLoginAnomaly(
        DataStream<LoginEvent> loginEvents
    ) {
        Pattern<LoginEvent, ?> pattern = Pattern
            .<LoginEvent>begin("failed")
            .where(new IterativeCondition<LoginEvent>() {
                @Override
                public boolean filter(LoginEvent event, Context<LoginEvent> ctx) {
                    return !event.isSuccessful();
                }
            })
            .times(3)  // 3 failed attempts
            .followedBy("success")
            .where(new IterativeCondition<LoginEvent>() {
                @Override
                public boolean filter(LoginEvent event, Context<LoginEvent> ctx) {
                    if (!event.isSuccessful()) {
                        return false;
                    }
                    
                    // Check if IP is different from failed attempts
                    Iterable<LoginEvent> failedEvents = ctx.getEventsForPattern("failed");
                    for (LoginEvent failed : failedEvents) {
                        if (event.getIpAddress().equals(failed.getIpAddress())) {
                            return false;  // Same IP, not anomalous
                        }
                    }
                    
                    return true;  // Different IP, suspicious
                }
            })
            .within(Time.minutes(10));
        
        PatternStream<LoginEvent> patternStream = CEP.pattern(
            loginEvents.keyBy(LoginEvent::getUserId),
            pattern
        );
        
        return patternStream.select((match) -> {
            List<LoginEvent> failed = match.get("failed");
            LoginEvent success = match.get("success").get(0);
            
            return new SecurityAlert(
                success.getUserId(),
                "Login from different IP after failed attempts",
                failed.size(),
                success.getIpAddress()
            );
        });
    }
    
    /**
     * Pattern: Gradual degradation
     * 
     * Error rate increases gradually over time
     */
    public static DataStream<DegradationAlert> detectGradualDegradation(
        DataStream<ServiceMetric> metrics
    ) {
        Pattern<ServiceMetric, ?> pattern = Pattern
            .<ServiceMetric>begin("baseline")
            .where(new IterativeCondition<ServiceMetric>() {
                @Override
                public boolean filter(ServiceMetric metric, Context<ServiceMetric> ctx) {
                    return metric.getErrorRate() < 0.01;  // <1% error rate
                }
            })
            .followedBy("elevated")
            .where(new IterativeCondition<ServiceMetric>() {
                @Override
                public boolean filter(ServiceMetric metric, Context<ServiceMetric> ctx) {
                    ServiceMetric baseline = ctx.getEventsForPattern("baseline")
                        .iterator().next();
                    return metric.getErrorRate() > baseline.getErrorRate() * 2;
                }
            })
            .followedBy("critical")
            .where(new IterativeCondition<ServiceMetric>() {
                @Override
                public boolean filter(ServiceMetric metric, Context<ServiceMetric> ctx) {
                    ServiceMetric baseline = ctx.getEventsForPattern("baseline")
                        .iterator().next();
                    return metric.getErrorRate() > baseline.getErrorRate() * 5;
                }
            })
            .within(Time.minutes(30));
        
        PatternStream<ServiceMetric> patternStream = CEP.pattern(
            metrics.keyBy(ServiceMetric::getServiceName),
            pattern
        );
        
        return patternStream.select((match) -> {
            ServiceMetric baseline = match.get("baseline").get(0);
            ServiceMetric critical = match.get("critical").get(0);
            
            return new DegradationAlert(
                critical.getServiceName(),
                baseline.getErrorRate(),
                critical.getErrorRate(),
                critical.getTimestamp()
            );
        });
    }
}
```

---

## 7. Production Patterns

### Event Schema Evolution

```java
package com.dateng.patterns;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * Event versioning for schema evolution
 */
@JsonIgnoreProperties(ignoreUnknown = true)  // Ignore new fields in old consumers
public class OrderPlacedEventV2 {
    
    @JsonProperty("event_version")
    private String eventVersion = "2.0";
    
    @JsonProperty("order_id")
    private String orderId;
    
    @JsonProperty("user_id")
    private String userId;
    
    @JsonProperty("items")
    private List<OrderItem> items;
    
    // V2: Added shipping address
    @JsonProperty("shipping_address")
    private Address shippingAddress;
    
    // V2: Added discount code
    @JsonProperty("discount_code")
    private String discountCode;
    
    // Backward compatibility: Provide defaults for new fields
    public Address getShippingAddress() {
        return shippingAddress != null ? shippingAddress : new Address();
    }
    
    public String getDiscountCode() {
        return discountCode != null ? discountCode : "";
    }
}
```

**Schema Evolution Strategies:**

1. **Additive Changes Only:** Add new fields, never remove
2. **Default Values:** New fields have sensible defaults
3. **Version Field:** Track event version
4. **Upcasting:** Convert old events to new schema

---

### Idempotent Event Processing

```java
package com.dateng.patterns;

import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Ensure events are processed exactly once
 */
public class IdempotentEventProcessor {
    
    private final Set<String> processedEventIds = ConcurrentHashMap.newKeySet();
    
    /**
     * Process event idempotently
     */
    public void processEvent(DomainEvent event) {
        String eventId = event.getEventId();
        
        // Check if already processed
        if (processedEventIds.contains(eventId)) {
            System.out.printf("Event %s already processed, skipping%n", eventId);
            return;
        }
        
        try {
            // Process event
            doProcess(event);
            
            // Mark as processed
            processedEventIds.add(eventId);
        } catch (Exception e) {
            // Don't mark as processed if failed
            System.err.printf("Failed to process event %s: %s%n", eventId, e.getMessage());
            throw e;
        }
    }
    
    private void doProcess(DomainEvent event) {
        // Actual processing logic
        System.out.printf("Processing event: %s%n", event.getEventId());
    }
}
```

**Production Implementation:**
- Store processed event IDs in Redis with TTL
- Use database transaction + unique constraint
- Implement at-least-once delivery + idempotent processing

---

### Saga Pattern (Distributed Transactions)

```java
package com.dateng.patterns;

/**
 * Saga pattern for distributed transactions
 * 
 * Example: Order placement saga
 * 1. Reserve inventory
 * 2. Process payment
 * 3. Create shipment
 * 
 * If any step fails, compensate previous steps
 */
public class OrderPlacementSaga {
    
    private final InventoryService inventoryService;
    private final PaymentService paymentService;
    private final ShipmentService shipmentService;
    
    public OrderPlacementSaga(
        InventoryService inventoryService,
        PaymentService paymentService,
        ShipmentService shipmentService
    ) {
        this.inventoryService = inventoryService;
        this.paymentService = paymentService;
        this.shipmentService = shipmentService;
    }
    
    /**
     * Execute saga
     */
    public void execute(OrderPlacedEvent event) {
        String orderId = event.getOrderId();
        String reservationId = null;
        String paymentId = null;
        
        try {
            // Step 1: Reserve inventory
            reservationId = inventoryService.reserve(event.getItems());
            System.out.printf("‚úì Inventory reserved: %s%n", reservationId);
            
            // Step 2: Process payment
            paymentId = paymentService.charge(event.getUserId(), event.getTotalAmount());
            System.out.printf("‚úì Payment processed: %s%n", paymentId);
            
            // Step 3: Create shipment
            String shipmentId = shipmentService.createShipment(event.getShippingAddress());
            System.out.printf("‚úì Shipment created: %s%n", shipmentId);
            
            // Success: Publish OrderCompletedEvent
            publishEvent(new OrderCompletedEvent(orderId, shipmentId));
            
        } catch (InventoryException e) {
            // No compensation needed (first step failed)
            System.err.println("‚úó Failed to reserve inventory");
            publishEvent(new OrderFailedEvent(orderId, "Inventory unavailable"));
            
        } catch (PaymentException e) {
            // Compensate: Release inventory
            System.err.println("‚úó Failed to process payment");
            if (reservationId != null) {
                inventoryService.release(reservationId);
                System.out.println("‚Üª Inventory reservation released");
            }
            publishEvent(new OrderFailedEvent(orderId, "Payment failed"));
            
        } catch (ShipmentException e) {
            // Compensate: Refund payment + release inventory
            System.err.println("‚úó Failed to create shipment");
            if (paymentId != null) {
                paymentService.refund(paymentId);
                System.out.println("‚Üª Payment refunded");
            }
            if (reservationId != null) {
                inventoryService.release(reservationId);
                System.out.println("‚Üª Inventory reservation released");
            }
            publishEvent(new OrderFailedEvent(orderId, "Shipment failed"));
        }
    }
    
    private void publishEvent(DomainEvent event) {
        // Publish to event bus (Kafka)
    }
}
```

---

## 8. Real-World Case Studies

### Case Study 1: Netflix - Event-Driven Microservices

**Scenario:** Video playback events trigger multiple downstream systems.

```
Video Play Event
    ‚Üì
Event Bus (Kafka)
    ‚Üì
‚îú‚îÄ‚Üí Recommendation Engine (update viewing history)
‚îú‚îÄ‚Üí Billing Service (track watch time)
‚îú‚îÄ‚Üí Analytics (engagement metrics)
‚îú‚îÄ‚Üí Content Delivery (pre-fetch next episode)
‚îî‚îÄ‚Üí Social Features (update "watching now")
```

**Implementation:**
```java
public class VideoPlayEventHandler {
    
    @Subscribe
    public void handleVideoPlay(VideoPlayEvent event) {
        // Update recommendations
        recommendationEngine.recordView(
            event.getUserId(),
            event.getVideoId(),
            event.getTimestamp()
        );
        
        // Track billing
        billingService.recordWatchTime(
            event.getUserId(),
            event.getDuration()
        );
        
        // Analytics
        analytics.trackEngagement(event);
        
        // Pre-fetch next episode
        if (event.isNearingEnd()) {
            contentDelivery.prefetch(
                event.getUserId(),
                event.getNextEpisodeId()
            );
        }
    }
}
```

**Key Learnings:**
- Event fanout to multiple consumers
- Async processing for low latency
- Each service has its own database (no shared DB)

---

### Case Study 2: Uber - Trip State Machine

**Scenario:** Trip lifecycle as event sourcing.

```
Events:
1. TripRequested
2. DriverAssigned
3. DriverArrived
4. TripStarted
5. TripCompleted
6. PaymentProcessed

State Machine:
REQUESTED ‚Üí ASSIGNED ‚Üí IN_PROGRESS ‚Üí COMPLETED ‚Üí PAID
```

**Implementation:**
```java
public class Trip {
    private String tripId;
    private TripState state;
    
    public void handle(TripRequestedEvent event) {
        this.tripId = event.getTripId();
        this.state = TripState.REQUESTED;
        apply(event);
    }
    
    public void handle(DriverAssignedEvent event) {
        if (state != TripState.REQUESTED) {
            throw new IllegalStateException("Cannot assign driver in state: " + state);
        }
        this.state = TripState.ASSIGNED;
        apply(event);
    }
    
    public void handle(TripStartedEvent event) {
        if (state != TripState.ASSIGNED) {
            throw new IllegalStateException("Cannot start trip in state: " + state);
        }
        this.state = TripState.IN_PROGRESS;
        apply(event);
    }
    
    // ... more state transitions
}
```

**Key Learnings:**
- State machines enforce business rules
- Event sourcing provides complete trip history
- Replay events for debugging/analytics

---

### Case Study 3: LinkedIn - Feed Ranking with Kafka Streams

**Scenario:** Real-time feed ranking based on engagement.

```java
public class FeedRankingTopology {
    
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();
        
        // Stream of post interactions
        KStream<String, Interaction> interactions = builder.stream("interactions");
        
        // Calculate engagement score per post
        KTable<String, Double> postScores = interactions
            .groupBy((key, interaction) -> interaction.getPostId())
            .aggregate(
                () -> 0.0,
                (postId, interaction, score) -> {
                    // Weight different interaction types
                    switch (interaction.getType()) {
                        case LIKE: return score + 1.0;
                        case COMMENT: return score + 5.0;
                        case SHARE: return score + 10.0;
                        default: return score;
                    }
                },
                Materialized.with(Serdes.String(), Serdes.Double())
            );
        
        // Apply time decay
        KTable<String, Double> decayedScores = postScores
            .mapValues((score) -> score * Math.exp(-0.1 * getHoursSincePost()));
        
        // Output ranked posts
        decayedScores.toStream().to("ranked_posts");
        
        KafkaStreams streams = new KafkaStreams(builder.build(), getConfig());
        streams.start();
    }
}
```

**Key Learnings:**
- Real-time aggregations with Kafka Streams
- Time-based decay for relevance
- Stateful processing at scale

---

## Summary

### When to Use Each Pattern

| Pattern | Use When | Example |
|---------|----------|---------|
| **CEP** | Detect patterns across events | Fraud detection, monitoring |
| **Event Sourcing** | Need complete history | Banking, audit trail |
| **CQRS** | Different read/write needs | Dashboard + transactional system |
| **Kafka Streams** | Kafka-centric architecture | Real-time analytics |
| **Saga** | Distributed transactions | Order processing |

### Production Checklist

‚úÖ **Schema Evolution:** Version events, handle old versions  
‚úÖ **Idempotency:** Process events exactly once  
‚úÖ **Monitoring:** Track lag, throughput, errors  
‚úÖ **Dead Letter Queue:** Handle poison messages  
‚úÖ **Backpressure:** Handle slow consumers  
‚úÖ **Ordering:** Partition by key for ordering  
‚úÖ **Replay:** Support event replay for debugging  

---

## Further Reading

**Books:**
- "Designing Event-Driven Systems" by Ben Stopford
- "Building Event-Driven Microservices" by Adam Bellemare
- "Event Sourcing and CQRS" by Vaughn Vernon

**Technologies:**
- Apache Kafka (event streaming)
- Apache Flink (CEP)
- EventStoreDB (event sourcing)
- Axon Framework (CQRS + Event Sourcing)

**Companies Using EDA:**
- Netflix (microservices)
- Uber (trip processing)
- LinkedIn (feed ranking)
- Amazon (order processing)
- Airbnb (booking workflow)

---

**End of Event Processing Module**

**Total Lines: ~5,000+**

This module provides Principal-level depth on event-driven architecture, CEP, event sourcing, CQRS, Kafka Streams, and production patterns with real-world case studies from FAANG companies.
