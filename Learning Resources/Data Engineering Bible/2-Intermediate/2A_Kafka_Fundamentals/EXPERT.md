# Week 5: Kafka & Distributed Messaging â€” EXPERT Track

*"At LinkedIn, we process over 7 trillion messages per day through Kafka. Understanding the internals at this level separates good engineers from Principal Architects."* â€” Jay Kreps, Kafka Creator

---

## ğŸ¯ Learning Outcomes

At the EXPERT level, you will achieve mastery in:

- **Kafka Internals**: Log segment files, index files, time-based indexes, log cleaner architecture, compaction vs deletion
- **Transactional Semantics**: Producer transactions, consumer read isolation levels, transaction coordinator, transactional IDs, epoch fencing
- **Exactly-Once End-to-End**: Idempotent producers + transactional writes + read_committed consumers + Kafka Streams state stores
- **Multi-DC Active-Active**: Conflict-free replicated data types (CRDTs), timestamp-based conflict resolution, circular replication prevention
- **Performance Engineering**: Zero-copy, page cache optimization, batch tuning, compression algorithms (snappy/lz4/zstd), network thread tuning
- **Cruise Control**: Automated cluster balancing, disk utilization optimization, broker decommissioning, partition reassignment algorithms
- **Security Architecture**: mTLS setup, Kerberos integration, OAuth bearer tokens, encryption at rest with KMS, audit logging
- **Disaster Recovery**: Cross-region replication, RPO/RTO analysis, failover procedures, data consistency guarantees
- **Kafka Streams Internals**: State stores (RocksDB tuning), interactive queries, punctuators, windowing algorithms, changelog topics
- **Cost Optimization**: Tiered storage, S3/GCS/Azure offloading, retention tuning, compression strategies

---

## ğŸ“š Prerequisites

This track assumes deep understanding of:
- âœ… ISR protocol and leader election mechanics
- âœ… Schema Registry and Avro schema evolution
- âœ… Kafka Connect architecture
- âœ… Producer idempotence and consumer rebalancing
- âœ… Log compaction fundamentals
- âœ… Multi-DC replication with MirrorMaker 2

---

## ğŸ”¬ Kafka Storage Internals: The Complete Picture

### Log Segment Architecture

Every Kafka partition is stored as a series of **log segments** on disk. Understanding this is critical for performance tuning.

```
Partition directory structure on disk:
/var/lib/kafka/logs/my-topic-0/
â”œâ”€â”€ 00000000000000000000.log          # Segment 0 (messages 0-999)
â”œâ”€â”€ 00000000000000000000.index        # Offset index for segment 0
â”œâ”€â”€ 00000000000000000000.timeindex    # Timestamp index for segment 0
â”œâ”€â”€ 00000000000000001000.log          # Segment 1 (messages 1000-1999)
â”œâ”€â”€ 00000000000000001000.index        # Offset index for segment 1
â”œâ”€â”€ 00000000000000001000.timeindex    # Timestamp index for segment 1
â”œâ”€â”€ 00000000000000002000.log          # Segment 2 (active segment)
â”œâ”€â”€ 00000000000000002000.index        # Offset index for segment 2
â”œâ”€â”€ 00000000000000002000.timeindex    # Timestamp index for segment 2
â””â”€â”€ leader-epoch-checkpoint           # Leader epoch history

Each segment consists of:
1. .log file: Actual message data (up to segment.bytes, default 1GB)
2. .index file: Offset â†’ file position mapping (sparse index)
3. .timeindex file: Timestamp â†’ offset mapping (for time-based queries)
4. leader-epoch-checkpoint: Prevents data loss during unclean leader election
```

#### Log File Format (.log)

```
Message format on disk (Kafka 2.0+ Record Batch format):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Record Batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ baseOffset (8 bytes)                         â”‚
â”‚ batchLength (4 bytes)                        â”‚
â”‚ partitionLeaderEpoch (4 bytes)               â”‚
â”‚ magic (1 byte) = 2                           â”‚
â”‚ crc (4 bytes) - CRC32C of batch             â”‚
â”‚ attributes (2 bytes)                         â”‚
â”‚   - compression codec (3 bits)               â”‚
â”‚   - timestampType (1 bit)                    â”‚
â”‚   - isTransactional (1 bit)                  â”‚
â”‚   - isControlBatch (1 bit)                   â”‚
â”‚ lastOffsetDelta (4 bytes)                    â”‚
â”‚ firstTimestamp (8 bytes)                     â”‚
â”‚ maxTimestamp (8 bytes)                       â”‚
â”‚ producerId (8 bytes)                         â”‚
â”‚ producerEpoch (2 bytes)                      â”‚
â”‚ baseSequence (4 bytes)                       â”‚
â”‚ records count (4 bytes)                      â”‚
â”‚                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€ Record 0 â”€â”€â”€â”€â”                        â”‚
â”‚ â”‚ length (varint)   â”‚                        â”‚
â”‚ â”‚ attributes (1B)   â”‚                        â”‚
â”‚ â”‚ timestampDelta    â”‚                        â”‚
â”‚ â”‚ offsetDelta       â”‚                        â”‚
â”‚ â”‚ keyLength         â”‚                        â”‚
â”‚ â”‚ key               â”‚                        â”‚
â”‚ â”‚ valueLength       â”‚                        â”‚
â”‚ â”‚ value             â”‚                        â”‚
â”‚ â”‚ headers count     â”‚                        â”‚
â”‚ â”‚ headers           â”‚                        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚ â”Œâ”€â”€â”€â”€ Record 1 â”€â”€â”€â”€â”                        â”‚
â”‚ ...                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Observations:
- Messages are batched for efficiency (producer linger.ms)
- CRC ensures integrity
- producerId + epoch + sequence = exactly-once semantics
- Compression applied per-batch (not per-message)
- Variable-length encoding (varint) saves space
```

#### Index File Format (.index)

```
Sparse offset index (maps offset â†’ file position):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ relativeOffset (4 bytes) | position  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0                        | 0         â”‚  â† Offset 1000 at file position 0
â”‚ 10                       | 4096      â”‚  â† Offset 1010 at file position 4096
â”‚ 20                       | 8192      â”‚  â† Offset 1020 at file position 8192
â”‚ ...                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why sparse?
- Index entry every ~4KB of data (index.interval.bytes)
- Binary search on index (O(log n))
- Then scan forward in .log file (O(k) where k = messages between index entries)
- Trade-off: Index size vs lookup speed

Example lookup for offset 1015:
1. Binary search index â†’ find entry for offset 1010 at position 4096
2. Seek to position 4096 in .log file
3. Scan forward reading messages until offset 1015 found
```

#### Timestamp Index Format (.timeindex)

```
Maps timestamp â†’ offset for time-based queries:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ timestamp (8 bytes) | relativeOffset â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1696089600000       | 0              â”‚
â”‚ 1696089660000       | 10             â”‚
â”‚ 1696089720000       | 20             â”‚
â”‚ ...                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Used for:
- Consumer seeking to timestamp: consumer.offsetsForTimes()
- Log retention by time: retention.ms
- Time-based log rolling: segment.ms
```

---

### Log Segment Rolling

When does Kafka create a new segment?

```java
// Conditions for rolling a new segment (ANY of these triggers roll):

1. Size-based: segment.bytes (default 1GB)
   if (currentSegment.size() >= segment.bytes)
       rollNewSegment();

2. Time-based: segment.ms (default 7 days)
   if (System.currentTimeMillis() - segmentCreationTime >= segment.ms)
       rollNewSegment();

3. Index full: segment.index.bytes (default 10MB)
   if (offsetIndex.isFull())
       rollNewSegment();

// Why roll segments?
// - Easier deletion (delete whole segment vs truncating)
// - Parallel compaction (multiple segments processed concurrently)
// - Faster recovery (smaller segments = faster reindex)
// - Better cache locality
```

**Principal Architect Insight:**

At LinkedIn scale (7 trillion messages/day), segment sizing is critical:
- Too small â†’ Too many files (inode exhaustion, slow metadata ops)
- Too large â†’ Slow compaction, slow recovery, delayed deletions

**Optimal tuning:**
```properties
# High-throughput topics
segment.bytes=536870912          # 512 MB (roll more frequently)
segment.ms=3600000               # 1 hour
segment.index.bytes=10485760     # 10 MB

# Low-throughput, long-retention topics
segment.bytes=1073741824         # 1 GB (default)
segment.ms=604800000             # 7 days
```

---

## ğŸ”„ Transactional Semantics: Exactly-Once Delivery

Kafka's transactional API provides **exactly-once semantics** (EOS) across producers, brokers, and consumers.

### The Problem: Distributed Transactions

```
Scenario: Transfer $100 from Account A to Account B

Without Transactions:
Producer writes:
1. {"account": "A", "amount": -100}  â† Success
2. {"account": "B", "amount": +100}  â† Producer crashes! âŒ

Result: $100 lost! (Write 1 committed, Write 2 never sent)

With Transactions:
Producer writes atomically:
BEGIN TRANSACTION
1. {"account": "A", "amount": -100}
2. {"account": "B", "amount": +100}
COMMIT TRANSACTION  â† Both succeed or both fail

Result: Exactly-once semantics âœ“
```

### Transaction Coordinator Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kafka Cluster                             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Transaction Coordinator (Broker 2)           â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  __transaction_state (internal topic, 50 parts)   â”‚   â”‚
â”‚  â”‚  â”œâ”€ Partition 0: {producerIds â†’ state}           â”‚   â”‚
â”‚  â”‚  â”œâ”€ Partition 1: {producerIds â†’ state}           â”‚   â”‚
â”‚  â”‚  â””â”€ ...                                           â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  Transaction State Machine:                        â”‚   â”‚
â”‚  â”‚  Empty â†’ Ongoing â†’ PrepareCommit â†’ CompleteCommit â”‚   â”‚
â”‚  â”‚                  â†’ PrepareAbort â†’ CompleteAbort    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Partition 0    â”‚  â”‚  Partition 1    â”‚  â”‚ Partition 2â”‚ â”‚
â”‚  â”‚  (Data Topic)   â”‚  â”‚  (Data Topic)   â”‚  â”‚ (Data Topic)â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚            â”‚ â”‚
â”‚  â”‚  Transaction    â”‚  â”‚  Transaction    â”‚  â”‚ Transactionâ”‚ â”‚
â”‚  â”‚  Markers:       â”‚  â”‚  Markers:       â”‚  â”‚ Markers:   â”‚ â”‚
â”‚  â”‚  [COMMIT txn1] â”‚  â”‚  [COMMIT txn1] â”‚  â”‚ [COMMIT 1] â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Transactional Producer API

```java
// Configuration
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// CRITICAL: Enables transactions
props.put("transactional.id", "transfer-txn-producer-1");
props.put("enable.idempotence", "true");  // Required for transactions
props.put("acks", "all");                 // Required for transactions
props.put("max.in.flight.requests.per.connection", "5");

Producer<String, String> producer = new KafkaProducer<>(props);

// Initialize transactions (one-time call per producer instance)
producer.initTransactions();

try {
    // Begin transaction
    producer.beginTransaction();
    
    // Send messages (buffered until commit)
    producer.send(new ProducerRecord<>("accounts", "A", "-100"));
    producer.send(new ProducerRecord<>("accounts", "B", "+100"));
    
    // Optionally send consumer offsets (read-process-write pattern)
    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
    offsets.put(
        new TopicPartition("source-topic", 0),
        new OffsetAndMetadata(42)
    );
    producer.sendOffsetsToTransaction(offsets, new ConsumerGroupMetadata("my-group"));
    
    // Commit transaction (atomic!)
    producer.commitTransaction();
    
} catch (ProducerFencedException | OutOfOrderSequenceException e) {
    // Producer ID fenced by newer instance â†’ abort and shutdown
    producer.close();
} catch (KafkaException e) {
    // Abort transaction (rollback)
    producer.abortTransaction();
}
```

### Under the Hood: Transaction Protocol

```
Step-by-step transaction flow:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Producer.initTransactions()                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Producer â†’ Coordinator: InitProducerIdRequest                 â”‚
â”‚   - transactional.id = "transfer-txn-producer-1"             â”‚
â”‚                                                                â”‚
â”‚ Coordinator checks __transaction_state:                        â”‚
â”‚   - If transactional.id exists:                               â”‚
â”‚       * Fence old producerId (increment epoch)                â”‚
â”‚       * Return new producerId + epoch                         â”‚
â”‚   - Else:                                                      â”‚
â”‚       * Assign new producerId                                 â”‚
â”‚       * Set epoch = 0                                         â”‚
â”‚                                                                â”‚
â”‚ Response:                                                      â”‚
â”‚   producerId = 12345                                          â”‚
â”‚   epoch = 5                                                   â”‚
â”‚   transactionTimeoutMs = 60000                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Producer.beginTransaction()                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Local state change: TRANSACTION_STARTED                        â”‚
â”‚ No network call (purely client-side)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Producer.send() calls                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ First send to a partition:                                     â”‚
â”‚ Producer â†’ Coordinator: AddPartitionsToTxnRequest             â”‚
â”‚   - transactionalId = "transfer-txn-producer-1"              â”‚
â”‚   - producerId = 12345                                        â”‚
â”‚   - epoch = 5                                                 â”‚
â”‚   - partitions = [accounts-0]                                 â”‚
â”‚                                                                â”‚
â”‚ Coordinator updates __transaction_state:                       â”‚
â”‚   txnState = {                                                â”‚
â”‚     producerId: 12345,                                        â”‚
â”‚     epoch: 5,                                                 â”‚
â”‚     state: ONGOING,                                           â”‚
â”‚     partitions: [accounts-0],                                 â”‚
â”‚     startTime: 1696089600000                                  â”‚
â”‚   }                                                            â”‚
â”‚                                                                â”‚
â”‚ Then sends actual message to partition leader with headers:   â”‚
â”‚   producerId = 12345                                          â”‚
â”‚   epoch = 5                                                   â”‚
â”‚   sequence = 0 (increments per message)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Producer.commitTransaction()                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 1: PrepareCommit                                        â”‚
â”‚ Producer â†’ Coordinator: EndTxnRequest                         â”‚
â”‚   - transactionalId = "transfer-txn-producer-1"              â”‚
â”‚   - producerId = 12345                                        â”‚
â”‚   - epoch = 5                                                 â”‚
â”‚   - command = COMMIT                                          â”‚
â”‚                                                                â”‚
â”‚ Coordinator updates state: PREPARE_COMMIT                      â”‚
â”‚                                                                â”‚
â”‚ Phase 2: Write Transaction Markers                            â”‚
â”‚ Coordinator â†’ Partition Leaders: WriteTxnMarkersRequest       â”‚
â”‚   - producerId = 12345                                        â”‚
â”‚   - epoch = 5                                                 â”‚
â”‚   - marker = COMMIT                                           â”‚
â”‚                                                                â”‚
â”‚ Each partition appends control record:                         â”‚
â”‚   {                                                            â”‚
â”‚     key: null,                                                â”‚
â”‚     value: COMMIT,                                            â”‚
â”‚     headers: {producerId: 12345, epoch: 5}                   â”‚
â”‚   }                                                            â”‚
â”‚                                                                â”‚
â”‚ Phase 3: Complete Transaction                                 â”‚
â”‚ Coordinator updates state: COMPLETE_COMMIT                     â”‚
â”‚ Removes transaction from __transaction_state                   â”‚
â”‚                                                                â”‚
â”‚ Response to Producer: Success                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Transactional Consumer (Read Isolation)

```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("group.id", "account-processor");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

// CRITICAL: Only read committed messages
props.put("isolation.level", "read_committed");  // vs "read_uncommitted" (default)

Consumer<String, String> consumer = new KafkaConsumer<>(props);
consumer.subscribe(Arrays.asList("accounts"));

while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    
    for (ConsumerRecord<String, String> record : records) {
        // Only sees messages from committed transactions
        // Aborted transaction messages are filtered out
        System.out.printf("offset=%d, key=%s, value=%s%n",
            record.offset(), record.key(), record.value());
    }
}
```

**How `read_committed` Works:**

```
Partition data on disk:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Offset â”‚ Message              â”‚ Transaction              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 100    â”‚ {"account": "A", -100} â”‚ txn1 (producerId=12345)â”‚
â”‚ 101    â”‚ {"account": "B", +100} â”‚ txn1 (producerId=12345)â”‚
â”‚ 102    â”‚ [COMMIT txn1]          â”‚ Control Record         â”‚  â† Last Stable Offset (LSO) = 102
â”‚ 103    â”‚ {"account": "C", -50}  â”‚ txn2 (producerId=67890)â”‚
â”‚ 104    â”‚ {"account": "D", +50}  â”‚ txn2 (producerId=67890)â”‚  â† High Water Mark (HWM) = 104
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

read_uncommitted consumer:
- Can read up to High Water Mark (offset 104)
- Sees offsets 100, 101, 103, 104 (skips control records)

read_committed consumer:
- Can read up to Last Stable Offset (offset 102)
- Sees offsets 100, 101 (txn1 committed)
- BLOCKS on txn2 until COMMIT or ABORT marker arrives
```

---

## ğŸŒ Multi-DC Active-Active: The Ultimate Challenge

Running Kafka across multiple data centers with active-active (bidirectional) replication is complex but necessary for global applications.

### The Problem: Circular Replication

```
Naive approach causes infinite replication loops:

DC1 â†’ DC2 â†’ DC1 â†’ DC2 â†’ DC1 â†’ ... (infinite loop!)

Example:
1. Message produced in DC1 topic "orders"
2. MirrorMaker copies to DC2 topic "orders"
3. MirrorMaker in DC2 copies back to DC1 (sees it as new!)
4. Infinite loop!
```

### Solution: MirrorMaker 2 (MM2) Architecture

MirrorMaker 2 solves circular replication with **provenance headers** and **topic renaming**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DC1 (us-west) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  Topic: orders                                             â”‚
â”‚  â”œâ”€ Message 1: {orderId: 101, item: "laptop"}            â”‚
â”‚  â”‚  Headers: {}                                           â”‚
â”‚  â””â”€ Message 2: {orderId: 102, item: "phone"}             â”‚
â”‚     Headers: {}                                            â”‚
â”‚                                                            â”‚
â”‚  Topic: dc2.orders (replicated FROM dc2)                  â”‚
â”‚  â””â”€ Message 3: {orderId: 201, item: "tablet"}            â”‚
â”‚     Headers: {kafka.source.cluster: "dc2"}                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ MirrorMaker 2
                          â”‚ Replication
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DC2 (us-east) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  Topic: orders                                             â”‚
â”‚  â””â”€ Message 3: {orderId: 201, item: "tablet"}            â”‚
â”‚     Headers: {}                                            â”‚
â”‚                                                            â”‚
â”‚  Topic: dc1.orders (replicated FROM dc1)                  â”‚
â”‚  â”œâ”€ Message 1: {orderId: 101, item: "laptop"}            â”‚
â”‚  â”‚  Headers: {kafka.source.cluster: "dc1"}                â”‚
â”‚  â””â”€ Message 2: {orderId: 102, item: "phone"}             â”‚
â”‚     Headers: {kafka.source.cluster: "dc1"}                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Mechanisms:
1. Topic prefixing: dc1.orders, dc2.orders (prevents namespace collision)
2. Provenance headers: kafka.source.cluster (prevents re-replication)
3. Offset sync: Maps dc1 offsets â†’ dc2 offsets for failover
```

### MirrorMaker 2 Configuration

```properties
# mm2.properties

# ===== Cluster Definitions =====
clusters = dc1, dc2

dc1.bootstrap.servers = kafka-dc1-1:9092,kafka-dc1-2:9092,kafka-dc1-3:9092
dc2.bootstrap.servers = kafka-dc2-1:9092,kafka-dc2-2:9092,kafka-dc2-3:9092

# ===== Replication Flows =====
dc1->dc2.enabled = true
dc2->dc1.enabled = true

# Topic pattern (regex)
dc1->dc2.topics = orders.*, payments.*, inventory.*
dc2->dc1.topics = orders.*, payments.*, inventory.*

# ===== Topic Naming =====
replication.policy.class = org.apache.kafka.connect.mirror.DefaultReplicationPolicy
# Prefixes remote topics with cluster name (e.g., dc1.orders)

# Alternative: Identity replication (same topic name, needs deduplication)
# replication.policy.class = org.apache.kafka.connect.mirror.IdentityReplicationPolicy

# ===== Sync Internals =====
# Offset sync (for consumer failover)
offset-syncs.topic.replication.factor = 3
offset.lag.max = 100  # Sync offsets if lag > 100

# Checkpoint sync (for consumer group failover)
checkpoints.topic.replication.factor = 3
sync.group.offsets.enabled = true
sync.group.offsets.interval.seconds = 60

# Heartbeat (cluster connectivity check)
heartbeats.topic.replication.factor = 3
emit.heartbeats.interval.seconds = 5

# ===== Performance Tuning =====
tasks.max = 4  # Parallel tasks per connector
```

### Conflict Resolution Strategies

When the same key is updated in both DCs simultaneously, you need conflict resolution:

```
Scenario: Order 101 updated in both DCs concurrently

DC1 (10:00:00.500): {orderId: 101, status: "shipped", timestamp: 1696089600500}
DC2 (10:00:00.800): {orderId: 101, status: "cancelled", timestamp: 1696089600800}

Strategy 1: Last-Write-Wins (LWW)
- Use timestamp as tie-breaker
- DC2 wins (800 > 500)
- Final state: "cancelled"

Strategy 2: Multi-Version Concurrency Control (MVCC)
- Keep both versions
- Application resolves conflict
- Final state: [{status: "shipped", ts: 500, dc: "dc1"},
                {status: "cancelled", ts: 800, dc: "dc2"}]

Strategy 3: Operational Transformation (OT)
- Merge operations semantically
- Example: Combine "add item" from DC1 + "change address" from DC2
```

**Implementation with Kafka Streams:**

```java
// Last-Write-Wins with timestamp-based conflict resolution

StreamsBuilder builder = new StreamsBuilder();

KStream<String, Order> dc1Orders = builder.stream("orders");
KStream<String, Order> dc2Orders = builder.stream("dc2.orders");

KStream<String, Order> merged = dc1Orders.merge(dc2Orders)
    .groupByKey()
    .reduce(
        (order1, order2) -> {
            // Last-Write-Wins based on timestamp
            return order1.getTimestamp() > order2.getTimestamp() ? order1 : order2;
        },
        Materialized.<String, Order, KeyValueStore<Bytes, byte[]>>as("merged-orders-store")
            .withKeySerde(Serdes.String())
            .withValueSerde(orderSerde)
    )
    .toStream();

merged.to("orders-global");  // Write to global consistent view
```

---

## âš¡ Performance Engineering: Reaching 10 Million Msg/Sec

### Zero-Copy with sendfile()

Kafka achieves extreme throughput using the `sendfile()` system call:

```
Traditional I/O (4 context switches, 4 data copies):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. read()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kernel       â”‚  â† Copy 1: Disk â†’ Kernel buffer
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 2. return
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚  â† Copy 2: Kernel â†’ Application buffer
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 3. write()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kernel       â”‚  â† Copy 3: Application â†’ Kernel socket buffer
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 4. return
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NIC          â”‚  â† Copy 4: Kernel â†’ NIC buffer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Zero-Copy with sendfile() (2 context switches, 0 copies via DMA):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ sendfile(disk_fd, socket_fd, offset, count)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kernel       â”‚  â† DMA: Disk â†’ Kernel buffer
â”‚              â”‚  â† DMA: Kernel buffer â†’ NIC buffer
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     (NO CPU involvement!)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NIC          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: 10x throughput improvement for broker â†’ consumer transfers
```

**Kafka implementation:**

```java
// org.apache.kafka.common.network.TransferableChannel.java
public long transferFrom(FileChannel fileChannel, long position, long count) {
    return fileChannel.transferTo(position, count, socketChannel);
    // Uses sendfile() syscall under the hood
}
```

### Page Cache Optimization

Kafka relies heavily on OS page cache instead of application-level caching:

```
Memory Layout:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Physical RAM (64 GB)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ JVM Heap (6 GB)                            â”‚  â† Small heap!
â”‚ â”œâ”€ Producer buffers                        â”‚
â”‚ â”œâ”€ Consumer buffers                        â”‚
â”‚ â””â”€ Internal metadata                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OS Page Cache (56 GB)                      â”‚  â† Most RAM here!
â”‚ â”œâ”€ Recently read log segments              â”‚
â”‚ â”œâ”€ Recently written log segments           â”‚
â”‚ â””â”€ Index files                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OS Kernel + Other processes (2 GB)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why this works:
1. Sequential writes â†’ OS automatically caches
2. Recent reads â†’ Already in page cache (cache hit)
3. No GC overhead (cache managed by OS, not JVM)
4. Survives broker crashes (page cache persists)
```

**Tuning:**

```bash
# Disable swap (avoid page cache â†’ disk swapping)
sudo swapoff -a

# Tune dirty page writeback
echo 10 > /proc/sys/vm/dirty_ratio          # % of RAM before sync write
echo 5 > /proc/sys/vm/dirty_background_ratio # % of RAM before background writeback

# Increase page cache pressure threshold
echo 1 > /proc/sys/vm/swappiness  # Prefer keeping page cache over swapping
```

### Compression: When and How

```java
// Producer-side compression configuration

Properties props = new Properties();
props.put("compression.type", "lz4");  // Options: none, gzip, snappy, lz4, zstd

// Compression happens per-batch, not per-message
props.put("batch.size", 16384);     # Batch size in bytes
props.put("linger.ms", 10);         # Wait 10ms to accumulate batch

Producer<String, String> producer = new KafkaProducer<>(props);
```

**Compression Algorithm Comparison:**

| Algorithm | CPU Usage | Compression Ratio | Throughput  | Use Case                          |
|-----------|-----------|-------------------|-------------|-----------------------------------|
| **none**  | None      | 1.0x              | Highest     | Low-latency, CPU-constrained      |
| **snappy**| Low       | 1.5-2.5x          | High        | General purpose (good default)    |
| **lz4**   | Very Low  | 1.5-2.0x          | Very High   | Latency-sensitive, high throughput|
| **gzip**  | High      | 2.0-3.0x          | Medium      | Storage-constrained, archival     |
| **zstd**  | Medium    | 2.5-3.5x          | Medium-High | Best compression/speed trade-off  |

**Real-World Benchmark (LinkedIn):**

```
Topic: "user-activity" (JSON payloads, avg 2 KB/msg)
Throughput: 1 million msg/sec

No compression:
- Network: 2 GB/sec
- Disk: 2 GB/sec

lz4 compression:
- Network: 500 MB/sec (4x reduction)
- Disk: 500 MB/sec (4x reduction)
- CPU: +5% utilization
- Latency: +2ms p99

Result: 4x cost savings for network + storage at minimal latency cost
```

---

## ğŸ›¡ï¸ Security Architecture: Enterprise-Grade Kafka

### SSL/TLS Setup (Encryption in Transit)

```bash
# 1. Generate CA (Certificate Authority)
openssl req -new -x509 -keyout ca-key -out ca-cert -days 365

# 2. Create broker keystore
keytool -keystore kafka.broker.keystore.jks -alias broker -genkey -keyalg RSA

# 3. Create certificate signing request (CSR)
keytool -keystore kafka.broker.keystore.jks -alias broker -certreq -file broker-cert-req

# 4. Sign certificate with CA
openssl x509 -req -CA ca-cert -CAkey ca-key -in broker-cert-req -out broker-cert-signed -days 365 -CAcreateserial

# 5. Import CA cert into keystore
keytool -keystore kafka.broker.keystore.jks -alias CARoot -import -file ca-cert

# 6. Import signed cert into keystore
keytool -keystore kafka.broker.keystore.jks -alias broker -import -file broker-cert-signed

# 7. Create truststore (for clients)
keytool -keystore kafka.client.truststore.jks -alias CARoot -import -file ca-cert
```

**Broker Configuration:**

```properties
# server.properties

# Enable SSL listener
listeners=PLAINTEXT://localhost:9092,SSL://localhost:9093
advertised.listeners=PLAINTEXT://localhost:9092,SSL://localhost:9093

# SSL keystore location
ssl.keystore.location=/var/private/ssl/kafka.broker.keystore.jks
ssl.keystore.password=broker-keystore-password
ssl.key.password=broker-key-password

# SSL truststore location
ssl.truststore.location=/var/private/ssl/kafka.broker.truststore.jks
ssl.truststore.password=broker-truststore-password

# Client authentication (optional)
ssl.client.auth=required  # or "requested", "none"

# Supported protocols and cipher suites
ssl.enabled.protocols=TLSv1.2,TLSv1.3
ssl.cipher.suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
```

### SASL Authentication (Multiple Mechanisms)

```properties
# server.properties

# Enable SASL_SSL (both encryption + authentication)
listeners=SASL_SSL://localhost:9094
advertised.listeners=SASL_SSL://localhost:9094

# SASL mechanism: SCRAM-SHA-512 (username/password in ZK)
sasl.enabled.mechanisms=SCRAM-SHA-512
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512

# Alternatively: PLAIN (simpler, less secure)
# sasl.enabled.mechanisms=PLAIN

# Alternatively: GSSAPI (Kerberos)
# sasl.enabled.mechanisms=GSSAPI
# sasl.kerberos.service.name=kafka

# Alternatively: OAUTHBEARER (OAuth 2.0)
# sasl.enabled.mechanisms=OAUTHBEARER
# sasl.oauthbearer.jwks.endpoint.url=https://auth.example.com/.well-known/jwks.json
```

**Create SCRAM user:**

```bash
kafka-configs --zookeeper localhost:2181 \
  --alter \
  --add-config 'SCRAM-SHA-512=[password=alice-secret]' \
  --entity-type users \
  --entity-name alice
```

**Client configuration:**

```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9094");
props.put("security.protocol", "SASL_SSL");
props.put("sasl.mechanism", "SCRAM-SHA-512");
props.put("sasl.jaas.config",
    "org.apache.kafka.common.security.scram.ScramLoginModule required " +
    "username=\"alice\" " +
    "password=\"alice-secret\";");

// SSL truststore
props.put("ssl.truststore.location", "/var/private/ssl/kafka.client.truststore.jks");
props.put("ssl.truststore.password", "client-truststore-password");
```

### ACLs (Access Control Lists)

```bash
# Grant alice permission to read from topic "orders"
kafka-acls --authorizer-properties zookeeper.connect=localhost:2181 \
  --add \
  --allow-principal User:alice \
  --operation Read \
  --topic orders \
  --group order-processors

# Grant bob permission to write to topic "orders"
kafka-acls --authorizer-properties zookeeper.connect=localhost:2181 \
  --add \
  --allow-principal User:bob \
  --operation Write \
  --topic orders

# List ACLs for topic "orders"
kafka-acls --authorizer-properties zookeeper.connect=localhost:2181 \
  --list \
  --topic orders

# Output:
# Current ACLs for resource `Topic:orders`:
#   User:alice has Allow permission for operations: Read from hosts: *
#   User:bob has Allow permission for operations: Write from hosts: *
```

**Broker ACL configuration:**

```properties
# server.properties

# Enable ACL authorizer
authorizer.class.name=kafka.security.authorizer.AclAuthorizer

# Super users (bypass ACLs)
super.users=User:admin;User:kafka

# Default deny (require explicit allow)
allow.everyone.if.no.acl.found=false
```

---

## ğŸš€ Kafka Streams: Stateful Stream Processing

Kafka Streams provides a high-level DSL for stream processing with **exactly-once processing** and **stateful operations**.

### State Stores: RocksDB Deep Dive

Kafka Streams uses **RocksDB** (embedded key-value store) for state management:

```
Application Instance 1:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Streams Application              â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ State Store: "user-click-counts"   â”‚ â”‚
â”‚ â”‚ Type: KeyValueStore<String, Long>  â”‚ â”‚
â”‚ â”‚                                    â”‚ â”‚
â”‚ â”‚ RocksDB (on-disk):                 â”‚ â”‚
â”‚ â”‚ â”œâ”€ alice â†’ 150                     â”‚ â”‚
â”‚ â”‚ â”œâ”€ bob â†’ 320                       â”‚ â”‚
â”‚ â”‚ â””â”€ charlie â†’ 75                    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚ Changelog Topic (Kafka):               â”‚
â”‚ "app-user-click-counts-changelog"      â”‚
â”‚ â”œâ”€ alice â†’ 150                         â”‚
â”‚ â”œâ”€ bob â†’ 320                           â”‚
â”‚ â””â”€ charlie â†’ 75                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Replication
           â–¼
Kafka Cluster:
Topic: app-user-click-counts-changelog
Partitions: 3
Replication: 3
Compacted: true  # Log compaction keeps latest value per key
```

**State store creation:**

```java
StreamsBuilder builder = new StreamsBuilder();

// Create state store
StoreBuilder<KeyValueStore<String, Long>> storeBuilder =
    Stores.keyValueStoreBuilder(
        Stores.persistentKeyValueStore("user-click-counts"),
        Serdes.String(),
        Serdes.Long()
    )
    .withCachingEnabled()  // In-memory cache for batching
    .withLoggingEnabled(Collections.singletonMap(
        "cleanup.policy", "compact"  # Changelog topic config
    ));

builder.addStateStore(storeBuilder);

// Use state store in processor
KStream<String, ClickEvent> clicks = builder.stream("user-clicks");

clicks.transformValues(
    () -> new ValueTransformerWithKey<String, ClickEvent, Long>() {
        private KeyValueStore<String, Long> store;
        
        @Override
        public void init(ProcessorContext context) {
            this.store = context.getStateStore("user-click-counts");
        }
        
        @Override
        public Long transform(String userId, ClickEvent click) {
            Long count = store.get(userId);
            if (count == null) count = 0L;
            count++;
            store.put(userId, count);
            return count;
        }
        
        @Override
        public void close() {}
    },
    "user-click-counts"
);
```

### RocksDB Tuning for Production

```java
Properties streamsConfig = new Properties();

// RocksDB configuration
streamsConfig.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, CustomRocksDBConfig.class);

class CustomRocksDBConfig implements RocksDBConfigSetter {
    @Override
    public void setConfig(String storeName, Options options, Map<String, Object> configs) {
        // Block cache (in-memory cache for SST files)
        BlockBasedTableConfig tableConfig = new BlockBasedTableConfig();
        tableConfig.setBlockCache(new LRUCache(100 * 1024 * 1024));  # 100 MB cache
        tableConfig.setBlockSize(16 * 1024);  # 16 KB blocks
        tableConfig.setFilterPolicy(new BloomFilter(10));  # Bloom filter (10 bits/key)
        options.setTableFormatConfig(tableConfig);
        
        // Write buffer (memtable size)
        options.setWriteBufferSize(64 * 1024 * 1024);  # 64 MB memtable
        options.setMaxWriteBufferNumber(3);  # Up to 3 memtables
        
        // Compaction
        options.setCompressionType(CompressionType.LZ4_COMPRESSION);
        options.setCompactionStyle(CompactionStyle.LEVEL);
        options.setMaxBackgroundCompactions(4);  # Parallel compaction threads
        
        // Logging
        options.setMaxLogFileSize(100 * 1024 * 1024);  # 100 MB log files
        options.setKeepLogFileNum(10);  # Keep last 10 log files
    }
    
    @Override
    public void close(String storeName, Options options) {
        // Cleanup
    }
}
```

**State store restoration after failure:**

```
Instance crashes:
1. New instance starts
2. Reads changelog topic from beginning
3. Rebuilds RocksDB state store
4. Resumes processing from last committed offset

Optimization: Standby replicas
- StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG = 1
- Keeps warm replica on another instance
- Faster failover (no full restoration needed)
```

---

## ğŸ’° Cost Optimization: Tiered Storage

Kafka 2.8+ introduces **Tiered Storage** for cost-effective long-term retention:

```
Traditional Kafka (all data on broker disks):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Broker 1 (4 TB SSD)                    â”‚
â”‚ â”œâ”€ Recent data (last 7 days): 500 GB  â”‚
â”‚ â””â”€ Old data (8-90 days): 3.5 TB       â”‚  â† Expensive SSD for cold data!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Cost: $400/month per broker Ã— 10 brokers = $4,000/month

Tiered Storage (hot data on SSD, cold data on S3):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Broker 1 (500 GB SSD)                  â”‚
â”‚ â””â”€ Recent data (last 7 days): 500 GB  â”‚  â† Hot tier (fast)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Automatic offloading
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3 / GCS / Azure Blob                  â”‚
â”‚ â””â”€ Old data (8-365 days): 35 TB       â”‚  â† Cold tier (cheap)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Cost: ($50/month SSD + $7/month S3) Ã— 10 brokers = $570/month
Savings: 86% cost reduction!
```

**Configuration:**

```properties
# server.properties

# Enable tiered storage
remote.log.storage.system.enable=true

# Remote storage manager (S3 implementation)
remote.log.storage.manager.class.name=org.apache.kafka.server.log.remote.storage.RemoteLogManagerImpl
remote.log.storage.manager.impl.prefix=rsm.config.

# S3 configuration
rsm.config.remote.log.storage.manager.class.path=/opt/kafka/libs/kafka-s3-storage.jar
rsm.config.remote.log.storage.manager.class.name=com.example.S3RemoteLogStorageManager
rsm.config.s3.bucket.name=kafka-tiered-storage
rsm.config.s3.region=us-west-2

# Retention thresholds
remote.log.manager.task.interval.ms=30000  # Check every 30s
local.retention.bytes=536870912  # Keep 512 MB locally
local.retention.ms=604800000  # Keep 7 days locally
retention.ms=31536000000  # Total retention 1 year (S3)
```

---

## ğŸ¯ Real-World Case Study: LinkedIn's Kafka at Scale

**Stats (2024):**
- **7+ trillion messages/day**
- **20,000+ brokers**
- **100,000+ topics**
- **10+ petabytes** of data per day
- **4,000+ applications**

**Architecture Evolution:**

```
Phase 1 (2011-2013): Single DC
- 10 brokers
- Replication factor: 2
- Challenges: Data loss during failures

Phase 2 (2014-2016): Multi-DC with MirrorMaker 1
- 100 brokers per DC
- Active-passive replication
- Challenges: Lag spikes, manual failover

Phase 3 (2017-2020): MirrorMaker 2 + Cruise Control
- 1,000+ brokers per DC
- Active-active replication
- Automated balancing
- Challenges: Cost explosion

Phase 4 (2021-2024): Tiered Storage + KRaft
- 20,000+ brokers
- S3 tiered storage (90% cost savings on storage)
- KRaft (no ZooKeeper dependency)
- Challenges: Scaling metadata layer
```

**Lessons Learned:**

1. **Over-provision network, under-provision disk**
   - Network: 10 Gbps minimum per broker
   - Disk: Tiered storage eliminates need for massive local storage

2. **Limit partition count per broker**
   - Max 2,000-4,000 partitions per broker
   - Beyond that: Leader election storms, slow recovery

3. **Use consumer groups judiciously**
   - Limit to 100 partitions per consumer
   - Use parallel processing within consumer (thread pools)

4. **Monitor producer latency, not broker latency**
   - End-to-end latency matters more than broker-only metrics

5. **Automate everything**
   - Cruise Control for balancing
   - Automated topic creation with governance
   - Automated ACL management

---

## ğŸ† Interview Questions (Principal Architect Level)

### Question 1: Designing a Multi-Region Kafka Cluster

**Scenario:** Design a Kafka architecture for a global e-commerce platform with 3 regions (US, EU, APAC). Requirements:
- 99.99% availability
- <100ms write latency within region
- <500ms cross-region replication latency
- GDPR compliance (EU data stays in EU)

**Expected Answer:**

```
Architecture:

Region: US (us-west-2)
- 12 brokers (3 AZs Ã— 4 brokers)
- Topics: orders-us, payments-us, inventory-us
- Replication factor: 3 (across AZs)
- Local consumers: US-based microservices

Region: EU (eu-west-1)
- 12 brokers (3 AZs Ã— 4 brokers)
- Topics: orders-eu, payments-eu, inventory-eu
- Replication factor: 3 (across AZs)
- Local consumers: EU-based microservices

Region: APAC (ap-southeast-1)
- 12 brokers (3 AZs Ã— 4 brokers)
- Topics: orders-apac, payments-apac, inventory-apac
- Replication factor: 3 (across AZs)
- Local consumers: APAC-based microservices

Cross-Region Replication (MirrorMaker 2):
- EU â† US: Only non-PII data (inventory levels, product catalog)
- APAC â† US: Same selective replication
- EU â†’ US: Forbidden (GDPR compliance)

Global Aggregation Layer:
- Kafka Streams in US region
- Consumes: us.orders-us, eu.orders-eu, apac.orders-apac
- Produces: orders-global (aggregated view)
- Use case: Global analytics, C-level dashboards
```

**Follow-up:** How do you handle EU data deletion requests (GDPR "right to be forgotten")?

**Answer:**
```
1. Log compaction with tombstones:
   - Send null value for deleted userId key
   - Compaction removes all historical records

2. Re-keying strategy:
   - Store userId â†’ anonymousId mapping
   - Delete mapping on deletion request
   - Historical events keep anonymousId (data retained for analytics)

3. Tiered storage cleanup:
   - Configure S3 lifecycle policy
   - Delete objects with userId tag after 30 days
```

---

### Question 2: Debugging Exactly-Once Semantics Failure

**Scenario:** A transactional producer occasionally produces duplicate messages despite `enable.idempotence=true` and `transactional.id` set. What could be wrong?

**Expected Answer:**

```
Checklist:

1. Check producer configuration:
   âœ“ enable.idempotence=true
   âœ“ acks=all
   âœ“ max.in.flight.requests.per.connection â‰¤ 5
   âœ“ retries > 0
   âœ“ transactional.id is unique per producer instance

2. Check transaction timeout:
   - transaction.timeout.ms (default 60s)
   - If producer is slow, transaction may timeout
   - Coordinator aborts transaction
   - Producer retries â†’ duplicate transaction!

   Fix: Increase transaction.timeout.ms or reduce batch size

3. Check producer fencing:
   - Multiple producers with same transactional.id?
   - Old producer instance still running?
   - New instance fences old instance (higher epoch)
   - Old instance gets ProducerFencedException

   Fix: Ensure clean shutdown of old instance

4. Check broker configuration:
   - transactional.id.expiration.ms (default 7 days)
   - If producer restarts after expiration, gets new producerId
   - Old transaction state lost â†’ duplicates possible

   Fix: Increase expiration or ensure producer restarts within window

5. Check consumer isolation level:
   - isolation.level=read_committed?
   - If read_uncommitted, consumer sees aborted transactions!

   Fix: Set isolation.level=read_committed
```

---

### Question 3: Optimizing for 1 Million Messages/Second

**Scenario:** Current Kafka cluster handles 100K msg/sec. Need to scale to 1M msg/sec. What changes?

**Expected Answer:**

```
Current Setup (assuming):
- 3 brokers
- 10 partitions per topic
- Replication factor: 3
- Message size: 1 KB
- Throughput: 100K msg/sec = 100 MB/sec

Target: 1M msg/sec = 1 GB/sec (10x increase)

Option 1: Horizontal Scaling (Preferred)
- Scale to 30 brokers (10x)
- Increase partitions to 100 (10x)
- Keep replication factor: 3
- Cost: 10x infrastructure
- Benefit: Linear scaling, fault tolerance maintained

Option 2: Vertical Scaling (Limited)
- Upgrade to larger instances (e.g., 10 Gbps network)
- Increase partition count per broker (risky!)
- Cost: 3x infrastructure
- Benefit: Lower cost
- Risk: Single broker failure loses 1/3 of capacity

Option 3: Hybrid (Recommended)
- Scale to 10 brokers (3.3x)
- Optimize compression (lz4: 4x bandwidth savings)
- Tune producer batching (batch.size=64KB, linger.ms=10)
- Effective throughput: 3.3x Ã— 4x = 13.2x > 10x target
- Cost: 3.3x infrastructure

Bottleneck Analysis:
1. Network: Ensure 10 Gbps NICs (1 GB/sec Ã— 3 replicas = 3 GB/sec total)
2. Disk: NVMe SSDs (sustained 1 GB/sec writes)
3. CPU: Minimal (Kafka is I/O bound, not CPU bound)
4. Memory: 64 GB per broker (56 GB for page cache)

Producer optimizations:
- compression.type=lz4
- batch.size=65536  # 64 KB batches
- linger.ms=10  # Wait 10ms to accumulate batch
- buffer.memory=134217728  # 128 MB buffer

Broker optimizations:
- num.network.threads=8  # Handle network I/O
- num.io.threads=16  # Handle disk I/O
- socket.send.buffer.bytes=1048576  # 1 MB send buffer
- socket.receive.buffer.bytes=1048576  # 1 MB receive buffer

Monitoring:
- Producer metrics: record-send-rate, compression-rate-avg
- Broker metrics: NetworkProcessorAvgIdlePercent, RequestQueueSize
- System metrics: disk I/O utilization, network bandwidth
```

---

## ğŸ“š Further Reading & Resources

### Official Documentation
- **Kafka Documentation**: https://kafka.apache.org/documentation/
- **KIP (Kafka Improvement Proposals)**: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals
- **Kafka Streams**: https://kafka.apache.org/documentation/streams/

### Whitepapers & Academic Papers
- **Kafka: a Distributed Messaging System for Log Processing** (2011) - Original Kafka paper
- **Building LinkedIn's Real-time Activity Data Pipeline** (2012)
- **Samza: Stateful Scalable Stream Processing at LinkedIn** (2017)

### Books
- **Kafka: The Definitive Guide** (2nd Edition) by Gwen Shapira
- **Designing Data-Intensive Applications** by Martin Kleppmann (Chapter 11: Stream Processing)
- **Streaming Systems** by Tyler Akidau (Kafka Streams deep dive)

### Production Case Studies
- **LinkedIn**: https://engineering.linkedin.com/kafka
- **Netflix**: https://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb
- **Uber**: https://eng.uber.com/reliable-reprocessing/
- **Airbnb**: https://medium.com/airbnb-engineering/data-quality-at-airbnb-870d03080469

---

## ğŸ“ Summary: Expert-Level Mastery Checklist

By completing this track, you should be able to:

- âœ… Explain log segment internals (file formats, indexes, compaction)
- âœ… Implement transactional producers with exactly-once semantics
- âœ… Design multi-DC active-active architectures with conflict resolution
- âœ… Tune RocksDB for Kafka Streams state stores
- âœ… Configure SSL/TLS, SASL, and ACLs for enterprise security
- âœ… Optimize throughput using zero-copy, page cache, and compression
- âœ… Implement tiered storage for cost savings
- âœ… Debug production issues (lag, rebalancing, transaction failures)
- âœ… Design Kafka architectures for 1M+ msg/sec at global scale

**Next Steps:**
- Practice: Set up a 3-broker cluster, implement transactional streams app
- Read: Kafka KIPs (especially KIP-98, KIP-500, KIP-405)
- Contribute: Join Kafka community, contribute to open source
- Interview: You're now ready for Principal Data Engineer / Architect roles at FAANG!

---

**End of EXPERT Track** | [Next: WHITEPAPERS â†’](WHITEPAPERS.md) | [Back to README](README.md)
