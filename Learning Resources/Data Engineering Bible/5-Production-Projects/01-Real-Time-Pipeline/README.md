# Real-Time Analytics Pipeline

**Target Role:** Principal Data Engineer / Staff Engineer (40+ LPA)  
**Complexity:** Production-Grade Full Stack  
**Implementation:** Kafka + Flink + Iceberg  
**Deployment:** Docker + Kubernetes  

---

## ğŸ“š Project Overview

This is a **production-ready** real-time analytics pipeline that processes clickstream events from a million-user e-commerce platform.

**Architecture:**
```
Web/Mobile Apps â†’ Kafka â†’ Flink Processing â†’ Iceberg Data Lake â†’ Query Engines
                                â†“
                            Redis Cache
                                â†“
                          Monitoring Stack
                       (Prometheus + Grafana)
```

**What You'll Build:**
- Event ingestion with Kafka (1M events/sec)
- Real-time processing with Flink (sessionization, aggregations)
- Data lake with Iceberg (ACID transactions, time travel)
- Caching layer with Redis
- Complete monitoring and alerting
- Kubernetes deployment with auto-scaling

**Real-World Use Cases:**
- **Netflix:** Real-time viewing metrics for recommendations
- **Uber:** Real-time trip analytics and pricing
- **Airbnb:** Search analytics and pricing optimization
- **LinkedIn:** Feed engagement metrics

---

## Table of Contents

1. [Architecture](#architecture)
2. [Prerequisites](#prerequisites)
3. [Project Structure](#project-structure)
4. [Implementation](#implementation)
   - [Kafka Setup](#1-kafka-setup)
   - [Flink Jobs](#2-flink-jobs)
   - [Iceberg Data Lake](#3-iceberg-data-lake)
   - [Redis Caching](#4-redis-caching)
   - [Monitoring](#5-monitoring)
5. [Deployment](#deployment)
6. [Operations Guide](#operations-guide)
7. [Performance Tuning](#performance-tuning)
8. [Troubleshooting](#troubleshooting)

---

## Architecture

### High-Level Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Producers                        â”‚
â”‚  (Web Apps, Mobile Apps, IoT Devices, Microservices)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Kafka Cluster                          â”‚
â”‚  Topic: clickstream_events (partitions: 30)                 â”‚
â”‚  Replication Factor: 3                                       â”‚
â”‚  Retention: 7 days                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Flink Processing                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Sessionization  â”‚  â”‚  Aggregations    â”‚               â”‚
â”‚  â”‚  Job             â”‚  â”‚  Job             â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                              â”‚
â”‚  Features:                                                   â”‚
â”‚  - Session windows (30min inactivity)                       â”‚
â”‚  - Tumbling windows (1min, 5min, 1hr)                      â”‚
â”‚  - Exactly-once semantics                                   â”‚
â”‚  - Late data handling (5min)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Iceberg Data Lake       â”‚  â”‚    Redis     â”‚
â”‚                           â”‚  â”‚    Cache     â”‚
â”‚  Tables:                  â”‚  â”‚              â”‚
â”‚  - events_raw             â”‚  â”‚  Hot data:   â”‚
â”‚  - sessions               â”‚  â”‚  - Last 1hr  â”‚
â”‚  - metrics_1min           â”‚  â”‚  - Top URLs  â”‚
â”‚  - metrics_hourly         â”‚  â”‚  - Active    â”‚
â”‚                           â”‚  â”‚    users     â”‚
â”‚  Features:                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  - ACID transactions      â”‚
â”‚  - Time travel queries    â”‚
â”‚  - Schema evolution       â”‚
â”‚  - Partition pruning      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Query Engines                            â”‚
â”‚  - Trino (ad-hoc queries)                                   â”‚
â”‚  - Presto (dashboards)                                       â”‚
â”‚  - Spark (batch jobs)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Monitoring Stack                           â”‚
â”‚  - Prometheus (metrics)                                      â”‚
â”‚  - Grafana (visualization)                                   â”‚
â”‚  - Elasticsearch (logs)                                      â”‚
â”‚  - Jaeger (traces)                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**Event Schema:**
```json
{
  "event_id": "uuid",
  "timestamp": "2024-01-15T10:30:00Z",
  "user_id": "user123",
  "session_id": "session456",
  "event_type": "page_view",
  "page_url": "/products/12345",
  "referrer": "https://google.com",
  "user_agent": "Mozilla/5.0...",
  "ip_address": "192.168.1.1",
  "geo": {
    "country": "US",
    "city": "San Francisco"
  },
  "device": {
    "type": "mobile",
    "os": "iOS"
  },
  "metadata": {
    "product_id": "12345",
    "category": "electronics"
  }
}
```

**Processing Steps:**

1. **Ingestion (Kafka)**
   - Events arrive at 1M/sec
   - Partitioned by user_id (for ordering)
   - Compressed with LZ4

2. **Stream Processing (Flink)**
   - Sessionization: Group events by user with 30min timeout
   - Aggregations: Count events per URL, category, device
   - Enrichment: Add geo data, device classification
   - Late data: Accept events up to 5min late

3. **Storage (Iceberg)**
   - Raw events: Partitioned by date, hour
   - Sessions: Partitioned by date
   - Metrics: Partitioned by time window

4. **Caching (Redis)**
   - Last 1hr metrics for dashboards
   - Top 100 URLs with visit counts
   - Active user count (HyperLogLog)

---

## Prerequisites

**Required Skills:**
- Java 11+ (Flink jobs)
- Scala (optional for Flink)
- SQL (Iceberg queries)
- Docker & Kubernetes
- Linux command line

**Tools to Install:**
- Docker Desktop (20.10+)
- kubectl (1.24+)
- Helm (3.0+)
- Maven (3.6+)
- Git

**Cloud Resources (if deploying to cloud):**
- AWS EKS / GCP GKE / Azure AKS
- S3 / GCS / ADLS (for Iceberg storage)
- 3x c5.2xlarge (Kafka brokers)
- 3x c5.4xlarge (Flink task managers)
- 1x r5.xlarge (Redis)

**Estimated Costs:**
- Development (local): $0
- Staging (cloud): ~$500/month
- Production (cloud): ~$5,000/month

---

## Project Structure

```
01-Real-Time-Pipeline/
â”‚
â”œâ”€â”€ README.md                          # This file
â”‚
â”œâ”€â”€ docker/                            # Docker configurations
â”‚   â”œâ”€â”€ docker-compose.yml             # Local dev environment
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ flink/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ prometheus.yml
â”‚       â””â”€â”€ grafana-dashboards/
â”‚
â”œâ”€â”€ k8s/                               # Kubernetes manifests
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”œâ”€â”€ kafka-cluster.yaml
â”‚   â”‚   â””â”€â”€ topics.yaml
â”‚   â”œâ”€â”€ flink/
â”‚   â”‚   â”œâ”€â”€ flink-configuration.yaml
â”‚   â”‚   â”œâ”€â”€ sessionization-job.yaml
â”‚   â”‚   â””â”€â”€ aggregations-job.yaml
â”‚   â”œâ”€â”€ iceberg/
â”‚   â”‚   â””â”€â”€ catalog-config.yaml
â”‚   â”œâ”€â”€ redis/
â”‚   â”‚   â””â”€â”€ redis-cluster.yaml
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ prometheus.yaml
â”‚       â”œâ”€â”€ grafana.yaml
â”‚       â””â”€â”€ alertmanager.yaml
â”‚
â”œâ”€â”€ flink-jobs/                        # Flink streaming jobs
â”‚   â”œâ”€â”€ pom.xml
â”‚   â”œâ”€â”€ src/main/java/com/dateng/
â”‚   â”‚   â”œâ”€â”€ SessionizationJob.java
â”‚   â”‚   â”œâ”€â”€ AggregationsJob.java
â”‚   â”‚   â”œâ”€â”€ schema/
â”‚   â”‚   â”‚   â”œâ”€â”€ ClickstreamEvent.java
â”‚   â”‚   â”‚   â””â”€â”€ Session.java
â”‚   â”‚   â”œâ”€â”€ functions/
â”‚   â”‚   â”‚   â”œâ”€â”€ SessionAssigner.java
â”‚   â”‚   â”‚   â”œâ”€â”€ EventAggregator.java
â”‚   â”‚   â”‚   â””â”€â”€ GeoEnrichment.java
â”‚   â”‚   â””â”€â”€ sinks/
â”‚   â”‚       â”œâ”€â”€ IcebergSink.java
â”‚   â”‚       â””â”€â”€ RedisSink.java
â”‚   â””â”€â”€ src/test/java/
â”‚
â”œâ”€â”€ data-generator/                    # Test data generator
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ ClickstreamGenerator.java
â”‚
â”œâ”€â”€ sql/                               # Iceberg DDL & queries
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â””â”€â”€ sample_queries.sql
â”‚
â”œâ”€â”€ scripts/                           # Helper scripts
â”‚   â”œâ”€â”€ setup-local.sh
â”‚   â”œâ”€â”€ deploy-k8s.sh
â”‚   â”œâ”€â”€ kafka-topics-create.sh
â”‚   â””â”€â”€ monitoring-setup.sh
â”‚
â””â”€â”€ docs/                              # Additional documentation
    â”œâ”€â”€ OPERATIONS.md
    â”œâ”€â”€ PERFORMANCE_TUNING.md
    â””â”€â”€ TROUBLESHOOTING.md
```

---

## Implementation

Next sections will cover:
1. Kafka cluster setup with optimal configurations
2. Flink jobs with exactly-once semantics
3. Iceberg table setup with time travel
4. Redis caching patterns
5. Complete monitoring stack

**File Organization:**
- Part 1: Kafka + Data Producer (01-kafka-setup.md)
- Part 2: Flink Sessionization Job (02-flink-sessionization.md)
- Part 3: Flink Aggregations Job (03-flink-aggregations.md)
- Part 4: Iceberg Data Lake (04-iceberg-setup.md)
- Part 5: Redis Caching (05-redis-caching.md)
- Part 6: Monitoring Stack (06-monitoring.md)
- Part 7: Kubernetes Deployment (07-k8s-deployment.md)
- Part 8: Operations Guide (08-operations.md)

---

## Quick Start

### Local Development (Docker Compose)

```bash
# 1. Clone the repository
git clone https://github.com/your-org/realtime-pipeline.git
cd realtime-pipeline

# 2. Start infrastructure
docker-compose up -d

# 3. Create Kafka topics
./scripts/kafka-topics-create.sh

# 4. Build Flink jobs
cd flink-jobs
mvn clean package

# 5. Submit Flink jobs
./scripts/submit-flink-jobs.sh

# 6. Start data generator
cd data-generator
mvn exec:java -Dexec.mainClass="com.dateng.ClickstreamGenerator"

# 7. View Flink UI
open http://localhost:8081

# 8. View Grafana dashboards
open http://localhost:3000
```

### Production Deployment (Kubernetes)

```bash
# 1. Setup Kubernetes cluster
eksctl create cluster -f k8s/cluster-config.yaml

# 2. Install Kafka with Strimzi operator
kubectl apply -f k8s/kafka/

# 3. Deploy Flink jobs
kubectl apply -f k8s/flink/

# 4. Deploy monitoring stack
kubectl apply -f k8s/monitoring/

# 5. Verify deployments
kubectl get pods -n realtime-pipeline
```

---

## Performance Targets

**Throughput:**
- Kafka ingestion: 1M events/sec
- Flink processing: 1M events/sec
- Iceberg writes: 100K events/sec

**Latency:**
- End-to-end (p99): < 5 seconds
- Sessionization: < 10 seconds
- Aggregations: < 1 second

**Reliability:**
- Uptime: 99.9% (43 min downtime/month)
- Data loss: Zero (exactly-once semantics)
- Recovery time: < 5 minutes

**Scalability:**
- Horizontal: 10x traffic without architecture change
- Vertical: Support 10M concurrent users

---

## Next Steps

Proceed to Part 1 for detailed Kafka setup:
- [01-kafka-setup.md](./01-kafka-setup.md)

---

**End of README**
