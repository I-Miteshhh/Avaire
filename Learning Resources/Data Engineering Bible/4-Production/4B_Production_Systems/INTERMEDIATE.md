# Week 15-16: Production Data Systems â€” INTERMEDIATE Track

*"Every system design is a trade-off. Understanding consistency vs availability, latency vs throughput, and cost vs performance separates L6 engineers from L5."* â€” Staff Engineer, Meta

---

## ğŸ¯ Learning Outcomes

- Master trade-off analysis (CAP theorem, PACELC, consistency models)
- Design for cost optimization
- Implement SLA/SLI/SLO frameworks
- Handle data partitioning and sharding strategies
- Design disaster recovery (RTO/RPO)
- Understand multi-region architectures
- Implement backpressure and flow control
- Build self-healing systems

---

## âš–ï¸ 1. Trade-Off Analysis

### The Universal Truth

```
"You cannot optimize for everything. Every decision is a trade-off."

Common Trade-Offs:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Optimize For        â”‚ Trade Off            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Low Latency         â”‚ High Cost            â”‚
â”‚ High Availability   â”‚ Consistency          â”‚
â”‚ Scalability         â”‚ Complexity           â”‚
â”‚ Strong Consistency  â”‚ Performance          â”‚
â”‚ Simplicity          â”‚ Features             â”‚
â”‚ Flexibility         â”‚ Performance          â”‚
â”‚ Durability          â”‚ Latency              â”‚
â”‚ Cost                â”‚ Everything else      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”º 2. CAP Theorem

### The Impossible Triangle

```
CAP Theorem (Brewer, 2000):
"A distributed system can provide at most 2 of the following 3 guarantees"

C = Consistency:
    All nodes see the same data at the same time
    Read reflects most recent write

A = Availability:
    Every request receives a response (success or failure)
    No request hangs indefinitely

P = Partition Tolerance:
    System continues to operate despite network partitions
    Some nodes cannot communicate with others

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Consistency (C)                            â”‚
â”‚              /\                                    â”‚
â”‚             /  \                                   â”‚
â”‚            /    \                                  â”‚
â”‚           /  CP  \                                 â”‚
â”‚          /  (e.g.,\                                â”‚
â”‚         /   MongoDB)\                              â”‚
â”‚        /            \                              â”‚
â”‚       /              \                             â”‚
â”‚      /                \                            â”‚
â”‚     /   CA (impossible \                           â”‚
â”‚    /    in distributed) \                          â”‚
â”‚   /                      \                         â”‚
â”‚  /________________________\                        â”‚
â”‚ Availability (A)           Partition Tolerance (P) â”‚
â”‚         AP                                         â”‚
â”‚    (e.g., Cassandra)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reality: P is mandatory (networks always partition)
So the real choice is: CP vs AP
```

---

### CP Systems (Consistency + Partition Tolerance)

```
Examples: MongoDB, HBase, Redis (single master), Spanner

Behavior during partition:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Normal Operation:                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚ â”‚ Node A â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ Node B â”‚  (Connected)          â”‚
â”‚ â”‚ value=5â”‚       â”‚ value=5â”‚                       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                    â”‚
â”‚ Write to A: value=10                               â”‚
â”‚ âœ… Replicate to B â†’ Both have value=10            â”‚
â”‚                                                    â”‚
â”‚ During Partition:                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   X   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚ â”‚ Node A â”‚ â”€ â”€ â”€ â”‚ Node B â”‚  (Disconnected)       â”‚
â”‚ â”‚ value=10â”‚      â”‚ value=10â”‚                      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                    â”‚
â”‚ Write to A: value=20                               â”‚
â”‚ âŒ Cannot replicate to B                          â”‚
â”‚ âŒ A rejects write OR becomes unavailable          â”‚
â”‚                                                    â”‚
â”‚ Result: Consistency guaranteed, availability lost â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Use Cases:
â”œâ”€ Financial transactions (no inconsistency allowed)
â”œâ”€ Inventory management (prevent overselling)
â””â”€ Strong consistency requirements
```

---

### AP Systems (Availability + Partition Tolerance)

```
Examples: Cassandra, DynamoDB, Riak, CouchDB

Behavior during partition:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Normal Operation:                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚ â”‚ Node A â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ Node B â”‚  (Connected)          â”‚
â”‚ â”‚ value=5â”‚       â”‚ value=5â”‚                       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                    â”‚
â”‚ Write to A: value=10                               â”‚
â”‚ âœ… Eventually replicate to B                       â”‚
â”‚                                                    â”‚
â”‚ During Partition:                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   X   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚ â”‚ Node A â”‚ â”€ â”€ â”€ â”‚ Node B â”‚  (Disconnected)       â”‚
â”‚ â”‚ value=10â”‚      â”‚ value=10â”‚                      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                    â”‚
â”‚ Write to A: value=20 âœ… (succeeds)                â”‚
â”‚ Write to B: value=30 âœ… (succeeds)                â”‚
â”‚                                                    â”‚
â”‚ After partition heals:                            â”‚
â”‚ â”œâ”€ Conflict: A has value=20, B has value=30       â”‚
â”‚ â””â”€ Resolve via:                                   â”‚
â”‚    â”œâ”€ Last-write-wins (timestamp)                 â”‚
â”‚    â”œâ”€ Vector clocks (causal ordering)             â”‚
â”‚    â””â”€ Application-level merge                     â”‚
â”‚                                                    â”‚
â”‚ Result: Availability guaranteed, consistency lost â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Use Cases:
â”œâ”€ Social media (temporary inconsistency OK)
â”œâ”€ Shopping carts (can merge later)
â””â”€ High availability required (24/7 uptime)
```

---

### PACELC Theorem (Extension of CAP)

```
PACELC = "If Partition, choose A or C, Else choose L or C"

During partition (P):
â”œâ”€ Choose Availability (A) or Consistency (C)

When no partition (E = Else):
â”œâ”€ Choose Latency (L) or Consistency (C)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ System       â”‚ If P        â”‚ Else        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DynamoDB     â”‚ AP          â”‚ EL (low L)  â”‚
â”‚ Cassandra    â”‚ AP          â”‚ EL (low L)  â”‚
â”‚ MongoDB      â”‚ CP          â”‚ EC (strong) â”‚
â”‚ HBase        â”‚ CP          â”‚ EC (strong) â”‚
â”‚ Spanner      â”‚ CP          â”‚ EC (strong) â”‚
â”‚ Cosmos DB    â”‚ Tunable     â”‚ Tunable     â”‚
â”‚ (consistency â”‚             â”‚             â”‚
â”‚  levels)     â”‚             â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ 3. Consistency Models

### Spectrum of Consistency

```
Strongest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Weakest
â”‚                                                   â”‚
Linearizability                          Eventual
(Strict)                                 Consistency
â”‚            â”‚           â”‚           â”‚              â”‚
â”‚            â”‚           â”‚           â”‚              â”‚
Sequential   Causal      Session     Read-your-writes

Linearizability (Strongest):
â”œâ”€ Reads always return most recent write
â”œâ”€ Global ordering of all operations
â”œâ”€ Example: Single-node database
â””â”€ Cost: High latency (coordination overhead)

Sequential Consistency:
â”œâ”€ All nodes see operations in same order
â”œâ”€ But not necessarily real-time order
â”œâ”€ Example: Spanner (within same region)
â””â”€ Cost: Medium latency

Causal Consistency:
â”œâ”€ Causally related operations seen in order
â”œâ”€ Concurrent operations can be reordered
â”œâ”€ Example: DynamoDB global tables
â””â”€ Cost: Low latency, complex implementation

Session Consistency:
â”œâ”€ Within single session, reads follow writes
â”œâ”€ Different sessions may see different order
â”œâ”€ Example: Azure Cosmos DB (session level)
â””â”€ Cost: Low latency

Read-Your-Writes:
â”œâ”€ User's reads reflect their own writes
â”œâ”€ Other users may see stale data
â”œâ”€ Example: Social media (you see your post immediately)
â””â”€ Cost: Very low latency

Eventual Consistency (Weakest):
â”œâ”€ All replicas eventually converge
â”œâ”€ No guarantee on convergence time
â”œâ”€ Example: S3, DNS, Cassandra (default)
â””â”€ Cost: Lowest latency, highest availability
```

---

### Choosing Consistency Level

```python
# Example: Cassandra consistency levels

# Write with QUORUM (majority)
session.execute(
    "INSERT INTO users (id, name) VALUES (1, 'Alice')",
    consistency_level=ConsistencyLevel.QUORUM
)
# Writes to majority of replicas (2 out of 3)
# Trade-off: Higher latency, stronger consistency

# Read with ONE (fastest)
session.execute(
    "SELECT * FROM users WHERE id = 1",
    consistency_level=ConsistencyLevel.ONE
)
# Reads from 1 replica (fastest)
# Trade-off: Might read stale data

# Strong consistency: R + W > N
# Example: N=3 replicas, W=2 (quorum), R=2 (quorum)
# Guarantees: Read always sees most recent write
# Cost: Higher latency (must wait for 2 nodes)

# Eventual consistency: R + W â‰¤ N
# Example: N=3, W=1, R=1
# Guarantees: None (might read stale)
# Cost: Lowest latency (only 1 node)
```

---

## ğŸ’° 4. Cost Optimization

### The $100K/Month Data Pipeline

```
Inefficient Architecture (Cost: $100K/month):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Cluster:                                 â”‚
â”‚ â”œâ”€ 100 nodes Ã— 8 cores Ã— $0.50/hour           â”‚
â”‚ â”œâ”€ Running 24/7 (even when idle)              â”‚
â”‚ â””â”€ Cost: 100 Ã— 8 Ã— $0.50 Ã— 730 = $292K/monthâ”‚
â”‚                                                â”‚
â”‚ S3 Storage:                                    â”‚
â”‚ â”œâ”€ 500 TB Ã— $0.023/GB = $11.5K/month          â”‚
â”‚ â”œâ”€ No lifecycle policy (hot storage)          â”‚
â”‚ â””â”€ No compression (3x larger than needed)     â”‚
â”‚                                                â”‚
â”‚ Total: $292K + $11.5K = $303.5K/month âŒ      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Optimized Architecture (Cost: $30K/month):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spot Instances + Autoscaling:                  â”‚
â”‚ â”œâ”€ 10-100 nodes (autoscale based on load)     â”‚
â”‚ â”œâ”€ Spot pricing (70% discount)                â”‚
â”‚ â”œâ”€ Run only during batch window (8 hours/day) â”‚
â”‚ â””â”€ Cost: 50 Ã— 8 Ã— $0.15 Ã— 240 = $14.4K/month â”‚
â”‚                                                â”‚
â”‚ S3 Intelligent Tiering:                        â”‚
â”‚ â”œâ”€ 500 TB compressed to 150 TB (Parquet snappy)â”‚
â”‚ â”œâ”€ 100 TB hot (frequent access): $2.3K/month  â”‚
â”‚ â”œâ”€ 50 TB warm (S3-IA): $0.6K/month            â”‚
â”‚ â””â”€ 400 TB cold (Glacier): $0.4K/month         â”‚
â”‚ Total storage: $3.3K/month                    â”‚
â”‚                                                â”‚
â”‚ Reserved Instances (1-year):                   â”‚
â”‚ â”œâ”€ 5 baseline nodes (always-on)               â”‚
â”‚ â””â”€ Cost: 5 Ã— 8 Ã— $0.20 Ã— 730 = $5.8K/month   â”‚
â”‚                                                â”‚
â”‚ Network Transfer (optimized):                  â”‚
â”‚ â”œâ”€ Use VPC endpoints (avoid NAT gateway)      â”‚
â”‚ â”œâ”€ Compress before transfer                   â”‚
â”‚ â””â”€ Cost: $2K/month                            â”‚
â”‚                                                â”‚
â”‚ Query Engine (Presto on EMR):                  â”‚
â”‚ â”œâ”€ On-demand cluster (1 hour/day)             â”‚
â”‚ â””â”€ Cost: $1K/month                            â”‚
â”‚                                                â”‚
â”‚ Total: $14.4K + $3.3K + $5.8K + $2K + $1K     â”‚
â”‚      = $26.5K/month âœ… (91% savings!)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Cost Optimization Strategies

```
1. Compute:
   â”œâ”€ Use spot instances (70% discount, handle interruptions)
   â”œâ”€ Autoscale (scale to zero when idle)
   â”œâ”€ Right-size instances (don't over-provision)
   â”œâ”€ Reserved instances for baseline (1-year commitment)
   â””â”€ Serverless for intermittent workloads (Lambda, Fargate)

2. Storage:
   â”œâ”€ Compress data (Parquet with Snappy: 3x compression)
   â”œâ”€ Use lifecycle policies (hot â†’ warm â†’ cold â†’ archive)
   â”œâ”€ Delete old data (retention policy: 3 years, then delete)
   â”œâ”€ Partition data (query only needed partitions)
   â””â”€ Use cheaper storage (S3 vs EBS: 10x cheaper)

3. Network:
   â”œâ”€ Use VPC endpoints (avoid NAT gateway: $0.045/GB)
   â”œâ”€ Compress before transfer (reduce data volume)
   â”œâ”€ Keep data in same region (avoid cross-region: $0.02/GB)
   â””â”€ Use CDN for hot data (CloudFront caching)

4. Query Optimization:
   â”œâ”€ Partition pruning (query only needed partitions)
   â”œâ”€ Predicate pushdown (filter at storage layer)
   â”œâ”€ Column pruning (read only needed columns)
   â”œâ”€ Materialized views (precompute aggregations)
   â””â”€ Query result caching (avoid re-computation)

5. Monitoring:
   â”œâ”€ Set budget alerts (notify when >80% of budget)
   â”œâ”€ Tag resources (track cost by team/project)
   â”œâ”€ Delete unused resources (orphaned EBS volumes)
   â””â”€ Use cost explorer (identify cost spikes)
```

---

## ğŸ“ 5. SLA, SLI, SLO Framework

### Definitions

```
SLI (Service Level Indicator):
â”œâ”€ What: Metric that measures service level
â”œâ”€ Examples:
â”‚  â”œâ”€ Latency: P50, P95, P99 response time
â”‚  â”œâ”€ Availability: Uptime percentage
â”‚  â”œâ”€ Throughput: Requests per second
â”‚  â”œâ”€ Error rate: Percentage of failed requests
â”‚  â””â”€ Freshness: Data staleness in minutes
â””â”€ Measurement: Actual observed values

SLO (Service Level Objective):
â”œâ”€ What: Target for SLI (internal goal)
â”œâ”€ Examples:
â”‚  â”œâ”€ "P95 latency < 200ms"
â”‚  â”œâ”€ "Availability > 99.9%"
â”‚  â”œâ”€ "Data freshness < 15 minutes"
â”‚  â””â”€ "Error rate < 0.1%"
â””â”€ Purpose: Set expectations, measure success

SLA (Service Level Agreement):
â”œâ”€ What: Contract with customers (legal commitment)
â”œâ”€ Examples:
â”‚  â”œâ”€ "99.9% uptime (43 minutes downtime/month)"
â”‚  â”œâ”€ "If violated: 10% refund"
â”‚  â””â”€ "P99 latency < 1 second"
â””â”€ Purpose: Customer expectations, penalties
```

---

### Calculating Error Budget

```
Error Budget = 100% - SLO

Example:
SLO = 99.9% availability
Error Budget = 100% - 99.9% = 0.1%

Time-based budget (30-day month):
Total time: 30 days Ã— 24 hours = 720 hours
Error budget: 720 Ã— 0.1% = 0.72 hours = 43 minutes

Budget Consumption:
â”œâ”€ Week 1: 10 minutes downtime â†’ 10/43 = 23% consumed
â”œâ”€ Week 2: 5 minutes downtime â†’ 5/43 = 12% consumed
â”œâ”€ Week 3: 15 minutes downtime â†’ 15/43 = 35% consumed
â”œâ”€ Week 4: 0 minutes downtime â†’ 0% consumed
â””â”€ Total: 30/43 = 70% consumed âœ… (within budget)

Actions based on budget:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Budget Remainingâ”‚ Action                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ > 50%           â”‚ Ship new features aggressively   â”‚
â”‚ 20-50%          â”‚ Ship features with caution       â”‚
â”‚ 10-20%          â”‚ Focus on reliability             â”‚
â”‚ < 10%           â”‚ Feature freeze, fix bugs only    â”‚
â”‚ 0%              â”‚ Emergency: rollback, incident    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Real-World SLOs

```python
# Example: Data Pipeline SLOs

SLOs = {
    # Latency SLO
    'latency_p50': {
        'sli': 'histogram_quantile(0.50, pipeline_duration_seconds)',
        'slo': 60,  # 1 minute P50
        'unit': 'seconds',
    },
    'latency_p95': {
        'sli': 'histogram_quantile(0.95, pipeline_duration_seconds)',
        'slo': 300,  # 5 minutes P95
        'unit': 'seconds',
    },
    
    # Availability SLO
    'availability': {
        'sli': 'sum(pipeline_success) / sum(pipeline_total)',
        'slo': 0.999,  # 99.9% success rate
        'unit': 'ratio',
    },
    
    # Freshness SLO
    'freshness': {
        'sli': 'time() - pipeline_last_success_timestamp',
        'slo': 900,  # 15 minutes max staleness
        'unit': 'seconds',
    },
    
    # Throughput SLO
    'throughput': {
        'sli': 'rate(pipeline_records_processed[5m])',
        'slo': 10000,  # 10K records/second
        'unit': 'records_per_second',
    },
}

# Monitor SLO compliance
def check_slo_compliance(slo_name, current_value):
    slo_config = SLOs[slo_name]
    target = slo_config['slo']
    
    if slo_name == 'availability':
        compliant = current_value >= target
    else:
        compliant = current_value <= target
    
    if not compliant:
        alert(f"SLO violated: {slo_name} = {current_value} (target: {target})")
    
    return compliant
```

---

## ğŸŒ 6. Multi-Region Architecture

### Single-Region vs Multi-Region

```
Single-Region (Simple):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ us-east-1                                â”‚
â”‚ â”œâ”€ App servers                           â”‚
â”‚ â”œâ”€ Database (primary)                    â”‚
â”‚ â””â”€ S3 bucket                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros:
â”œâ”€ Simple (no replication)
â”œâ”€ Low latency (local reads/writes)
â””â”€ Low cost (no cross-region transfer)

Cons:
â”œâ”€ Single point of failure (region outage = downtime)
â”œâ”€ High latency for distant users (Asia â†’ us-east-1)
â””â”€ No disaster recovery

Multi-Region (Complex):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ us-east-1 (Primary)                      â”‚
â”‚ â”œâ”€ App servers                           â”‚
â”‚ â”œâ”€ Database (read replica)               â”‚
â”‚ â””â”€ S3 bucket (replicated)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Replication
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ eu-west-1 (Secondary)                    â”‚
â”‚ â”œâ”€ App servers                           â”‚
â”‚ â”œâ”€ Database (read replica)               â”‚
â”‚ â””â”€ S3 bucket (replicated)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Replication
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ap-south-1 (Secondary)                   â”‚
â”‚ â”œâ”€ App servers                           â”‚
â”‚ â”œâ”€ Database (read replica)               â”‚
â”‚ â””â”€ S3 bucket (replicated)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros:
â”œâ”€ High availability (region outage = failover)
â”œâ”€ Low latency (route users to nearest region)
â””â”€ Disaster recovery (multi-region backups)

Cons:
â”œâ”€ Complex (replication, consistency)
â”œâ”€ Higher cost (3x storage, cross-region transfer)
â””â”€ Data consistency challenges (eventual consistency)
```

---

### Multi-Region Replication Strategies

```
1. Active-Passive (Disaster Recovery):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Primary      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Secondary    â”‚
   â”‚ (us-east-1)  â”‚ Async     â”‚ (eu-west-1)  â”‚
   â”‚ - All writes â”‚ Replicationâ”‚ - Standby    â”‚
   â”‚ - All reads  â”‚           â”‚ - No traffic â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   Normal: All traffic â†’ Primary
   Failover: Traffic switches â†’ Secondary
   
   Pros: Simple, low cost
   Cons: Secondary unused (wasted $), slow failover

2. Active-Active (Low Latency):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â—„â”€â”€â”€â”€â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Region A     â”‚ Bidirectionalâ”‚ Region B     â”‚
   â”‚ (us-east-1)  â”‚ Replication â”‚ (eu-west-1)  â”‚
   â”‚ - US writes  â”‚             â”‚ - EU writes  â”‚
   â”‚ - US reads   â”‚             â”‚ - EU reads   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   Routing: DNS routes US users â†’ Region A
           DNS routes EU users â†’ Region B
   
   Pros: Low latency, high availability
   Cons: Complex (conflict resolution), high cost

3. Active-Active with Sharding:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Region A     â”‚             â”‚ Region B     â”‚
   â”‚ - Shard 0-99 â”‚             â”‚ - Shard 100- â”‚
   â”‚              â”‚             â”‚   199        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   Routing: user_id % 200 â†’ Shard
           Shard 0-99 â†’ Region A
           Shard 100-199 â†’ Region B
   
   Pros: No conflicts (non-overlapping data)
   Cons: Cross-shard queries expensive
```

---

## ğŸ”„ 7. Disaster Recovery (RTO/RPO)

### Definitions

```
RTO (Recovery Time Objective):
â”œâ”€ How long can we be down?
â”œâ”€ Example: "Restore service within 4 hours"
â””â”€ Metric: Time to recovery

RPO (Recovery Point Objective):
â”œâ”€ How much data loss is acceptable?
â”œâ”€ Example: "Lose at most 1 hour of data"
â””â”€ Metric: Data loss window

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Timeline:                                      â”‚
â”‚                                                â”‚
â”‚ Last Backup   Disaster    Recovery             â”‚
â”‚     â”‚            â”‚            â”‚                â”‚
â”‚     â–¼            â–¼            â–¼                â”‚
â”‚ â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â–º           â”‚
â”‚     â”‚â—„â”€â”€â”€RPOâ”€â”€â”€â”€â–ºâ”‚â—„â”€â”€â”€RTOâ”€â”€â”€â–ºâ”‚                â”‚
â”‚                                                â”‚
â”‚ RPO: Data between last backup and disaster     â”‚
â”‚      is LOST (1 hour in this example)          â”‚
â”‚                                                â”‚
â”‚ RTO: Time to restore service (4 hours)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### RTO/RPO Tiers

```
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tier â”‚ RTO     â”‚ RPO     â”‚ Strategy      â”‚ Cost     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gold â”‚ Minutes â”‚ Seconds â”‚ Active-Active â”‚ Very Highâ”‚
â”‚      â”‚         â”‚         â”‚ Multi-region  â”‚          â”‚
â”‚      â”‚         â”‚         â”‚ Sync replica  â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚Silverâ”‚ Hours   â”‚ Minutes â”‚ Active-Passiveâ”‚ Medium   â”‚
â”‚      â”‚         â”‚         â”‚ Async replica â”‚          â”‚
â”‚      â”‚         â”‚         â”‚ Warm standby  â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚Bronzeâ”‚ Days    â”‚ Hours   â”‚ Backups only  â”‚ Low      â”‚
â”‚      â”‚         â”‚         â”‚ Cold storage  â”‚          â”‚
â”‚      â”‚         â”‚         â”‚ Manual restoreâ”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Example: Gold-Tier DR

```python
# Gold Tier: RTO=5 min, RPO=0 (no data loss)

# Architecture: Active-Active with synchronous replication

# Region A (Primary)
primary_db = Database(
    region='us-east-1',
    mode='read-write',
    replication='sync',
    replicas=['eu-west-1', 'ap-south-1'],
)

# Region B (Secondary)
secondary_db = Database(
    region='eu-west-1',
    mode='read-write',
    replication='sync',
    replicas=['us-east-1', 'ap-south-1'],
)

# Write path (synchronous):
def write_data(data):
    # Write to primary
    primary_db.write(data)
    
    # Wait for sync replication (blocks until all replicas ACK)
    wait_for_replication(replicas=['eu-west-1', 'ap-south-1'])
    
    # Return success only after all replicas confirm
    return 'success'

# Failover (automatic):
def health_check():
    if not primary_db.is_healthy():
        # Promote secondary to primary
        secondary_db.promote_to_primary()
        
        # Update DNS (Route 53 health check)
        update_dns(primary='eu-west-1')
        
        # Alert on-call
        page_oncall('Primary region down, failed over to eu-west-1')

# Cost: 3x compute + 3x storage + cross-region transfer
# RTO: 5 minutes (automatic failover)
# RPO: 0 seconds (synchronous replication, no data loss)
```

---

## ğŸ“Š Summary Checklist

- [ ] Understand fundamental trade-offs (latency vs consistency, cost vs performance)
- [ ] Apply CAP theorem and PACELC to system design
- [ ] Choose appropriate consistency model (linearizability to eventual)
- [ ] Optimize costs (spot instances, lifecycle policies, compression)
- [ ] Define SLAs, SLIs, SLOs with error budgets
- [ ] Design multi-region architectures (active-passive, active-active)
- [ ] Implement disaster recovery with RTO/RPO targets
- [ ] Build self-healing systems with backpressure

**Next:** [EXPERT.md â†’](EXPERT.md)
