# Week 15-16: Production Data Systems â€” WHITEPAPERS

*"The Lambda Architecture paper by Nathan Marz, Kappa by Jay Kreps, and Data Mesh by Zhamak Dehghani fundamentally changed how we think about data platforms at scale."* â€” VP Engineering, LinkedIn

---

## ğŸ“„ Paper 1: Lambda Architecture (Nathan Marz, 2011)

**Link:** http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html

### Abstract

Lambda Architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. It provides a systematic approach to building robust, fault-tolerant, and scalable data systems.

---

### Core Concepts

#### 1. The Three Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Lambda Architecture                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚                     ALL DATA                                 â”‚
â”‚                        â”‚                                     â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚            â–¼                       â–¼                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚    â”‚ Batch Layer    â”‚      â”‚ Speed Layer    â”‚               â”‚
â”‚    â”‚                â”‚      â”‚                â”‚               â”‚
â”‚    â”‚ - Process ALL  â”‚      â”‚ - Process      â”‚               â”‚
â”‚    â”‚   historical   â”‚      â”‚   RECENT data  â”‚               â”‚
â”‚    â”‚   data         â”‚      â”‚   only         â”‚               â”‚
â”‚    â”‚ - High latency â”‚      â”‚ - Low latency  â”‚               â”‚
â”‚    â”‚   (hours)      â”‚      â”‚   (seconds)    â”‚               â”‚
â”‚    â”‚ - Accurate     â”‚      â”‚ - Approximate  â”‚               â”‚
â”‚    â”‚ - Immutable    â”‚      â”‚ - Mutable      â”‚               â”‚
â”‚    â”‚                â”‚      â”‚                â”‚               â”‚
â”‚    â”‚ Tech:          â”‚      â”‚ Tech:          â”‚               â”‚
â”‚    â”‚ - Spark        â”‚      â”‚ - Flink        â”‚               â”‚
â”‚    â”‚ - MapReduce    â”‚      â”‚ - Storm        â”‚               â”‚
â”‚    â”‚ - Hadoop       â”‚      â”‚ - Samza        â”‚               â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚             â”‚                       â”‚                        â”‚
â”‚             â–¼                       â–¼                        â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚ Batch Views      Speed Views        â”‚                  â”‚
â”‚    â”‚ (Precomputed)    (Real-time)        â”‚                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚               â”‚              â”‚                               â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                      â–¼                                       â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚            â”‚ Serving Layer    â”‚                              â”‚
â”‚            â”‚                  â”‚                              â”‚
â”‚            â”‚ - Merge views    â”‚                              â”‚
â”‚            â”‚ - Query API      â”‚                              â”‚
â”‚            â”‚ - Low latency    â”‚                              â”‚
â”‚            â”‚                  â”‚                              â”‚
â”‚            â”‚ query(t) =       â”‚                              â”‚
â”‚            â”‚   batch(0 to T-1)â”‚                              â”‚
â”‚            â”‚   + speed(T to t)â”‚                              â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 2. Properties of Lambda Architecture

```
Property 1: Human Fault-Tolerance
â”œâ”€ Batch layer processes immutable raw data
â”œâ”€ If bug in algorithm â†’ Recompute batch views
â”œâ”€ No data loss, no corruption
â””â”€ Example: Bug in aggregation logic
    â†’ Fix code, rerun batch job, overwrite views âœ…

Property 2: Data is Immutable
â”œâ”€ Only append new data, never update/delete
â”œâ”€ Benefits:
â”‚  â”œâ”€ Simplicity (no complex update logic)
â”‚  â”œâ”€ Debuggability (full history preserved)
â”‚  â””â”€ Reprocessing (can replay any time range)
â””â”€ Example:
    Raw events: [e1, e2, e3, e4, ...]
    Never: UPDATE e1 SET value = X âŒ
    Always: APPEND e5 (correction for e1) âœ…

Property 3: Recomputation
â”œâ”€ Ability to recompute views from scratch
â”œâ”€ Use case: Algorithm change, bug fix, schema evolution
â””â”€ Example:
    Old algorithm: COUNT(DISTINCT user_id)
    New algorithm: HyperLogLog(user_id) (more accurate)
    â†’ Recompute all batch views with new algorithm

Property 4: Separation of Concerns
â”œâ”€ Batch layer: Accuracy, completeness
â”œâ”€ Speed layer: Latency, availability
â”œâ”€ Serving layer: Query performance
â””â”€ Each layer optimized independently
```

---

#### 3. Example: Pageview Counter

```python
# Lambda Architecture for counting pageviews

# ===== Batch Layer =====
# Runs daily, processes ALL historical data

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BatchLayer").getOrCreate()

# Read ALL pageviews from data lake (1 year = 100 TB)
pageviews = spark.read.parquet('s3://datalake/pageviews/')

# Aggregate
batch_views = pageviews.groupBy('url', 'date') \
    .agg({'user_id': 'count'}) \
    .withColumnRenamed('count(user_id)', 'total_views')

# Write to batch view (overwrite daily)
batch_views.write.mode('overwrite').parquet('s3://batch-views/pageviews/')

# Runtime: 4 hours (processes 100 TB)
# Freshness: Up to 1 day old

# ===== Speed Layer =====
# Runs continuously, processes RECENT data only

from pyflink.datastream import StreamExecutionEnvironment

env = StreamExecutionEnvironment.get_execution_environment()

# Read from Kafka (real-time pageviews)
pageviews = env.add_source(FlinkKafkaConsumer('pageviews', ...))

# Aggregate (5-minute tumbling window)
speed_views = pageviews \
    .key_by(lambda x: (x['url'], x['date'])) \
    .window(TumblingEventTimeWindows.of(Time.minutes(5))) \
    .aggregate(CountAggregateFunction())

# Write to speed view (append)
speed_views.add_sink(RedisSink('speed-views'))

# Runtime: Continuous
# Freshness: <5 minutes

# ===== Serving Layer =====
# Merges batch + speed views

def get_pageviews(url, date):
    # Get batch view (accurate, but up to 1 day old)
    batch_count = read_from_parquet(f's3://batch-views/pageviews/{date}/')
    batch_count = batch_count[batch_count['url'] == url]['total_views']
    
    # Get speed view (recent, last 24 hours)
    speed_count = redis.get(f'speed-views:{url}:{date}')
    
    # Merge
    total = batch_count + speed_count
    return total

# Query latency: <100ms
# Accuracy: Eventual (batch layer catches up daily)
```

---

### Critique of Lambda Architecture

```
Pros:
â”œâ”€ âœ… Handles both batch and real-time
â”œâ”€ âœ… Fault-tolerant (recompute batch views)
â”œâ”€ âœ… Scalable (batch and speed layers scale independently)
â””â”€ âœ… Proven at scale (LinkedIn, Twitter, Uber)

Cons:
â”œâ”€ âŒ Complexity: Maintain 2 codebases (batch + speed)
â”œâ”€ âŒ Synchronization: Keep batch and speed views consistent
â”œâ”€ âŒ Cost: Run both batch and streaming infrastructure
â””â”€ âŒ Learning curve: Need expertise in both batch and streaming
```

---

## ğŸ“˜ Paper 2: Kappa Architecture (Jay Kreps, 2014)

**Link:** https://www.oreilly.com/radar/questioning-the-lambda-architecture/

### Abstract

Jay Kreps (creator of Kafka, co-founder of Confluent) proposed **Kappa Architecture** as a simplification of Lambda. Key insight: *"What if everything is a stream?"*

---

### Core Concepts

#### The Kappa Principle

```
"Instead of maintaining batch and speed layers,
 use streaming-only with replayable logs."

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kappa Architecture                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚                     ALL DATA                                 â”‚
â”‚                        â”‚                                     â”‚
â”‚                        â–¼                                     â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                â”‚ Immutable Log â”‚                             â”‚
â”‚                â”‚ (Kafka)       â”‚                             â”‚
â”‚                â”‚ - Infinite    â”‚                             â”‚
â”‚                â”‚   retention   â”‚                             â”‚
â”‚                â”‚ - Replayable  â”‚                             â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                        â”‚                                     â”‚
â”‚                        â–¼                                     â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                â”‚ Stream        â”‚                             â”‚
â”‚                â”‚ Processor     â”‚                             â”‚
â”‚                â”‚ (Flink/Beam)  â”‚                             â”‚
â”‚                â”‚ - Stateful    â”‚                             â”‚
â”‚                â”‚ - Exactly-onceâ”‚                             â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                        â”‚                                     â”‚
â”‚                        â–¼                                     â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                â”‚ Materialized  â”‚                             â”‚
â”‚                â”‚ Views         â”‚                             â”‚
â”‚                â”‚ (Redis, DB)   â”‚                             â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                              â”‚
â”‚ Reprocessing:                                                â”‚
â”‚ 1. Deploy new stream processor (version 2)                  â”‚
â”‚ 2. Replay Kafka from offset 0                               â”‚
â”‚ 3. Build new materialized views                             â”‚
â”‚ 4. Switch traffic to new views                              â”‚
â”‚ 5. Decommission old version                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Key Differences from Lambda

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aspect             â”‚ Lambda          â”‚ Kappa           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layers             â”‚ Batch + Speed   â”‚ Stream only     â”‚
â”‚ Codebases          â”‚ 2 (different)   â”‚ 1 (unified)     â”‚
â”‚ Reprocessing       â”‚ Batch job       â”‚ Replay stream   â”‚
â”‚ Complexity         â”‚ High            â”‚ Medium          â”‚
â”‚ Cost               â”‚ High (both)     â”‚ Medium (stream) â”‚
â”‚ Latency            â”‚ Mixed           â”‚ Low (uniform)   â”‚
â”‚ Storage            â”‚ HDFS/S3         â”‚ Kafka (log)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Example: Kappa Implementation

```python
# Kappa Architecture for pageview counting

# ===== Immutable Log (Kafka) =====
kafka_config = {
    'topic': 'pageviews',
    'partitions': 100,
    'replication_factor': 3,
    'retention': -1,  # Infinite retention âœ…
    'compression': 'zstd',  # 3x compression
}

# Write pageviews to Kafka
producer.send('pageviews', {
    'url': 'https://example.com/article',
    'user_id': 12345,
    'timestamp': '2025-10-15T10:30:00Z',
})

# ===== Stream Processor (Flink) =====
from pyflink.datastream import StreamExecutionEnvironment

env = StreamExecutionEnvironment.get_execution_environment()

# Read from Kafka (can replay from any offset)
pageviews = env.add_source(FlinkKafkaConsumer(
    topics='pageviews',
    deserialization_schema=JsonDeserializationSchema(),
    properties={'bootstrap.servers': 'kafka:9092'},
))

# Aggregate (exactly-once semantics)
aggregated = pageviews \
    .key_by(lambda x: (x['url'], x['date'])) \
    .window(TumblingEventTimeWindows.of(Time.hours(1))) \
    .aggregate(CountAggregateFunction())

# Write to materialized view (Redis)
aggregated.add_sink(RedisSink('pageview-counts'))

# ===== Reprocessing =====
# Scenario: Algorithm changed (use HyperLogLog for unique users)

# Step 1: Deploy new Flink job (version 2) with new algorithm
flink_job_v2 = FlinkJob(
    algorithm='hyperloglog',
    kafka_offset='earliest',  # Replay from beginning
    output_table='pageview-counts-v2',  # New table
)

# Step 2: Wait for catch-up (process 1 year of data)
# Throughput: 100K events/sec Ã— 86400 sec/day Ã— 365 days
#           = 3.15 trillion events
# Time to process: ~3 days (with 200-node cluster)

# Step 3: Switch traffic (blue-green deployment)
app.config['redis_table'] = 'pageview-counts-v2'

# Step 4: Decommission old job
flink_job_v1.stop()

# Result: Reprocessing without maintaining separate batch layer âœ…
```

---

### When to Use Kappa vs Lambda

```
Use Kappa if:
â”œâ”€ âœ… All data can be treated as streams
â”œâ”€ âœ… Reprocessing is acceptable (can replay Kafka)
â”œâ”€ âœ… Team has streaming expertise
â””â”€ âœ… Want to minimize complexity (1 codebase)

Use Lambda if:
â”œâ”€ âœ… Have legacy batch systems (can't rewrite everything)
â”œâ”€ âœ… Very large historical data (100+ PB, replay too slow)
â”œâ”€ âœ… Batch and streaming algorithms differ significantly
â””â”€ âœ… Need to separate concerns (different teams own batch vs stream)

Real-World Examples:
â”œâ”€ Kappa: Uber (replaced Lambda with Kappa in 2017)
â”œâ”€ Lambda: LinkedIn (still uses Lambda for massive scale)
â””â”€ Hybrid: Netflix (Kappa for real-time, batch for ML training)
```

---

## ğŸŒ Paper 3: Data Mesh (Zhamak Dehghani, 2019)

**Link:** https://martinfowler.com/articles/data-monolith-to-mesh.html

### Abstract

Zhamak Dehghani (Principal at ThoughtWorks) proposed **Data Mesh** as a paradigm shift from centralized data platforms to domain-oriented, decentralized data ownership.

---

### Core Principles

#### 1. Domain-Oriented Decentralization

```
Traditional: Centralized Data Platform

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Central Data Team (Bottleneck)                              â”‚
â”‚ â”œâ”€ Owns all data pipelines                                  â”‚
â”‚ â”œâ”€ Ingests from all domains                                 â”‚
â”‚ â””â”€ Serves all teams                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Marketingâ”‚     â”‚ Sales    â”‚     â”‚ Support  â”‚
  â”‚ Team     â”‚     â”‚ Team     â”‚     â”‚ Team     â”‚
  â”‚ (waits)  â”‚     â”‚ (waits)  â”‚     â”‚ (waits)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problems:
â”œâ”€ âŒ Bottleneck: Central team overwhelmed
â”œâ”€ âŒ Slow: Weeks to build new pipeline
â”œâ”€ âŒ Context loss: Data team doesn't understand domain
â””â”€ âŒ Fragile: One team owns all critical infrastructure

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Data Mesh: Decentralized Ownership

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Marketing    â”‚   â”‚ Sales        â”‚   â”‚ Support      â”‚
â”‚ Domain       â”‚   â”‚ Domain       â”‚   â”‚ Domain       â”‚
â”‚              â”‚   â”‚              â”‚   â”‚              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Data     â”‚ â”‚   â”‚ â”‚ Data     â”‚ â”‚   â”‚ â”‚ Data     â”‚ â”‚
â”‚ â”‚ Product  â”‚ â”‚   â”‚ â”‚ Product  â”‚ â”‚   â”‚ â”‚ Product  â”‚ â”‚
â”‚ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚   â”‚ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚   â”‚ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚ â”‚ - Pipelineâ”‚ â”‚   â”‚ â”‚ - Pipelineâ”‚ â”‚   â”‚ â”‚ - Pipelineâ”‚ â”‚
â”‚ â”‚ - Quality â”‚ â”‚   â”‚ â”‚ - Quality â”‚ â”‚   â”‚ â”‚ - Quality â”‚ â”‚
â”‚ â”‚ - SLA     â”‚ â”‚   â”‚ â”‚ - SLA     â”‚ â”‚   â”‚ â”‚ - SLA     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â”‚                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Self-Service     â”‚
              â”‚ Platform         â”‚
              â”‚ (Infrastructure) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
â”œâ”€ âœ… No bottleneck: Each domain owns their data
â”œâ”€ âœ… Fast: Domain team builds pipeline themselves
â”œâ”€ âœ… Context: Domain experts own their data
â””â”€ âœ… Scalable: Add domains without central team
```

---

#### 2. Data as a Product

```
Treat data like a product, not a byproduct

Traditional Data:
â”œâ”€ Dumped into data lake (no quality guarantees)
â”œâ”€ No SLA (might be stale, incomplete)
â”œâ”€ No ownership (who do I ask for help?)
â””â”€ No documentation (what does this field mean?)

Data as Product:
â”œâ”€ âœ… Quality: Schema validation, data quality checks
â”œâ”€ âœ… SLA: Freshness (<15 min), completeness (>99%)
â”œâ”€ âœ… Ownership: Clear owner (Slack channel, PagerDuty)
â”œâ”€ âœ… Documentation: README, data dictionary, examples
â””â”€ âœ… Discoverability: Data catalog, search

Example: "Customer Events" Data Product

Product Spec:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Product: customer_events                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Owner: marketing-data-team@company.com             â”‚
â”‚ Slack: #marketing-data                             â”‚
â”‚                                                    â”‚
â”‚ Schema:                                            â”‚
â”‚ â”œâ”€ event_id: STRING (UUID, required)               â”‚
â”‚ â”œâ”€ customer_id: STRING (UUID, required)            â”‚
â”‚ â”œâ”€ event_type: STRING (ENUM: click, view, purchase)â”‚
â”‚ â”œâ”€ timestamp: TIMESTAMP (UTC, required)            â”‚
â”‚ â””â”€ properties: JSON (optional metadata)            â”‚
â”‚                                                    â”‚
â”‚ SLA:                                               â”‚
â”‚ â”œâ”€ Freshness: <5 minutes (P95)                     â”‚
â”‚ â”œâ”€ Completeness: >99.9% (no data loss)             â”‚
â”‚ â”œâ”€ Availability: 99.9% (43 min downtime/month)     â”‚
â”‚ â””â”€ Schema stability: Backward compatible changes   â”‚
â”‚                                                    â”‚
â”‚ Access:                                            â”‚
â”‚ â”œâ”€ Format: Parquet (Iceberg table)                 â”‚
â”‚ â”œâ”€ Location: s3://data-mesh/customer_events/       â”‚
â”‚ â”œâ”€ Query: Presto (catalog: data_mesh)              â”‚
â”‚ â””â”€ Stream: Kafka (topic: customer-events)          â”‚
â”‚                                                    â”‚
â”‚ Documentation:                                     â”‚
â”‚ â”œâ”€ README: https://wiki.company.com/customer-eventsâ”‚
â”‚ â”œâ”€ Examples: https://github.com/company/examples   â”‚
â”‚ â””â”€ Changelog: Schema version history               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 3. Self-Service Data Infrastructure Platform

```
Platform team provides infrastructure, not pipelines

Platform Capabilities:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Data Pipeline Templates:                       â”‚
â”‚    â”œâ”€ Kafka â†’ S3 (Firehose)                        â”‚
â”‚    â”œâ”€ Database â†’ S3 (DMS)                          â”‚
â”‚    â””â”€ S3 â†’ Iceberg (Spark job)                     â”‚
â”‚                                                    â”‚
â”‚ 2. Data Quality Framework:                         â”‚
â”‚    â”œâ”€ Schema validation (Great Expectations)       â”‚
â”‚    â”œâ”€ Freshness checks (data age < SLA)            â”‚
â”‚    â””â”€ Completeness checks (row count reconciliation)â”‚
â”‚                                                    â”‚
â”‚ 3. Observability:                                  â”‚
â”‚    â”œâ”€ Metrics (Prometheus dashboards)              â”‚
â”‚    â”œâ”€ Logging (Elasticsearch)                      â”‚
â”‚    â””â”€ Alerting (PagerDuty)                         â”‚
â”‚                                                    â”‚
â”‚ 4. Data Catalog:                                   â”‚
â”‚    â”œâ”€ Search (Amundsen, DataHub)                   â”‚
â”‚    â”œâ”€ Lineage (track data flow)                    â”‚
â”‚    â””â”€ Metadata (owner, SLA, schema)                â”‚
â”‚                                                    â”‚
â”‚ 5. Governance:                                     â”‚
â”‚    â”œâ”€ Access control (RBAC)                        â”‚
â”‚    â”œâ”€ Data retention policies                      â”‚
â”‚    â””â”€ Compliance (GDPR, encryption)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Domain teams self-serve using platform âœ…
```

---

#### 4. Federated Computational Governance

```
Centralized policies, decentralized execution

Governance Examples:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Global Policies (Platform Team):                   â”‚
â”‚ â”œâ”€ PII must be encrypted at rest (AES-256)         â”‚
â”‚ â”œâ”€ Data retention: 7 years for compliance          â”‚
â”‚ â”œâ”€ SLA: P95 freshness <15 minutes                  â”‚
â”‚ â””â”€ Schema: Use Avro/Protobuf (not JSON)            â”‚
â”‚                                                    â”‚
â”‚ Automated Enforcement:                             â”‚
â”‚ â”œâ”€ Pipeline CI/CD fails if policy violated         â”‚
â”‚ â”œâ”€ Data quality checks run automatically           â”‚
â”‚ â””â”€ Access control enforced by platform             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Domain teams comply automatically (platform enforces) âœ…
```

---

### Data Mesh vs Traditional

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aspect               â”‚ Traditional     â”‚ Data Mesh       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ownership            â”‚ Central team    â”‚ Domain teams    â”‚
â”‚ Bottleneck           â”‚ Yes (central)   â”‚ No (distributed)â”‚
â”‚ Scalability          â”‚ Linear (1 team) â”‚ Exponential     â”‚
â”‚ Context              â”‚ Lost            â”‚ Preserved       â”‚
â”‚ Time to Pipeline     â”‚ Weeks           â”‚ Days            â”‚
â”‚ Data Quality         â”‚ Variable        â”‚ Product-grade   â”‚
â”‚ SLA                  â”‚ Best-effort     â”‚ Guaranteed      â”‚
â”‚ Documentation        â”‚ Sparse          â”‚ Required        â”‚
â”‚ Discoverability      â”‚ Manual          â”‚ Catalog         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Challenges of Data Mesh

```
Pros:
â”œâ”€ âœ… Scales with organization (no central bottleneck)
â”œâ”€ âœ… Domain expertise (owners understand their data)
â”œâ”€ âœ… Faster iteration (self-service)
â””â”€ âœ… Clear ownership (no ambiguity)

Cons:
â”œâ”€ âŒ Complex coordination (100+ data products)
â”œâ”€ âŒ Requires cultural shift (not just technology)
â”œâ”€ âŒ Higher initial cost (build self-service platform)
â””â”€ âŒ Risk of fragmentation (if governance weak)

When to Use:
â”œâ”€ âœ… Large organization (>500 engineers)
â”œâ”€ âœ… Multiple domains (marketing, sales, engineering, etc.)
â”œâ”€ âœ… Central team is bottleneck
â””â”€ âœ… Willing to invest in platform

When NOT to Use:
â”œâ”€ âŒ Small team (<50 engineers)
â”œâ”€ âŒ Single domain
â””â”€ âŒ Central team works fine
```

---

## ğŸ“š Summary Checklist

- [ ] Understand Lambda Architecture (batch + speed + serving layers)
- [ ] Understand Kappa Architecture (streaming-only with replay)
- [ ] Know when to use Lambda vs Kappa
- [ ] Understand Data Mesh principles (domain ownership, data as product)
- [ ] Design self-service data platforms
- [ ] Implement federated governance
- [ ] Compare centralized vs decentralized data architectures

**Next:** [README.md â†’](README.md) â€” Final Capstone Project!
