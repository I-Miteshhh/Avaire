# Design Distributed Logging System - Complete System Design

**Difficulty:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Interview Frequency:** High (Infrastructure companies, 40 LPA+)  
**Time to Complete:** 35-40 minutes  
**Real-World Examples:** Elasticsearch (ELK), Splunk, Datadog Logs

---

## üìã Problem Statement

**Design a distributed logging system that can:**
- Ingest 10 TB of logs per day (100K log events/sec)
- Support structured logging (JSON) and unstructured (plain text)
- Provide real-time search and filtering
- Aggregate logs from 10K+ microservices
- Retain logs for 30 days (hot) + 1 year (cold storage)
- Support log correlation (trace IDs, request IDs)
- Provide alerting on log patterns
- Handle spikes (10x traffic during incidents)
- Query latency <1 second for recent logs

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LOG INGESTION PIPELINE                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  Application Servers (10K instances):                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ   App 1     ‚îÇ  ‚îÇ   App 2     ‚îÇ  ‚îÇ   App 3     ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (stdout/    ‚îÇ  ‚îÇ (file logs) ‚îÇ  ‚îÇ (syslog)    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  stderr)    ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ         ‚îÇ                ‚îÇ                ‚îÇ                  ‚îÇ
‚îÇ         ‚ñº                ‚ñº                ‚ñº                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ          LOG COLLECTION AGENTS                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  (Filebeat / Fluentd / Vector)                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  1. Tail log files (inotify)                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  2. Parse log format (JSON, syslog, regex)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  3. Add metadata (hostname, pod, namespace)         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  4. Buffer locally (disk queue for reliability)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  5. Batch and compress (gzip)                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  6. Send to Kafka                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                        ‚îÇ                                     ‚îÇ
‚îÇ                        ‚ñº                                     ‚îÇ
‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ          ‚îÇ   KAFKA (Message Buffer)    ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ   ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   Topics:                    ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   - logs-raw (all logs)      ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   - logs-errors (filtering) ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   - logs-audit              ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ                              ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   Retention: 24 hours        ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   Partitions: 100            ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ   Replication: 3             ‚îÇ                   ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                     ‚îÇ                                        ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ         ‚îÇ           ‚îÇ           ‚îÇ                            ‚îÇ
‚îÇ         ‚ñº           ‚ñº           ‚ñº                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ  Stream  ‚îÇ ‚îÇ  Stream  ‚îÇ ‚îÇ  Stream  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ Processor‚îÇ ‚îÇ Processor‚îÇ ‚îÇ Processor‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ    1     ‚îÇ ‚îÇ    2     ‚îÇ ‚îÇ    3     ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ         ‚îÇ           ‚îÇ           ‚îÇ                            ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îÇ                     ‚ñº                                        ‚îÇ
‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ          ‚îÇ  ELASTICSEARCH CLUSTER       ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ  Index pattern:              ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ  logs-YYYY.MM.DD             ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ                              ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ  100 shards √ó 2 replicas     ‚îÇ                   ‚îÇ
‚îÇ          ‚îÇ  500 GB per day              ‚îÇ                   ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 LOG PROCESSING & ENRICHMENT                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  Raw Log:                                                    ‚îÇ
‚îÇ  "2024-01-15 10:30:45 ERROR user login failed: invalid pwd" ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ         ‚ñº                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  PARSING & STRUCTURING (Logstash/Flink)               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Grok pattern:                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  %{TIMESTAMP_ISO8601:timestamp}                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  %{LOGLEVEL:level}                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  %{GREEDYDATA:message}                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Extracted fields:                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  {                                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "timestamp": "2024-01-15T10:30:45Z",              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "level": "ERROR",                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "message": "user login failed: invalid pwd",      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "service": "auth-service",                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "hostname": "web-server-01",                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "pod": "auth-abc123",                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "namespace": "production"                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  }                                                     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                                                     ‚îÇ
‚îÇ         ‚ñº                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  ENRICHMENT                                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1. Add trace context (from distributed tracing):     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     - trace_id: abc123                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     - span_id: def456                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  2. GeoIP lookup (if IP present):                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     - country: US                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     - city: San Francisco                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  3. Add business context:                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     - user_id: 12345                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     - customer_tier: premium                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                                                     ‚îÇ
‚îÇ         ‚ñº                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  FILTERING & SAMPLING                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Drop debug logs in production (too noisy)          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Sample INFO logs (1% rate)                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Keep ALL ERROR/FATAL logs                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Deduplicate identical logs (window: 5 min)         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   SEARCH & QUERYING                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  Query DSL (Elasticsearch):                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GET /logs-*/_search                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  {                                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "query": {                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ      "bool": {                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        "must": [                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ          {"match": {"service": "auth-service"}},      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ          {"range": {"@timestamp": {                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ            "gte": "now-1h",                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ            "lte": "now"                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ          }}}                                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        ],                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        "filter": [                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ          {"term": {"level": "ERROR"}}                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        ]                                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ      }                                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    },                                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    "aggs": {                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ      "errors_over_time": {                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        "date_histogram": {                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ          "field": "@timestamp",                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ          "interval": "5m"                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        }                                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ      }                                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    }                                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  }                                                     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíª Implementation

### **1. Log Shipper (Python Agent)**

```python
import json
import gzip
import time
from kafka import KafkaProducer
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class LogShipper(FileSystemEventHandler):
    """
    Tail log files and ship to Kafka
    """
    
    def __init__(self, log_file_path: str, kafka_topic: str):
        self.log_file = log_file_path
        self.kafka = KafkaProducer(
            bootstrap_servers=['kafka1:9092', 'kafka2:9092'],
            compression_type='gzip',
            batch_size=16384,
            linger_ms=10
        )
        self.topic = kafka_topic
        self.buffer = []
        self.buffer_size = 100
    
    def on_modified(self, event):
        """
        File changed - read new lines
        """
        if event.src_path == self.log_file:
            with open(self.log_file, 'r') as f:
                # Seek to last read position
                f.seek(self.last_position)
                
                for line in f:
                    self._process_log_line(line.strip())
                
                # Update position
                self.last_position = f.tell()
    
    def _process_log_line(self, line: str):
        """
        Parse and enrich log line
        """
        
        # Parse log (assume JSON)
        try:
            log_event = json.loads(line)
        except:
            # Fallback to plain text
            log_event = {'message': line}
        
        # Add metadata
        log_event['@timestamp'] = int(time.time() * 1000)
        log_event['hostname'] = self._get_hostname()
        log_event['service'] = self._get_service_name()
        
        # Buffer
        self.buffer.append(log_event)
        
        if len(self.buffer) >= self.buffer_size:
            self._flush()
    
    def _flush(self):
        """
        Send buffered logs to Kafka
        """
        
        for log in self.buffer:
            self.kafka.send(
                self.topic,
                value=json.dumps(log).encode()
            )
        
        self.kafka.flush()
        self.buffer.clear()


### **2. Log Parser (Flink)**

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer
from pyflink.common.serialization import SimpleStringSchema
import json
import re

def parse_logs():
    """
    Parse raw logs using Flink
    """
    
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # Kafka source
    kafka_source = FlinkKafkaConsumer(
        topics='logs-raw',
        deserialization_schema=SimpleStringSchema(),
        properties={'bootstrap.servers': 'kafka:9092'}
    )
    
    # Read stream
    log_stream = env.add_source(kafka_source)
    
    # Parse and enrich
    parsed_stream = log_stream.map(parse_log_event)
    
    # Filter errors
    error_stream = parsed_stream.filter(lambda log: log['level'] == 'ERROR')
    
    # Kafka sink
    kafka_sink = FlinkKafkaProducer(
        topic='logs-errors',
        serialization_schema=SimpleStringSchema(),
        producer_config={'bootstrap.servers': 'kafka:9092'}
    )
    
    error_stream.map(lambda log: json.dumps(log)).add_sink(kafka_sink)
    
    env.execute("Log Parser")


def parse_log_event(raw_log: str) -> dict:
    """
    Parse log line using regex (Grok-style)
    """
    
    # Pattern: timestamp level message
    pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?P<level>\w+) (?P<message>.*)'
    
    match = re.match(pattern, raw_log)
    
    if match:
        return {
            'timestamp': match.group('timestamp'),
            'level': match.group('level'),
            'message': match.group('message'),
            'raw': raw_log
        }
    else:
        return {'raw': raw_log, 'level': 'UNKNOWN'}


### **3. Elasticsearch Indexer**

```python
from elasticsearch import Elasticsearch, helpers
from kafka import KafkaConsumer
import json

class LogIndexer:
    """
    Index logs into Elasticsearch
    """
    
    def __init__(self):
        self.es = Elasticsearch(['http://es1:9200', 'http://es2:9200'])
        self.kafka = KafkaConsumer(
            'logs-raw',
            bootstrap_servers=['kafka:9092'],
            auto_offset_reset='earliest',
            group_id='log-indexer'
        )
    
    def run(self):
        """
        Consume from Kafka and index to ES
        """
        
        batch = []
        batch_size = 1000
        
        for message in self.kafka:
            log_event = json.loads(message.value)
            
            # Prepare for bulk index
            batch.append({
                '_index': self._get_index_name(log_event['@timestamp']),
                '_source': log_event
            })
            
            if len(batch) >= batch_size:
                self._bulk_index(batch)
                batch.clear()
    
    def _get_index_name(self, timestamp: int) -> str:
        """
        Time-based index: logs-2024.01.15
        """
        from datetime import datetime
        dt = datetime.fromtimestamp(timestamp / 1000)
        return f"logs-{dt.strftime('%Y.%m.%d')}"
    
    def _bulk_index(self, batch):
        """
        Bulk index for performance
        """
        helpers.bulk(self.es, batch)


### **4. Log Search API**

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/search', methods=['GET'])
def search_logs():
    """
    Search logs
    
    Query params:
    - query: search text
    - service: filter by service
    - level: filter by log level
    - from_time: start timestamp
    - to_time: end timestamp
    """
    
    query_text = request.args.get('query', '*')
    service = request.args.get('service')
    level = request.args.get('level')
    from_time = request.args.get('from_time', 'now-1h')
    to_time = request.args.get('to_time', 'now')
    
    # Build Elasticsearch query
    es_query = {
        "query": {
            "bool": {
                "must": [
                    {"query_string": {"query": query_text}},
                    {"range": {"@timestamp": {"gte": from_time, "lte": to_time}}}
                ],
                "filter": []
            }
        },
        "sort": [{"@timestamp": "desc"}],
        "size": 100
    }
    
    if service:
        es_query['query']['bool']['filter'].append({"term": {"service": service}})
    
    if level:
        es_query['query']['bool']['filter'].append({"term": {"level": level}})
    
    # Execute search
    result = es.search(index="logs-*", body=es_query)
    
    logs = [hit['_source'] for hit in result['hits']['hits']]
    
    return jsonify({
        'total': result['hits']['total']['value'],
        'logs': logs
    }), 200


@app.route('/aggregate', methods=['GET'])
def aggregate_logs():
    """
    Aggregate log statistics
    """
    
    # Aggregation query
    agg_query = {
        "size": 0,
        "aggs": {
            "errors_by_service": {
                "filter": {"term": {"level": "ERROR"}},
                "aggs": {
                    "services": {
                        "terms": {"field": "service.keyword", "size": 10}
                    }
                }
            },
            "log_volume_over_time": {
                "date_histogram": {
                    "field": "@timestamp",
                    "interval": "1h"
                }
            }
        }
    }
    
    result = es.search(index="logs-*", body=agg_query)
    
    return jsonify(result['aggregations']), 200


### **5. Log Alerting**

```python
from elasticsearch import Elasticsearch
import time

class LogAlerter:
    """
    Alert on log patterns
    """
    
    def __init__(self, es_client, alert_webhook):
        self.es = es_client
        self.webhook = alert_webhook
    
    def check_error_spike(self):
        """
        Alert if error rate exceeds threshold
        """
        
        # Count errors in last 5 minutes
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"level": "ERROR"}},
                        {"range": {"@timestamp": {"gte": "now-5m"}}}
                    ]
                }
            }
        }
        
        result = self.es.count(index="logs-*", body=query)
        error_count = result['count']
        
        # Threshold: 1000 errors in 5 min
        if error_count > 1000:
            self._send_alert({
                'title': 'High Error Rate',
                'message': f'{error_count} errors in last 5 minutes',
                'severity': 'critical'
            })
    
    def check_log_pattern(self, pattern: str):
        """
        Alert on specific log message pattern
        """
        
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"message": pattern}},
                        {"range": {"@timestamp": {"gte": "now-1m"}}}
                    ]
                }
            }
        }
        
        result = self.es.count(index="logs-*", body=query)
        
        if result['count'] > 0:
            self._send_alert({
                'title': f'Log Pattern Detected: {pattern}',
                'message': f'{result["count"]} occurrences',
                'severity': 'warning'
            })
```

---

## üéì Interview Points

**Capacity:**
```
Ingestion: 10 TB/day = 115 MB/sec
Events: 100K/sec √ó 1KB = 100 MB/sec

Storage:
Hot (30 days): 300 TB
Cold (1 year): 3.6 PB (S3)

Elasticsearch:
100 shards √ó 3 GB = 300 GB per day
30 days: 9 TB (with replication: 18 TB)
```

**Key challenges:** Parsing, enrichment, retention, search performance!
