# Distributed Systems - WHITEPAPERS

**Reading Time:** 4-6 weeks  
**Prerequisites:** BEGINNER.md + INTERMEDIATE.md + EXPERT.md  
**Focus:** Seminal papers that shaped distributed systems

---

## ğŸ“š Table of Contents

1. [Time, Clocks, and the Ordering of Events (Lamport, 1978)](#1-time-clocks-and-the-ordering-of-events)
2. [FLP Impossibility Result (1985)](#2-flp-impossibility-result)
3. [Paxos Made Simple (Lamport, 2001)](#3-paxos-made-simple)
4. [Dynamo: Amazon's Highly Available Key-Value Store (2007)](#4-dynamo-amazons-highly-available-key-value-store)
5. [Raft Consensus Algorithm (2014)](#5-raft-consensus-algorithm)
6. [Spanner: Google's Globally Distributed Database (2012)](#6-spanner-googles-globally-distributed-database)
7. [Calvin: Fast Distributed Transactions (2012)](#7-calvin-fast-distributed-transactions)
8. [Bonus Papers](#8-bonus-papers)

---

## 1. Time, Clocks, and the Ordering of Events

**Author:** Leslie Lamport  
**Published:** Communications of the ACM, July 1978  
**Link:** https://lamport.azurewebsites.net/pubs/time-clocks.pdf

### The Problem

```
Question: How do we order events in a distributed system without a global clock?

Example:
â”œâ”€ Event A: User sends email from New York
â”œâ”€ Event B: User receives email in London
â””â”€ Which happened first?

Challenge:
â”œâ”€ Clocks on different machines drift
â”œâ”€ Network delays are variable
â””â”€ No way to know absolute order
```

### Lamport's Solution: Happened-Before Relation

```
Definition: Event a â†’ Event b (a "happened before" b) if:

1. Same process: a occurs before b in program order
2. Message passing: a is send(m), b is receive(m)
3. Transitivity: If a â†’ c and c â†’ b, then a â†’ b

Example:
Process P1: a â”€â”€â”€â”€â”€â”€â†’ b â”€â”€â”€â”€â”€â”€â†’ send(m1) â”€â”€â”€â”€â”€â†’
                                    â†“
Process P2:                    receive(m1) â”€â”€â†’ c â”€â”€â†’ send(m2) â”€â”€â†’
                                                         â†“
Process P3:                                         receive(m2) â”€â”€â†’ d

Ordering:
â”œâ”€ a â†’ b (same process)
â”œâ”€ send(m1) â†’ receive(m1) (message)
â”œâ”€ receive(m1) â†’ c (same process)
â”œâ”€ a â†’ b â†’ send(m1) â†’ receive(m1) â†’ c (transitivity)
â”œâ”€ c â†’ send(m2) â†’ receive(m2) â†’ d (transitivity)
â””â”€ Conclusion: a â†’ d âœ…

Concurrent events (no ordering):
â”œâ”€ a and c (no path from a to c or c to a)
â””â”€ a || c (concurrent)
```

### Logical Clocks

**Algorithm:**
```python
class LamportClock:
    def __init__(self):
        self.time = 0
    
    def tick(self):
        """Local event"""
        self.time += 1
        return self.time
    
    def send(self):
        """Sending message"""
        self.time += 1
        return self.time
    
    def receive(self, msg_time):
        """Receiving message"""
        self.time = max(self.time, msg_time) + 1
        return self.time

# Example
p1 = LamportClock()
p2 = LamportClock()

# P1: Event a
t_a = p1.tick()  # 1

# P1: Event b
t_b = p1.tick()  # 2

# P1: Send message
t_send = p1.send()  # 3

# P2: Receive message
t_recv = p2.receive(t_send)  # max(0, 3) + 1 = 4

# P2: Event c
t_c = p2.tick()  # 5

# Property: If a â†’ b, then C(a) < C(b) âœ…
print(f"a: {t_a}, b: {t_b}, send: {t_send}, recv: {t_recv}, c: {t_c}")
# Output: a: 1, b: 2, send: 3, recv: 4, c: 5
```

**Limitation:**
```
C(a) < C(b) does NOT imply a â†’ b

Counterexample:
â”œâ”€ P1: Event a at time 1
â”œâ”€ P2: Event b at time 2 (concurrent)
â”œâ”€ C(a) < C(b) BUT a || b (concurrent, not ordered)
```

### Total Ordering

```
To break ties, use process ID:

(time, process_id)

Example:
â”œâ”€ Event a: (5, P1)
â”œâ”€ Event b: (5, P2)
â”œâ”€ Total order: (5, P1) < (5, P2) because P1 < P2 âœ…
```

### Key Insights

```
1. Physical time is not needed for ordering
2. Logical clocks capture causality
3. Concurrent events cannot be ordered
4. Total ordering requires breaking ties (process ID)
```

### Impact on Distributed Systems

```
Applications:
â”œâ”€ Distributed debugging (order log events)
â”œâ”€ Consistency models (causal consistency)
â”œâ”€ Version vectors (detect conflicts)
â””â”€ Blockchain (order transactions)

Still relevant in 2025:
â”œâ”€ Every distributed database uses logical clocks
â”œâ”€ Foundation for vector clocks, version vectors
â””â”€ Required reading for distributed systems engineers
```

---

## 2. FLP Impossibility Result

**Authors:** Fischer, Lynch, Paterson  
**Published:** Journal of the ACM, April 1985  
**Link:** https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf

### The Theorem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FLP Impossibility Theorem                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ In an asynchronous network where even ONE process can  â”‚
â”‚ fail by crashing, there is NO deterministic consensus  â”‚
â”‚ algorithm that guarantees termination.                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Properties:
â”œâ”€ Agreement: All non-faulty processes decide same value
â”œâ”€ Validity: Decided value was proposed by some process
â”œâ”€ Termination: All non-faulty processes eventually decide

FLP proves: Cannot have all three in asynchronous networks!
```

### Why Consensus is Impossible

```
Scenario: 2 processes, 1 can fail

Process A proposes: 0
Process B proposes: 1

Problem: Cannot distinguish crashed process from slow process

Case 1: A crashed
â”œâ”€ B waits for A's message
â”œâ”€ A never responds (crashed)
â”œâ”€ B cannot decide (might violate agreement if A is just slow)

Case 2: A is slow
â”œâ”€ B waits for A's message
â”œâ”€ A sends message, but network is slow
â”œâ”€ B decides without A (might violate agreement if A decides differently)

Catch-22:
â”œâ”€ Wait forever: No termination âŒ
â””â”€ Decide without waiting: Might violate agreement âŒ
```

### Proof Sketch

```python
# Simplified proof intuition

def consensus_algorithm(initial_value, processes):
    """Hypothetical consensus algorithm"""
    
    # Start in bivalent state (can decide 0 or 1)
    state = "bivalent"
    
    while state == "bivalent":
        # Receive messages from processes
        messages = receive_messages(processes)
        
        # Problem: What if one process is slow/crashed?
        # â”œâ”€ Wait: Might wait forever âŒ
        # â””â”€ Don't wait: Might decide prematurely âŒ
        
        # FLP shows: There exists an execution where system
        # remains bivalent forever (never reaches decision)
        pass
    
    # Cannot guarantee termination!


# The "always bivalent" execution
def flp_execution():
    """Adversarial execution that prevents decision"""
    
    # Scheduler delays messages to keep system bivalent
    while True:
        # Identify critical message (would make system univalent)
        critical_msg = find_critical_message()
        
        # Delay that message
        delay(critical_msg)
        
        # Process other messages
        # System remains bivalent (can still decide 0 or 1)
```

### Practical Implications

```
Does FLP mean consensus is impossible in practice?

NO! FLP has loopholes:

1. Randomization (not deterministic)
   â”œâ”€ Example: Random timeouts
   â””â”€ Guarantee: Terminate with probability 1 (not certainty)

2. Partial Synchrony
   â”œâ”€ Assumption: Eventually network becomes synchronous
   â””â”€ Paxos, Raft work in practice (assume bounded delays)

3. Failure Detectors
   â”œâ”€ Eventually Perfect detector (â—‡P)
   â””â”€ Eventually suspects all crashed processes

4. Weakening Guarantees
   â”œâ”€ Probabilistic termination
   â””â”€ Bounded time with high probability
```

### Real-World Systems

```
How do real systems handle FLP?

Paxos/Raft:
â”œâ”€ Use timeouts (randomization)
â”œâ”€ Assume partial synchrony
â””â”€ Work in practice, but no theoretical guarantee

Bitcoin:
â”œâ”€ Probabilistic finality (6 confirmations â‰ˆ 99.9% certain)
â””â”€ Not guaranteed, but good enough

Spanner:
â”œâ”€ TrueTime (bounded clock uncertainty)
â”œâ”€ Wait out uncertainty
â””â”€ Strong consistency in practice
```

### Key Takeaway

```
FLP teaches us:
â”œâ”€ Perfect consensus is impossible in asynchronous networks
â”œâ”€ Real systems make trade-offs (timeouts, assumptions)
â”œâ”€ Understand limitations when designing distributed systems
â””â”€ "In theory, theory and practice are the same. In practice, they are not."
```

---

## 3. Paxos Made Simple

**Author:** Leslie Lamport  
**Published:** ACM SIGACT News, November 2001  
**Link:** https://lamport.azurewebsites.net/pubs/paxos-simple.pdf

### Background

```
Original Paxos paper (1998): "The Part-Time Parliament"
â”œâ”€ Written as Greek allegory
â”œâ”€ Reviewers couldn't understand it
â””â”€ Rejected multiple times

"Paxos Made Simple" (2001):
â”œâ”€ Lamport rewrote in plain English
â”œâ”€ Opening line: "The Paxos algorithm, when presented in plain English, is very simple."
â””â”€ Still considered complex by many! ğŸ˜…
```

### The Paxos Problem

```
Goal: Multiple proposers, choose single value

Setup:
â”œâ”€ N processes (odd number, e.g., 5)
â”œâ”€ Any process can propose value
â”œâ”€ Goal: All processes agree on ONE value
â””â”€ Tolerate F = (N-1)/2 failures

Example:
â”œâ”€ 5 processes: Tolerate 2 failures
â””â”€ 3 processes: Tolerate 1 failure
```

### Paxos Roles

```
1. Proposer: Proposes values
2. Acceptor: Votes on proposals (needs majority)
3. Learner: Learns chosen value

Note: One process can play multiple roles
```

### Paxos Algorithm (Detailed)

```
Phase 1a: Prepare
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Proposer:                                               â”‚
â”‚ â”œâ”€ Choose proposal number n (higher than any seen)     â”‚
â”‚ â””â”€ Send PREPARE(n) to majority of acceptors            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1b: Promise
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Acceptor:                                               â”‚
â”‚ â”œâ”€ Receive PREPARE(n)                                   â”‚
â”‚ â”œâ”€ If n > highest_promised_n:                           â”‚
â”‚ â”‚  â”œâ”€ Promise not to accept proposals < n              â”‚
â”‚ â”‚  â”œâ”€ Reply with highest accepted (n_a, v_a)           â”‚
â”‚ â”‚  â””â”€ highest_promised_n = n                           â”‚
â”‚ â””â”€ Else: Ignore (already promised higher)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 2a: Accept
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Proposer:                                               â”‚
â”‚ â”œâ”€ Receive promises from majority                      â”‚
â”‚ â”œâ”€ If any acceptor returned (n_a, v_a):                â”‚
â”‚ â”‚  â””â”€ Use v_a with highest n_a (not our value!)       â”‚
â”‚ â”œâ”€ Else: Use our proposed value v                      â”‚
â”‚ â””â”€ Send ACCEPT(n, v) to majority of acceptors          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 2b: Accepted
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Acceptor:                                               â”‚
â”‚ â”œâ”€ Receive ACCEPT(n, v)                                 â”‚
â”‚ â”œâ”€ If n >= highest_promised_n:                          â”‚
â”‚ â”‚  â”œâ”€ Accept (n, v)                                    â”‚
â”‚ â”‚  â””â”€ Notify learners                                  â”‚
â”‚ â””â”€ Else: Reject (promised higher)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Execution

```
Setup: 5 acceptors (A1, A2, A3, A4, A5)
       2 proposers (P1, P2)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T1: P1 proposes value "X" with n=1

    P1 â†’ A1, A2, A3: PREPARE(n=1)

T2: Acceptors promise

    A1 â†’ P1: PROMISE (no prior accepts)
    A2 â†’ P1: PROMISE (no prior accepts)
    A3 â†’ P1: PROMISE (no prior accepts)

    P1 has majority (3/5) âœ…

T3: P1 sends ACCEPT

    P1 â†’ A1, A2, A3: ACCEPT(n=1, v="X")

T4: Acceptors accept

    A1: accepted = (1, "X")
    A2: accepted = (1, "X")
    
    "X" is chosen! (majority accepted) âœ…

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T5: P2 proposes value "Y" with n=2 (concurrent)

    P2 â†’ A1, A2, A3: PREPARE(n=2)

T6: Acceptors promise, report accepted value

    A1 â†’ P2: PROMISE (accepted=(1, "X"))
    A2 â†’ P2: PROMISE (accepted=(1, "X"))
    A3 â†’ P2: PROMISE (no accept yet)

T7: P2 MUST use "X" (not "Y"!)

    Highest accepted: (1, "X")
    P2 â†’ A1, A2, A3: ACCEPT(n=2, v="X")  â† Not "Y"!

T8: Result: "X" remains chosen âœ…
```

### Why Paxos Works

```
Safety (agreement):
â”œâ”€ Once value is chosen, all future proposals choose same value
â”œâ”€ Proof: Proposer adopts highest accepted value
â””â”€ Invariant: Chosen value appears in all majority quorums

Liveness (termination):
â”œâ”€ Eventually one proposer succeeds
â”œâ”€ Challenge: Dueling proposers (livelock)
â””â”€ Solution: Leader election, exponential backoff
```

### Multi-Paxos Optimization

```
Basic Paxos: 2 round trips per decision (slow!)

Multi-Paxos optimization:
â”œâ”€ Run Phase 1 once for entire log
â”œâ”€ Leader sends Phase 2 (ACCEPT) for each entry
â””â”€ 1 round trip per decision âœ…

State Machine Replication:
â”œâ”€ Paxos chooses log entries (commands)
â”œâ”€ All replicas execute commands in same order
â””â”€ Result: Replicated state machine
```

### Paxos in Production

```
Google Chubby (2006):
â”œâ”€ Distributed lock service
â”œâ”€ Uses Paxos for consensus
â””â”€ Powers BigTable, GFS

Apache ZooKeeper:
â”œâ”€ Based on ZAB (similar to Paxos)
â”œâ”€ Coordination service (locks, config, leader election)
â””â”€ Used by Kafka, HBase, Hadoop

etcd:
â”œâ”€ Uses Raft (not Paxos)
â”œâ”€ Simpler than Paxos
â””â”€ Powers Kubernetes
```

---

## 4. Dynamo: Amazon's Highly Available Key-Value Store

**Authors:** DeCandia et al. (Amazon)  
**Published:** SOSP 2007  
**Link:** https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf

### The Problem

```
Amazon's requirements:
â”œâ”€ 99.9% availability (5.26 min downtime/month max)
â”œâ”€ Handle peak load (10M requests/second)
â”œâ”€ Low latency (<300ms P99)
â””â”€ Eventual consistency acceptable (shopping cart can merge)

Traditional databases:
â”œâ”€ Strong consistency â†’ Low availability âŒ
â””â”€ Cannot meet 99.9% SLA
```

### Dynamo's Design Principles

```
1. Always Writable
   â”œâ”€ Accept writes even during failures
   â””â”€ Resolve conflicts later (eventual consistency)

2. Incremental Scalability
   â”œâ”€ Add nodes without downtime
   â””â”€ Automatic data redistribution

3. Symmetry
   â”œâ”€ No master/slave
   â””â”€ All nodes have same responsibilities

4. Decentralization
   â”œâ”€ No single point of failure
   â””â”€ Peer-to-peer architecture

5. Heterogeneity
   â”œâ”€ Nodes have different capacities
   â””â”€ Workload proportional to capacity
```

### Dynamo Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Partitioning: Consistent Hashing                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Hash Ring (0 to 2^128):                                â”‚
â”‚                                                         â”‚
â”‚         Node A (token: 0x0000)                          â”‚
â”‚            â†“                                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚    â”‚               â”‚                                    â”‚
â”‚    â”‚               â”‚ â† Node D (token: 0xC000)          â”‚
â”‚    â”‚               â”‚                                    â”‚
â”‚    â”‚               â”‚                                    â”‚
â”‚    â”‚               â”‚                                    â”‚
â”‚    â”‚               â”‚ â† Node C (token: 0x8000)          â”‚
â”‚    â”‚               â”‚                                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚            â†‘                                            â”‚
â”‚         Node B (token: 0x4000)                          â”‚
â”‚                                                         â”‚
â”‚ Key placement:                                          â”‚
â”‚ â”œâ”€ hash("user123") = 0x3000 â†’ Node B                  â”‚
â”‚ â”œâ”€ hash("user456") = 0xA000 â†’ Node C                  â”‚
â”‚ â””â”€ Add/remove node: Only 1/N keys move âœ…              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Replication: N replicas (default: 3)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Key "user123" â†’ hash = 0x3000 â†’ Node B                â”‚
â”‚                                                         â”‚
â”‚ Replicas:                                               â”‚
â”‚ â”œâ”€ Primary: Node B (coordinator)                       â”‚
â”‚ â”œâ”€ Replica 1: Node C (next clockwise)                  â”‚
â”‚ â””â”€ Replica 2: Node D (next clockwise)                  â”‚
â”‚                                                         â”‚
â”‚ Preference list: [B, C, D]                             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Versioning: Vector Clocks                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Track causality to detect conflicts:                    â”‚
â”‚                                                         â”‚
â”‚ Version 1: {A:1} (Node A wrote)                        â”‚
â”‚ Version 2: {A:1, B:1} (Node B updated)                 â”‚
â”‚ Version 3a: {A:1, B:1, C:1} (Node C updated)           â”‚
â”‚ Version 3b: {A:1, B:2} (Node B updated, concurrent!)   â”‚
â”‚                                                         â”‚
â”‚ Conflict: 3a and 3b are concurrent                     â”‚
â”‚ â”œâ”€ Neither {A:1, B:1, C:1} â‰¤ {A:1, B:2}               â”‚
â”‚ â””â”€ Application must resolve (e.g., merge carts)       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Quorum: Tunable Consistency                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Parameters:                                             â”‚
â”‚ â”œâ”€ N = replication factor (e.g., 3)                    â”‚
â”‚ â”œâ”€ W = write quorum (e.g., 2)                          â”‚
â”‚ â””â”€ R = read quorum (e.g., 2)                           â”‚
â”‚                                                         â”‚
â”‚ Constraint: R + W > N (ensures overlap)                â”‚
â”‚                                                         â”‚
â”‚ Configurations:                                         â”‚
â”‚ â”œâ”€ (R=1, W=3): Fast reads, slow writes                â”‚
â”‚ â”œâ”€ (R=3, W=1): Slow reads, fast writes                â”‚
â”‚ â””â”€ (R=2, W=2): Balanced                                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Anti-Entropy: Merkle Trees                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Detect inconsistencies efficiently:                     â”‚
â”‚                                                         â”‚
â”‚        Root Hash (entire dataset)                       â”‚
â”‚          /                \                             â”‚
â”‚    Hash(Keys 0-100)   Hash(Keys 101-200)              â”‚
â”‚      /        \          /           \                  â”‚
â”‚  H(0-50)  H(51-100)  H(101-150)  H(151-200)           â”‚
â”‚                                                         â”‚
â”‚ Sync protocol:                                          â”‚
â”‚ 1. Exchange root hashes                                 â”‚
â”‚ 2. If different, exchange subtree hashes                â”‚
â”‚ 3. Identify divergent leaf, sync only that range        â”‚
â”‚                                                         â”‚
â”‚ Efficiency: O(log N) hashes exchanged âœ…               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dynamo Read/Write Protocol

```python
class DynamoNode:
    def __init__(self, node_id, N=3, R=2, W=2):
        self.node_id = node_id
        self.N = N  # Replication factor
        self.R = R  # Read quorum
        self.W = W  # Write quorum
        self.data = {}  # key â†’ [(value, vector_clock)]
    
    def put(self, key, value):
        """Write with quorum"""
        # 1. Coordinate write
        coordinator = self.get_coordinator(key)
        
        # 2. Get preference list (N replicas)
        replicas = self.get_preference_list(key, self.N)
        
        # 3. Increment vector clock
        vector_clock = self.get_vector_clock(key)
        vector_clock[self.node_id] = vector_clock.get(self.node_id, 0) + 1
        
        # 4. Write to W replicas
        acks = 0
        for replica in replicas:
            try:
                replica.local_put(key, value, vector_clock)
                acks += 1
                if acks >= self.W:
                    return "SUCCESS"
            except Exception:
                pass
        
        raise Exception(f"Failed to write to {self.W} replicas")
    
    def get(self, key):
        """Read with quorum"""
        # 1. Get preference list
        replicas = self.get_preference_list(key, self.N)
        
        # 2. Read from R replicas
        versions = []
        for replica in replicas:
            try:
                version = replica.local_get(key)
                versions.append(version)
                if len(versions) >= self.R:
                    break
            except Exception:
                pass
        
        if len(versions) < self.R:
            raise Exception(f"Failed to read from {self.R} replicas")
        
        # 3. Reconcile versions
        return self.reconcile(versions)
    
    def reconcile(self, versions):
        """Resolve conflicts using vector clocks"""
        # Remove dominated versions (A â‰¤ B means A is older)
        current = []
        for v1 in versions:
            is_dominated = False
            for v2 in versions:
                if self.is_dominated(v1["clock"], v2["clock"]):
                    is_dominated = True
                    break
            if not is_dominated:
                current.append(v1)
        
        if len(current) == 1:
            return current[0]  # No conflict
        else:
            # Conflict! Return all versions, let app resolve
            return {"conflict": True, "versions": current}
    
    def is_dominated(self, clock1, clock2):
        """Check if clock1 â‰¤ clock2 (clock1 is older)"""
        for node_id in set(clock1.keys()).union(clock2.keys()):
            if clock1.get(node_id, 0) > clock2.get(node_id, 0):
                return False
        return True

# Example: Shopping cart
dynamo = DynamoNode("node1", N=3, R=2, W=2)

# User adds item
dynamo.put("cart:user123", ["apple"])
# Version: {node1: 1}

# User adds another item (from different node)
dynamo.put("cart:user123", ["apple", "banana"])
# Version: {node1: 1, node2: 1}

# Concurrent add (conflict!)
# Node 3 adds "orange" without seeing "banana"
# Version: {node1: 1, node3: 1}

# Read returns conflict
result = dynamo.get("cart:user123")
# result = {
#   "conflict": True,
#   "versions": [
#     {"value": ["apple", "banana"], "clock": {node1: 1, node2: 1}},
#     {"value": ["apple", "orange"], "clock": {node1: 1, node3: 1}}
#   ]
# }

# Application resolves: Merge carts
merged = ["apple", "banana", "orange"]
dynamo.put("cart:user123", merged)
# Version: {node1: 1, node2: 1, node3: 1, node4: 1}
```

### Dynamo's Impact

```
Influenced many systems:
â”œâ”€ Apache Cassandra (Dynamo + BigTable)
â”œâ”€ Riak (direct Dynamo implementation)
â”œâ”€ Voldemort (LinkedIn)
â””â”€ DynamoDB (AWS managed service)

Key innovations:
â”œâ”€ Consistent hashing (partition + add nodes)
â”œâ”€ Vector clocks (detect conflicts)
â”œâ”€ Quorum (tunable consistency)
â”œâ”€ Merkle trees (efficient anti-entropy)
â””â”€ Always writable (high availability)

Lessons:
â”œâ”€ Eventual consistency is acceptable for many apps
â”œâ”€ Application-level conflict resolution works
â””â”€ Availability > Consistency (for Amazon's use case)
```

---

## 5. Raft Consensus Algorithm

**Authors:** Diego Ongaro, John Ousterhout (Stanford)  
**Published:** USENIX ATC 2014  
**Link:** https://raft.github.io/raft.pdf

### Motivation

```
Problem with Paxos:
â”œâ”€ Difficult to understand
â”œâ”€ Difficult to implement correctly
â”œâ”€ Incomplete specification (Multi-Paxos not in paper)
â””â”€ Google took years to implement Chubby

Raft's goal: "Understandability as primary design goal"
â”œâ”€ Easier to teach in courses
â”œâ”€ Easier to implement in systems
â””â”€ Same guarantees as Paxos
```

### Raft's Key Ideas

```
1. Strong Leader
   â”œâ”€ All writes go through leader
   â”œâ”€ Log entries flow one direction (leader â†’ followers)
   â””â”€ Simpler than Paxos (no dueling proposers)

2. Leader Election
   â”œâ”€ Randomized timeouts prevent split votes
   â”œâ”€ Candidate with longest log wins
   â””â”€ One leader per term (epoch)

3. Log Replication
   â”œâ”€ Leader appends to local log
   â”œâ”€ Replicates to followers
   â”œâ”€ Commits when majority replicated
   â””â”€ Eventually all logs identical

4. Safety
   â”œâ”€ Elected leader has all committed entries
   â”œâ”€ Log matching property
   â””â”€ State machine safety
```

### Raft Algorithm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader Election                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ States: Follower, Candidate, Leader                    â”‚
â”‚                                                         â”‚
â”‚ Timeout (follower â†’ candidate):                        â”‚
â”‚ â”œâ”€ Increment term                                       â”‚
â”‚ â”œâ”€ Vote for self                                        â”‚
â”‚ â”œâ”€ Request votes from all peers                        â”‚
â”‚ â””â”€ If majority: Become leader                          â”‚
â”‚                                                         â”‚
â”‚ Voting rules:                                           â”‚
â”‚ â”œâ”€ One vote per term (first come first served)         â”‚
â”‚ â”œâ”€ Vote only if candidate's log â‰¥ ours                 â”‚
â”‚ â””â”€ Reset timeout when voting                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Log Replication                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Client â†’ Leader: Command                               â”‚
â”‚                                                         â”‚
â”‚ Leader:                                                 â”‚
â”‚ â”œâ”€ Append to local log                                 â”‚
â”‚ â”œâ”€ Send AppendEntries to followers                     â”‚
â”‚ â”œâ”€ Wait for majority to replicate                      â”‚
â”‚ â”œâ”€ Commit (advance commitIndex)                        â”‚
â”‚ â”œâ”€ Apply to state machine                              â”‚
â”‚ â””â”€ Reply to client                                     â”‚
â”‚                                                         â”‚
â”‚ Log matching property:                                  â”‚
â”‚ â”œâ”€ If two entries have same (index, term), they have   â”‚
â”‚ â”‚  same command AND all preceding entries are identicalâ”‚
â”‚ â””â”€ Enforced by AppendEntries consistency check         â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Safety                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ Election Safety:                                        â”‚
â”‚ â”œâ”€ At most one leader per term                         â”‚
â”‚ â””â”€ Proof: Majority votes, can't have two majorities    â”‚
â”‚                                                         â”‚
â”‚ Leader Completeness:                                    â”‚
â”‚ â”œâ”€ If entry committed in term T, it appears in logs of â”‚
â”‚ â”‚  all leaders for terms â‰¥ T                           â”‚
â”‚ â””â”€ Proof: Candidate must have majority's log, majority â”‚
â”‚    has committed entry                                  â”‚
â”‚                                                         â”‚
â”‚ State Machine Safety:                                   â”‚
â”‚ â”œâ”€ If server applies entry at index i, no other server â”‚
â”‚ â”‚  applies different entry at i                        â”‚
â”‚ â””â”€ Follows from leader completeness                    â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Raft vs Paxos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature          â”‚ Raft              â”‚ Paxos             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Understandabilityâ”‚ âœ… High           â”‚ âŒ Low            â”‚
â”‚ Leader           â”‚ âœ… Strong         â”‚ âš ï¸ Weak (Multi-P) â”‚
â”‚ Log structure    â”‚ âœ… Must be contig â”‚ âŒ Gaps allowed   â”‚
â”‚ Election         â”‚ âœ… Simple (term)  â”‚ âš ï¸ Complex (ballots)â”‚
â”‚ Implementation   â”‚ âœ… Straightforwardâ”‚ âŒ Tricky         â”‚
â”‚ Liveness         â”‚ âœ… Good (rand TO) â”‚ âš ï¸ Dueling props  â”‚
â”‚ Performance      â”‚ â‰ˆ Same            â”‚ â‰ˆ Same            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Raft in Production

```
Systems using Raft:
â”œâ”€ etcd (Kubernetes, CoreOS)
â”œâ”€ Consul (HashiCorp)
â”œâ”€ TiKV (TiDB)
â”œâ”€ CockroachDB
â””â”€ InfluxDB

Why Raft won:
â”œâ”€ Easier to implement (companies can build it in-house)
â”œâ”€ Easier to debug (clearer invariants)
â”œâ”€ Great educational resources (raft.github.io)
â””â”€ Same performance as Paxos
```

---

## 6. Spanner: Google's Globally Distributed Database

**Authors:** Corbett et al. (Google)  
**Published:** OSDI 2012  
**Link:** https://research.google/pubs/pub39966/

### The Grand Challenge

```
Build a database that is:
â”œâ”€ Globally distributed (replicate across continents)
â”œâ”€ Strongly consistent (external consistency = linearizability)
â”œâ”€ High availability (survive data center failures)
â”œâ”€ Scalable (millions of QPS, petabytes of data)
â””â”€ SQL support (not just key-value)

Problem: CAP theorem says can't have consistency + availability!

Spanner's answer: Redefine "available"
â”œâ”€ 99.999% uptime (5 min downtime/year)
â”œâ”€ Sacrifice availability during network partitions (rare)
â””â”€ Choose consistency over availability (CP system)
```

### TrueTime API

```
The key innovation: Bounded clock uncertainty

API:
â”œâ”€ TT.now() â†’ [earliest, latest]
â”œâ”€ TT.after(t) â†’ true if t definitely in past
â””â”€ TT.before(t) â†’ true if t definitely in future

Implementation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Each Google data center:                               â”‚
â”‚ â”œâ”€ GPS receivers (cheap, 1Î¼s accuracy when locked)    â”‚
â”‚ â”œâ”€ Atomic clocks (expensive, but no drift)             â”‚
â”‚ â””â”€ Time masters: Sync from GPS + atomic clocks         â”‚
â”‚                                                         â”‚
â”‚ Every server:                                           â”‚
â”‚ â”œâ”€ Daemon syncs with time masters                      â”‚
â”‚ â”œâ”€ Uses Marzullo's algorithm (best estimate)           â”‚
â”‚ â””â”€ Uncertainty: Îµ â‰ˆ 1-7 milliseconds                   â”‚
â”‚                                                         â”‚
â”‚ Failure modes:                                          â”‚
â”‚ â”œâ”€ GPS signal lost: Use atomic clocks (hours backup)   â”‚
â”‚ â”œâ”€ Both fail: Increase Îµ (uncertainty grows)           â”‚
â”‚ â””â”€ Îµ too large: Refuse to serve (availabilityâ†“)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Spanner Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Universe (Spanner deployment)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Zone (DC 1) â”‚  â”‚ Zone (DC 2) â”‚  â”‚ Zone (DC 3) â”‚     â”‚
â”‚ â”‚             â”‚  â”‚             â”‚  â”‚             â”‚     â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚ â”‚ â”‚Spanserverâ”‚ â”‚  â”‚ â”‚Spanserverâ”‚ â”‚  â”‚ â”‚Spanserverâ”‚ â”‚     â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚ â”‚      ...    â”‚  â”‚      ...    â”‚  â”‚      ...    â”‚     â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚ â”‚ â”‚Spanserverâ”‚ â”‚  â”‚ â”‚Spanserverâ”‚ â”‚  â”‚ â”‚Spanserverâ”‚ â”‚     â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                         â”‚
â”‚ Spanserver = 100-1000 tablets (partitions)             â”‚
â”‚ Tablet = Paxos group (5-7 replicas across zones)       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data organization:
â”œâ”€ Directory: Unit of data movement
â”œâ”€ Tablet: Range of rows [start_key, end_key)
â”œâ”€ Paxos group: Replicate tablet (one leader)
â””â”€ Spanserver: Hosts multiple tablets
```

### Spanner Transactions

```python
class SpannerTransaction:
    def __init__(self, true_time):
        self.tt = true_time
        self.reads = []
        self.writes = []
    
    def read_write_txn(self, operations):
        """Read-write transaction (uses 2PC + Paxos)"""
        
        # 1. Acquire locks on participants
        participants = self.get_participants(operations)
        
        # 2. Execute reads/writes (buffer writes)
        start = self.tt.now()
        
        for op in operations:
            if op["type"] == "READ":
                self.reads.append(self.read(op["key"]))
            elif op["type"] == "WRITE":
                self.writes.append((op["key"], op["value"]))
        
        # 3. Choose commit timestamp
        # Must be after all reads, within TrueTime bound
        commit_ts = start["latest"]
        
        # 4. Two-phase commit
        # Prepare phase
        for participant in participants:
            participant.prepare(commit_ts, self.writes)
        
        # 5. Commit wait (KEY INNOVATION!)
        # Wait until commit_ts is definitely in the past
        while not self.tt.after(commit_ts):
            time.sleep(0.001)  # Wait ~7ms
        
        # 6. Commit phase
        for participant in participants:
            participant.commit(commit_ts)
        
        # 7. Release locks
        self.release_locks()
        
        return commit_ts
    
    def read_only_txn(self, keys, timestamp=None):
        """Read-only transaction (NO locks, NO 2PC!)"""
        
        if timestamp is None:
            # Use latest safe time (TT.now().latest)
            timestamp = self.tt.now()["latest"]
        
        # Read from any replica at timestamp
        results = []
        for key in keys:
            # Might block if timestamp > replica's safe time
            value = self.read_at_timestamp(key, timestamp)
            results.append(value)
        
        return results


# External consistency example

tt = TrueTime()
txn1 = SpannerTransaction(tt)
txn2 = SpannerTransaction(tt)

# T1: Transfer $100 from Alice to Bob
ts1 = txn1.read_write_txn([
    {"type": "READ", "key": "alice"},
    {"type": "WRITE", "key": "alice", "value": 900},
    {"type": "WRITE", "key": "bob", "value": 600}
])
# Commit wait: Waits ~7ms before returning to client

# Client sees T1 committed, starts T2
# T2: Read balances
ts2 = txn2.read_write_txn([
    {"type": "READ", "key": "alice"},  # Sees 900
    {"type": "READ", "key": "bob"}     # Sees 600
])

# Property: ts1 < ts2 (external consistency) âœ…
# Guaranteed by commit wait!
```

### Spanner's Impact

```
Proved CAP is not absolute:
â”œâ”€ With bounded clock uncertainty, can have strong consistency + high availability
â”œâ”€ TrueTime: 1-7ms uncertainty, acceptable for Google
â””â”€ Redefines "available" (99.999%, not 100%)

Influenced:
â”œâ”€ CockroachDB (HLC: Hybrid Logical Clocks, no GPS)
â”œâ”€ YugabyteDB (similar architecture)
â”œâ”€ Amazon Aurora (different approach, MySQL compatible)
â””â”€ Azure Cosmos DB (tunable consistency)

Key lesson:
â”œâ”€ Hardware can help solve distributed systems problems
â”œâ”€ GPS + atomic clocks = globally synchronized time
â””â”€ Trade latency (7ms commit wait) for strong guarantees
```

---

## 7. Calvin: Fast Distributed Transactions

**Authors:** Thomson et al. (Yale)  
**Published:** SIGMOD 2012  
**Link:** http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf

### The Problem

```
Traditional distributed transactions:
â”œâ”€ Two-phase commit (2PC)
â”œâ”€ Locks acquired during transaction
â”œâ”€ High latency (multiple round trips)
â””â”€ Deadlocks possible

Calvin's idea: Deterministic database
â”œâ”€ Pre-order transactions (sequence number)
â”œâ”€ All replicas execute in same order
â”œâ”€ No 2PC needed! (order determines outcome)
â””â”€ Locks acquired upfront (no deadlocks)
```

### Calvin Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Calvin's 3 Layers                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ 1. Sequencing Layer (Paxos/Raft)                       â”‚
â”‚    â”œâ”€ Assign global sequence number to transactions    â”‚
â”‚    â”œâ”€ Replicate log to all replicas                    â”‚
â”‚    â””â”€ Output: Ordered transaction log                  â”‚
â”‚                                                         â”‚
â”‚ 2. Scheduling Layer                                     â”‚
â”‚    â”œâ”€ Lock manager (acquire in sequence order)         â”‚
â”‚    â”œâ”€ Deterministic: Same locks, same order            â”‚
â”‚    â””â”€ No deadlocks (acquire in global order)           â”‚
â”‚                                                         â”‚
â”‚ 3. Storage Layer                                        â”‚
â”‚    â”œâ”€ Execute transaction (reads, writes)              â”‚
â”‚    â”œâ”€ Deterministic: Same inputs â†’ same outputs        â”‚
â”‚    â””â”€ Commit (release locks)                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Calvin Protocol

```python
class CalvinNode:
    def __init__(self, node_id, sequencer):
        self.node_id = node_id
        self.sequencer = sequencer  # Paxos/Raft instance
        self.lock_manager = {}
        self.storage = {}
        self.sequence_number = 0
    
    def submit_transaction(self, txn):
        """Client submits transaction"""
        # 1. Send to sequencer (Paxos/Raft)
        seq_num = self.sequencer.append(txn)
        return seq_num
    
    def execute_batch(self, txns):
        """Execute batch of transactions in sequence order"""
        
        # 1. Collect locks needed (from txn metadata)
        lock_table = {}
        for i, txn in enumerate(txns):
            for key in txn["read_set"] + txn["write_set"]:
                if key not in lock_table:
                    lock_table[key] = []
                lock_table[key].append(i)
        
        # 2. Acquire locks in sequence order (no deadlocks!)
        for txn_idx in range(len(txns)):
            txn = txns[txn_idx]
            for key in txn["read_set"] + txn["write_set"]:
                self.acquire_lock(key, txn_idx)
        
        # 3. Execute transactions in order
        for txn_idx in range(len(txns)):
            txn = txns[txn_idx]
            self.execute_txn(txn)
        
        # 4. Release locks
        for txn_idx in range(len(txns)):
            txn = txns[txn_idx]
            for key in txn["read_set"] + txn["write_set"]:
                self.release_lock(key, txn_idx)
    
    def execute_txn(self, txn):
        """Execute single transaction deterministically"""
        
        # Read phase
        reads = {}
        for key in txn["read_set"]:
            reads[key] = self.storage.get(key)
        
        # Execute logic (must be deterministic!)
        writes = txn["logic"](reads)
        
        # Write phase
        for key, value in writes.items():
            self.storage[key] = value


# Example: Transfer money

def transfer_logic(reads):
    """Deterministic transaction logic"""
    alice_balance = reads["alice"]
    bob_balance = reads["bob"]
    
    # Transfer $100
    return {
        "alice": alice_balance - 100,
        "bob": bob_balance + 100
    }

txn = {
    "read_set": ["alice", "bob"],
    "write_set": ["alice", "bob"],
    "logic": transfer_logic
}

# Submit to Calvin
calvin = CalvinNode("node1", sequencer)
calvin.submit_transaction(txn)

# All replicas execute in same order â†’ Same result âœ…
# No 2PC needed!
```

### Calvin's Benefits

```
Advantages:
â”œâ”€ No distributed deadlocks (locks in sequence order)
â”œâ”€ No 2PC overhead (order determines commit)
â”œâ”€ Deterministic replicas (easy to debug)
â””â”€ High throughput (batch transactions)

Disadvantages:
â”œâ”€ Requires read/write sets upfront (not all apps can provide)
â”œâ”€ Aborts expensive (must re-sequence)
â”œâ”€ Multi-partition transactions still slow (lock contention)
â””â”€ Not suitable for interactive transactions (unknown read set)
```

### Calvin's Impact

```
Inspired:
â”œâ”€ FaunaDB (Calvin-based commercial database)
â”œâ”€ Deterministic databases (VoltDB)
â””â”€ Research on determinism vs coordination

Key insight:
â”œâ”€ Determinism eliminates coordination overhead
â”œâ”€ Pre-ordering transactions = strong consistency without 2PC
â””â”€ Trade-off: Need to know read/write sets upfront
```

---

## 8. Bonus Papers

### 8.1 MapReduce (Google, 2004)

**Link:** https://research.google/pubs/pub62/

**Impact:** Started big data revolution, inspired Hadoop

### 8.2 BigTable (Google, 2006)

**Link:** https://research.google/pubs/pub27898/

**Impact:** Wide-column store, inspired HBase, Cassandra

### 8.3 Kafka (LinkedIn, 2011)

**Link:** https://www.confluent.io/blog/event-streaming-platform-1/

**Impact:** Distributed log, unified streaming platform

### 8.4 Chandy-Lamport Snapshot (1985)

**Link:** https://www.microsoft.com/en-us/research/publication/distributed-snapshots-determining-global-states-distributed-system/

**Impact:** Foundation for Flink savepoints, distributed debugging

---

## ğŸ¯ Reading Guide

```
Essential (must-read):
â”œâ”€ Lamport's Time & Clocks (foundational)
â”œâ”€ FLP Impossibility (know the limits)
â”œâ”€ Paxos Made Simple (consensus gold standard)
â”œâ”€ Raft (modern consensus, easier to implement)
â””â”€ Spanner (state-of-the-art distributed database)

Important (should-read):
â”œâ”€ Dynamo (eventual consistency, influenced NoSQL)
â”œâ”€ Calvin (deterministic transactions)
â””â”€ Chandy-Lamport (distributed snapshots)

Nice-to-have:
â”œâ”€ MapReduce (historical context)
â”œâ”€ BigTable (wide-column stores)
â””â”€ Kafka (distributed logs)
```

---

## âœ… Reading Checklist

- [ ] Lamport's Time & Clocks (understand happened-before, logical clocks)
- [ ] FLP Impossibility (know why consensus is "impossible")
- [ ] Paxos Made Simple (understand consensus algorithm)
- [ ] Dynamo (eventual consistency, vector clocks, quorums)
- [ ] Raft (modern consensus, compare with Paxos)
- [ ] Spanner (TrueTime, external consistency, commit wait)
- [ ] Calvin (deterministic execution, no 2PC)

---

**Next:** [README.md](README.md) â€” Module overview, learning path, capstone project.
